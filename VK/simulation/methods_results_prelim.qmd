---
title: "Methods & Results"
author: Valentin Kriegmair
format:
    pdf:
        fontsize: 12pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes:
           - \usepackage{float}
           - \floatplacement{table}{H}
           - \usepackage{tikz}
           - \usetikzlibrary{arrows.meta, positioning, calc}
           - \usepackage{geometry}
           - \geometry{margin=1in}
fontsize: 12pt
engine: knitr
bibliography: ../../bibliography.bib
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r}
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
library(data.table)
summary_1 <- readRDS("../simulation/results/summary_study1.rds")
summary_2 <- readRDS("../simulation/results/summary_study2.rds")
```

# 

# Methods

The following description of the simulation studies is based on the established structure for simulation studies to coalign with the other conducted studies to facilitate a potential collaboration.

### Aims, objectives and research questions

Both studies aimed to evaluate the performance of vanilla SEM compared to global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) under various conditions. The two research questions we established prior to conducting the studies served as general basis for both studies.

### Population Models and Data Generation Mechanisms

#### Study 1:

Data were generated based on a 5-factor population structural model with 3 indicators for each factor. Four different models were simulated (see figure 1-4):

-   **Model 1.1:** Correctly specified model.
-   **Model 1.2:** Misspecified with cross-loadings ignored in the estimation model.
-   **Model 1.3:** Misspecified with wrong direction of path and omitted correlated residuals.
-   **Model 1.4:** Misspecified with multiple omitted structural paths.

For all models, the population-level values of the structural parameters were set to 0.1. Indicator reliability levels were manipulated with factor loadings set at low ($\lambda = 0.3$), moderate ($\lambda = 0.5$), or high ($\lambda = 0.7$).

::: {layout="[[40,-20,40],[40,-20,40]]"}
![Model 1.1](figures/model1_1.tex){#fig-model1} Figure 1: Note. Model 1.1: Error terms are not explicitly shown in the figure.

![Model 1.2](figures/model1_2.tex){#fig-model2} Figure 2: Note. Model 1.2: Error terms are not explicitly shown in the figure.

![Model 1.3](figures/model1_3.tex){#fig-model3} Figure 3: Note. Model 1.3: Error terms are not explicitly shown in the figure.

![Model 1.4](figures/model1_4.tex){#fig-model4} Figure 4: Note. Model 1.4: Error terms are not explicitly shown in the figure.
:::

**Study 2:**

Data were generated based on a 5-factor population structural model with 3 indicators for each factor, similar to Study 1, but with the additional condition of varying variance explained ($R^2$) by the endogenous factors was set at low ($R^2 = 0.1$) or medium ($R^2 = 0.4$), so that the regression weights were either between 0.183 and 0.224 (low) or between 0.365 and 0.447 (medium). Note however that the computation of this was a simplification and does not accurately result in said $R^2$ values. The aim here was only generally to modulate between lower and higher regression weights. Misspecifications included omitting a residual covariance and a factor cross-loading in either the exogenous or endogenous part of the model, or both (see figure 5-6).

::: {layout="[[40,-20,40], [100]]"}
![Model 2.1](figures/model2_1.tex) Figure 5: Note. Model 2.1: Error terms are not explicitly shown in the figure.

![Model 2.2](figures/model2_2.tex) Figure 6: Note. Model 2.2: Error terms are not explicitly shown in the figure Orange lines represent misspecifications in the endogenous part of the model. Green lines represent misspecifications in the exogenous part of the model. These types of misspecifications result in different realisations of model 2.2 when they are modulated as factors in study 2 but are subsumed under one model here.
:::

**Experimental Design of simulation procedures**

**Study 1**

The study varied three main conditions: - Sample sizes: small ($N = 100$), moderate ($N = 400$), and large ($N = 6400$). - Indicator reliability: low ($\lambda = 0.3$), moderate ($\lambda = 0.5$), high ($\lambda = 0.7$). - Model specifications: correctly specified model and misspecified models (1.2, 1.3, and 1.4).

**Study 2**

The study varied five main conditions: - Sample sizes: small ($N = 100$), medium ($N = 400$), and large ($N = 6400$). - Variance explained by endogenous factors: low ($R^2 = 0.1$) and medium ($R^2 = 0.4$). - Indicator reliability: low ($\lambda = 0.3$), moderate ($\lambda = 0.5$), and high ($\lambda = 0.7$). - Model misspecifications: varying the population model by omitting a residual covariance and a factor cross-loading in different parts of the model. - Number of measurement blocks: separate measurement model per latent variable ($b = 5$) and joint measurement model for all exogenous variables ($b = 3$).

Both studies were conducted with 10000 replications for each condition.

**Method Selection**

**Study 1 and Study 2:** The performance of four estimation methods was compared: - Vanilla SEM. - Global SAM (gSAM). - Local SAM with maximum likelihood (lSAM-ML). - Local SAM with unweighted least squares (lSAM-ULS).

**Performance Measures**

The following performance measures were captured: - Convergence rates. - Empirical relative biases. - Empirical coverage levels of 95% confidence intervals (CIs). - Root Mean Squared Errors (RMSE).

**Software**

All analyses were conducted in @r_core_team_r_2023. See <https://github.com/valentinkm/AdversarialSimulation> for more details.

**Analysis and Interpretation plan**

The results were interpreted by descriptively comparing the performance measures (bias, MSE, convergence rates) of the different estimation methods under varying sample sizes, indicator reliability levels, and model misspecifications.

# Results

In the following the main patterns and trends of the results are reported and only examplary of select conditions are shown for each study to illustrate the main findings. The full results can be found in the supplementary materials.

## Study 1

### Convergence Rate

For moderate and large sample sizes ($N = 400$ and $N = 6400$), all methods achieved 100% convergence. At a small sample size ($N = 100$), SEM shows lower convergence rates, especially at lower reliability ($\lambda= 0.3$), while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates. SEM faces significant convergence drops at the lowest reliability ($\lambda=0.3$), particularly with misspecifications present.

```{r}
create_convergence_table_1 <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, ConvergenceRate) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Convergence Rate` = ConvergenceRate
    ) %>%
    mutate(
      `Convergence Rate` = round(`Convergence Rate`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Convergence Rate`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type in Study 1 with custom captions
convergence_table_1_1 <- create_convergence_table_1(0, "Convergence Rates under correct model specification (Model 1.1)", "1.1")
#convergence_table_1_1

convergence_table_1_2 <- create_convergence_table_1(0, "Convergence Rates under omitted Cross Loadings (Model 1.2)", "1.2")
#convergence_table_1_2

convergence_table_1_3 <- create_convergence_table_1(1, "Convergence Rates under omitted Correlated Residuals and structural misspecifcation (Model 1.3)", "1.3")
#convergence_table_1_3

convergence_table_1_4 <- create_convergence_table_1(0, "Convergence Rates under Structural Misspecification (Model 1.4)", "1.4")
#convergence_table_1_4

```

### Average Relative Biases

Here this study diverged from the original of @rosseel_structural_2022 by focusing exclusively on the average relative bias under a single type of correctly specified model (model 1), which does not include any cross loadings or correlated residuals. This simplification, allowed to concentrate on the core advantage of SAM over traditional SEM: its more robust estimation of structural parameters, especially when measurement models are misspecified. For the correct model (1.1), SEM showed higher biases at smaller sample sizes and lower reliability, while GSAM, SAM-ML, and SAM-ULS exhibited lower biases. Under omitted cross loadings (model 1.2) and structural missepcifcation (model 1.4) , all methods demonstrated substantial biases, particularly SEM, at smaller sample sizes and lower reliability. With omitted correlated residuals and structural misspecifcation (1.3), GSAM, SAM-ML, and SAM-ULS showed more negative biases compared to SEM. Overall, GSAM, SAM-ML, and SAM-ULS were more robust with lower biases compared to SEM, especially under challenging conditions with smaller sample sizes and lower reliability.\
Table 1 shows average relative bias percentages for different methods (SEM, GSAM, SAM-ML, SAM-ULS) under varying model specifications and sample sizes under under omitted correlated residuals (model 1.2) and Table 2 under correlated residuals and structural misspeifcation

```{r}
create_bias_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average Relative Bias (%)` = MeanRelativeBias
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average Relative Bias (%)` = round(`Average Relative Bias (%)` * 100, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average Relative Bias (%)`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the bias tables for each model type in Study 1 with custom captions
bias_table_1_1 <- create_bias_table(1, "Average Relative Biases (in Percentages) under correct model specification (Model 1.1)", "1.1")
#bias_table_1_1

bias_table_1_2 <- create_bias_table(2, "Average Relative Biases (in Percentages) under omitted Cross Loadings (Model 1.2)", "1.2")
bias_table_1_2

bias_table_1_3 <- create_bias_table(3, "Average Relative Biases (in Percentages) under omitted Correlated Residuals (Model 1.3)", "1.3")
bias_table_1_3

bias_table_1_4 <- create_bias_table(4, "Average Relative Biases (in Percentages) under Structural Misspecification (Model 1.4)", "1.4")
#bias_table_1_4

```

### Coverage

For the correct model (1.1), SEM exhibited undercoverage at smaller sample sizes and lower reliability, indicating narrow CIs and less reliable estimates. GSAM, SAM-ML, and SAM-ULS showed slight overcoverage, reflecting more conservative estimates. Under the cross loadings model (1.2), all methods had substantial undercoverage, especially SEM. The correlated residuals model (1.3) consistently revealed undercoverage across all methods, with SEM showing the highest negative biases. For the structural model (1.4), SEM suffered from undercoverage at lower sample sizes and reliability, while GSAM, SAM-ML, and SAM-ULS performed better with relatively consistent biases. Overall, GSAM, SAM-ML, and SAM-ULS demonstrated better empirical coverage than SEM but the overcoverage indicates too wide confidence intervals.\
Table 2 show the average difference between empirical coverage levels of the 95% confidence intervals (CIs) and their nominal level (95%) using each method for different reliability values and sample sizes under omitted cross loadings (model 1.2)

```{r}
create_coverage_diff_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanCoverage) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average Coverage Difference (%)` = MeanCoverage
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average Coverage Difference (%)` = round((`Average Coverage Difference (%)` - 0.95) * 100, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average Coverage Difference (%)`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the coverage difference tables for each model type in Study 1 with custom captions
coverage_diff_table_1_1 <- create_coverage_diff_table(1, "Average Coverage Difference (in Percentages) under correct model specification (Model 1.1)", "1.1")
#coverage_diff_table_1_1

coverage_diff_table_1_2 <- create_coverage_diff_table(2, "Average Coverage Difference (in Percentages) under omitted Cross Loadings (Model 1.2)", "1.2")
coverage_diff_table_1_2

coverage_diff_table_1_3 <- create_coverage_diff_table(3, "Average Coverage Difference (in Percentages) under omitted Correlated Residuals (Model 1.3)", "1.3")
#coverage_diff_table_1_3

coverage_diff_table_1_4 <- create_coverage_diff_table(4, "Average Coverage Difference (in Percentages) under Structural Misspecification (Model 1.4)", "1.4")
#coverage_diff_table_1_4
```

### RMSE

For the correct model (1.1), SEM had higher RMSE values at smaller sample sizes and lower reliability, while GSAM, SAM-ML, and SAM-ULS demonstrated lower RMSE values. Under omitted cross loadings (1.2) and correlated residuals (1.3), SEM showed significantly higher RMSE values, although all methods exhibited increased RMSE under these conditions. The structural model (1.4) revealed SEM had higher RMSE values at lower sample sizes and reliability, whereas GSAM, SAM-ML, and SAM-ULS showed better performance with lower RMSE. Overall, GSAM, SAM-ML, and SAM-ULS demonstrated more robustness with lower RMSE values compared to SEM, particularly in challenging conditions with smaller sample sizes and lower reliability.

```{r}
create_rmse_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average RMSE` = round(`Average RMSE`, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average RMSE`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the RMSE tables for each model type in Study 1 with custom captions
rmse_table_1_1 <- create_rmse_table(1, "Average RMSE under correct model specification (Model 1.1)", "1.1")
#rmse_table_1_1

rmse_table_1_2 <- create_rmse_table(2, "Average RMSE under omitted Cross Loadings (Model 1.2)", "1.2")
#rmse_table_1_2

rmse_table_1_3 <- create_rmse_table(3, "Average RMSE under omitted Correlated Residuals (Model 1.3)", "1.3")
#rmse_table_1_3

rmse_table_1_4 <- create_rmse_table(4, "Average RMSE under Structural Misspecification (Model 1.4)", "1.4")
#rmse_table_1_4
```

### Improper Solutions

Improper solutions occur when parameter estimates fall outside the boundary of the parameter space, such as when variances are estimated to be negative or correlations exceed an absolute value of one. These issues are more likely to arise in smaller sample sizes and can indicate problems with model estimation.\
SEM exhibited a significantly higher count of improper solutions compared to GSAM, SAM-ML, and SAM-ULS, particularly in small sample sizes and lower reliability conditions. GSAM, SAM-ML, and SAM-ULS consistently showed minimal improper solutions across all scenarios, highlighting their robustness and reliability in parameter estimation.

```{r}
create_improper_solutions_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, ImproperSolutionsCount) %>%
    mutate(ImproperSolutionsCount = ImproperSolutionsCount / 10000 * 100) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Improper Solutions Percentage` = ImproperSolutionsCount,
      `Method` = method
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Improper Solutions Percentage`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, digits = 2, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the improper solutions tables for each model type in Study 1 with custom captions
improper_solutions_table_1_1 <- create_improper_solutions_table(1, "Percentage of Improper Solutions under correct model specification (Model 1.1)", "1.1")
#improper_solutions_table_1_1

improper_solutions_table_1_2 <- create_improper_solutions_table(2, "Percentage of Improper Solutions under omitted Cross Loadings (Model 1.2)", "1.2")
#improper_solutions_table_1_2

improper_solutions_table_1_3 <- create_improper_solutions_table(3, "Percentage of Improper Solutions under omitted Correlated Residuals (Model 1.3)", "1.3")
#improper_solutions_table_1_3

improper_solutions_table_1_4 <- create_improper_solutions_table(4, "Percentage of Improper Solutions under Structural Misspecification (Model 1.4)", "1.4")
#improper_solutions_table_1_4

```

## Study 2

### Convergence

In Study 2, all methods achieved a 100% convergence rate for moderate and large sample sizes (N = 400, 6400). For small sample sizes (N = 100), SEM showed lower convergence rates, especially at lower reliability levels (0.3), while GSAM, SAM-ML, and SAM-ULS maintained high convergence rates. R-squared values and measurement block size had minimal impact on convergence rates, indicating robustness of GSAM, SAM-ML, and SAM-ULS across various conditions, whereas SEM struggled particularly with lower reliability and smaller sample sizes.

```{r}
create_convergence_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, R_squared, b, method, ConvergenceRate) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Convergence Rate` = ConvergenceRate
    ) %>%
    mutate(
      `Convergence Rate` = round(`Convergence Rate`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Convergence Rate`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type in Study 2 with custom captions
convergence_table_2_1 <- create_convergence_table(1, "Convergence Rates for correct model specification (Model 2.1)", "2.1")
#convergence_table_2_1

convergence_table_2_2_exo <- create_convergence_table(2, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#convergence_table_2_2_exo

convergence_table_2_2_endo <- create_convergence_table(3, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#convergence_table_2_2_endo

convergence_table_2_2_both <- create_convergence_table(4, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#convergence_table_2_2_both
```

### The Influence of Number of Measurement Blocks on Bias and RMSE

For low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), using 3 measurement blocks generally resulted in less negative bias compared to 5 blocks, although 5 blocks had lower RMSE. As reliability and sample sizes increased, the differences in bias and RMSE between 3 and 5 blocks diminished. Thus, for low reliability and small sample sizes, 3 measurement blocks were preferable for reducing bias, while 5 blocks performed better for RMSE. For subsequent comparisons, we considered lSAM with 3 measurement blocks as this difference was more pronounced especially in low sample size and reliability conditions.

```{r}
create_relative_bias_table_lSAM <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(method %in% c("lSAM_ML", "lSAM_ULS")) %>%  # Filter for lSAM methods
    select(N, reliability, R_squared, b, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Mean Relative Bias` = MeanRelativeBias
    ) %>%
    mutate(
      `Mean Relative Bias` = round(`Mean Relative Bias` * 100, 2),  # Convert to percentage and round to 2 decimal places
      Method = recode(Method, 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative Bias`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

relative_bias_table_lSAM_correct <- create_relative_bias_table_lSAM(5, "Mean Relative Bias for lSAM methods only under correct model specification (Model 2.1)", "2.1")
#relative_bias_table_lSAM_correct

relative_bias_table_lSAM_exo <- create_relative_bias_table_lSAM(6, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#relative_bias_table_lSAM_exo

relative_bias_table_lSAM_endo <- create_relative_bias_table_lSAM(7, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#relative_bias_table_lSAM_endo

relative_bias_table_lSAM_both <- create_relative_bias_table_lSAM(8, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#relative_bias_table_lSAM_both
```

```{r}
# Function to create an RMSE table for lSAM methods only
create_rmse_diff_table_lSAM <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(method %in% c("lSAM_ML", "lSAM_ULS")) %>%  # Filter for lSAM methods
    select(N, reliability, R_squared, b, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Mean Relative RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      `Mean Relative RMSE` = round(`Mean Relative RMSE`, 3),
      Method = recode(Method, 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative RMSE`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}
# Create and display the lSAM tables with custom captions
rmse_diff_table_lSAM_5 <- create_rmse_diff_table_lSAM(13, "Mean Relative RMSE for lSAM methods only under correct model specification (Model 2.1)", "2.1")
#rmse_diff_table_lSAM_5

rmse_diff_table_lSAM_6 <- create_rmse_diff_table_lSAM(14, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#rmse_diff_table_lSAM_6

rmse_diff_table_lSAM_7 <- create_rmse_diff_table_lSAM(15, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#rmse_diff_table_lSAM_7

rmse_diff_table_lSAM_8 <- create_rmse_diff_table_lSAM(16, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#rmse_diff_table_lSAM_8
```

### Average Relative Bias

For the correctly specified model with low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), traditional SEM showed a larger average relative bias compared to SAM methods. GSAM and lSAM-ML had small negative biases, while lSAM-ULS had a positive bias in challenging conditions. Higher $R^2$ had little impact on SEMs bias as absolute values, but SAM methods showed increased bias. For higher reliability and sample sizes, biases decreased for all methods.\
Under misspecification (correlated residuals and crossloadings in the exogenous model), SEM had an even greater positive bias in challenging conditions, while SAM methods showed consistent negative biases, with SAM-ULS performing best.\
In the endogenous misspecification model, SEM exhibited relatively small biases compared to SAM methods, which showed substantial negative biases. SEM's bias worsened with larger sample sizes and low reliability, while SAM methods' biases remained similarly large and negative.\
For misspecifications in both endogenous and exogenous parts, SAM methods had consistent negative biases. SEM showed a positive bias with low reliability and sample size, turning negative with increasing sample size and higher $R^2$ values.

```{r}
# Function to create a relative bias table for a specific model type with SEM performance included
create_relative_bias_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(b == 3) %>%  # Filter for Measurement Blocks = 3
    select(N, reliability, R_squared, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Method` = method,
      `Mean Relative Bias` = MeanRelativeBias
    ) %>%
    mutate(
      `Mean Relative Bias` = round(`Mean Relative Bias` * 100, 2),  # Convert to percentage and round to 2 decimal places
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative Bias`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
relative_bias_table_correct <- create_relative_bias_table(1, "Mean Relative Bias in Percentages for vanilla SEM and different SAM methods under correct model specification (Model 2.1)", "2.1")
# relative_bias_table_correct

relative_bias_table_exo <- create_relative_bias_table(2, "Mean Relative Bias in Percentages for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
# relative_bias_table_exo

relative_bias_table_endo <- create_relative_bias_table(3, "Mean Relative Bias in Percentages for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#relative_bias_table_endo

relative_bias_table_both <- create_relative_bias_table(4, "Mean Relative Bias in Percentages for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
relative_bias_table_both
```

```{r}
# # Function to create a coverage table for a specific model type with differences to SEM
# create_coverage_diff_table <- function(model_type_label, model_type_value) {
#   sem_values <- summary_2 %>%
#     filter(model_type == model_type_value, method == "SEM") %>%
#     select(N, reliability, R_squared, b, MeanCoverage) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Blocks` = b,
#       SEM_Value = MeanCoverage
#     )
#   
#   summary_2 %>%
#     filter(model_type == model_type_value, method != "SEM") %>%
#     select(N, reliability, R_squared, b, method, MeanCoverage) %>%
#     left_join(sem_values, by = c("N", "reliability" = "Reliability", "R_squared" = "R_Squared", "b" = "Measurement Blocks")) %>%
#     mutate(Diff_to_SEM = MeanCoverage - SEM_Value) %>%
#     select(N, reliability, R_squared, b, method, Diff_to_SEM) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Blocks` = b,
#       Method = method,
#       `Difference to SEM` = Diff_to_SEM
#     ) %>%
#     mutate(
#       `Difference to SEM` = round(`Difference to SEM`, 3),
#       Method = recode(Method, 
#                       gSAM = "GSAM", 
#                       lSAM_ML = "SAM-ML", 
#                       lSAM_ULS = "SAM-ULS")
#     ) %>%
#     pivot_wider(names_from = Method, values_from = `Difference to SEM`) %>%
#     kbl(caption = paste("Difference in Mean Coverage to SEM for (Mis-)Specification (Model):", model_type_label),
#         format = "html", booktabs = TRUE) %>%
#     kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# }
# 
# # Create and display the tables for each model type in Study 2
# coverage_diff_table_2_1 <- create_coverage_diff_table("2.1", "2.1")
# coverage_diff_table_2_1
# 
# coverage_diff_table_2_2_exo <- create_coverage_diff_table("2.2_exo", "2.2_exo")
# coverage_diff_table_2_2_exo
# 
# coverage_diff_table_2_2_endo <- create_coverage_diff_table("2.2_endo", "2.2_endo")
# coverage_diff_table_2_2_endo
# 
# coverage_diff_table_2_2_both <- create_coverage_diff_table("2.2_both", "2.2_both")
# coverage_diff_table_2_2_both
```

### RMSE

For the correctly specified model with low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), traditional SEM showed higher RMSE compared to SAM methods. GSAM and SAM-ML had lower RMSEs, while SAM-ULS had a slightly higher RMSE. Increasing regression weights ($R^2$) improved RMSE for both SEM and SAM methods. Under exogenous misspecifications, SEM had higher RMSE compared to SAM methods at low reliability and small sample size. This pattern persisted for endogenous misspecifications and misspecifications in both exogenous and endogenous factors, with GSAM and SAM-ML consistently showing lower RMSEs and SAM-ULS performing slightly worse. RMSE improved for all methods with higher $R^2$.

```{r}
# Function to create a RMSE table for a specific model type with SEM performance included
create_rmse_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(b == 3) %>%  # Filter for Measurement Blocks = 3
    select(N, reliability, R_squared, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Method` = method,
      `Mean Relative RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      `Mean Relative RMSE` = round(`Mean Relative RMSE`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative RMSE`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
rmse_table_correct <- create_rmse_table(1, "Mean Relative RMSE for vanilla SEM and different SAM methods under correct model specification (Model 2.1)", "2.1")
#rmse_table_correct

rmse_table_exo <- create_rmse_table(2, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#rmse_table_exo

rmse_table_endo <- create_rmse_table(3, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#rmse_table_endo

rmse_table_both <- create_rmse_table(4, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#rmse_table_both

```

### Improper Solutions

For small sample sizes ($N = 100$) and lower reliability ($\lambda = 0.3$), SEM exhibited a high percentage of improper solutions, while GSAM, SAM-ML, and SAM-ULS consistently showed very low or zero improper solutions across all conditions.

```{r}
# Function to create an Improper Solutions table for a specific model type
create_improper_solutions_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    group_by(N, reliability, R_squared, b, method) %>%
    summarise(
      ImproperSolutionsCount = sum(ImproperSolutionsCount, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    mutate(
      ImproperSolutionsPercentage = round((ImproperSolutionsCount / 10000) * 100, 2),
      R_squared = recode(R_squared, `0.1` = "low", `0.4` = "high")
    ) %>%
    select(N, reliability, R_squared, b, method, ImproperSolutionsPercentage) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Improper Solutions (%)` = ImproperSolutionsPercentage
    ) %>%
    mutate(
      Method = recode(Method,
                      SEM = "SEM",
                      gSAM = "GSAM",
                      lSAM_ML = "SAM-ML",
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Improper Solutions (%)`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
improper_solutions_table_1 <- create_improper_solutions_table(1, "Percentage of Improper Solutions for correct model specification (Model 2.1)", "2.1")
#improper_solutions_table_1

improper_solutions_table_2 <- create_improper_solutions_table(2, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#improper_solutions_table_2

improper_solutions_table_3 <- create_improper_solutions_table(3, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#improper_solutions_table_3

improper_solutions_table_4 <- create_improper_solutions_table(4, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#improper_solutions_table_4
```
