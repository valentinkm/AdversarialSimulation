The generalizability of Monte Carlo simulation studies is often compromised by researchers' degrees of freedom in operationalizing general claims, enabling implicit or explicit use of questionable research practices, leading to conflicting results and ambiguity. While common open science practices promote transparency and reproducibility, they do not fully address the challenge of operational and designing unbiased rigorous tests for general claims.
This thesis adapts the concept of adversarial collaboration from empirical research to simulation studies, introducing "adversarial simulation" as a method to reduce bias, enhance rigour and increase generalizability. As a case study, conflicting findings on the performance of Structural After Measurement (SAM) versus traditional Structural Equation Modeling (SEM) were examined. Two collaborators independently replicated prior studies supporting opposing views and then attempted a joint simulation study to reconcile these differences. The individual studies confirmed their respective original findings, with SAM outperforming SEM in some conditions and vice versa. Even though one collaborator terminated the collaboration, a joint study by the other revealed that SAM methods generally offered advantages over traditional SEM, particularly in handling model misspecifications in challenging conditions like small sample sizes and low indicator reliability. This process demonstrated that adversarial collaboration is a viable and practical approach for resolving disagreements and enhancing generalizability in simulation studies. Despite challenges such as increased time and effort and applicability to specific scenarios, adversarial simulation holds promise for improving the robustness and transparency of simulation-based statistical research.