\setlength{\parindent}{1.27cm}

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

#### Aims, objectives and research questions

Both studies aimed to evaluate the performance of traditional SEM with maximum likelihood (ML) compared to global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) under various conditions. The two research questions we jointly established prior to conducting the studies served as a general basis for both studies:

1. How do SAM and traditional system-wide SEM estimation compare in terms of bias, mean squared error (MSE), and convergence rates in small to moderate samples?
2. What is the impact of model misspecifications, such as residual correlations and cross-loadings, on the performance of SAM compared to traditional SEM methods?

#### Population Models and Data Generating Mechanism.

##### Study 1.

Data were generated based on a 5-factor population structural model with three indicators for each factor. Four different models were simulated (see Figure 2). In line with @rosseel_structural_2022, this model design was chosen to represent a realistic model with sufficient complexity to pose a challenge for the estimation methods, especially in the presence of misspecifications:

- Model 1.1: Correctly specified model.
- Model 1.2: Misspecified with cross-loadings in the population model that are ignored in the estimation model (model 1.1)
- Model 1.3: Misspecified with correlated residuals and a reversed structural path between the third and fourth latent factors in the population model that are ignored in the estimation model (model 1.1)
- Model 1.4: Misspecified with a bidirectional structural relation between factors 3 and 4 specified as only one directional

Factor loadings were fixed across all reliability conditions, with the first indicator of each factor serving as the scaling indicator ($\lambda$ = 1.0) and the other two indicators having loadings of 0.7. Indicator reliability levels were manipulated by adjusting the measurement error variances in the $\Theta$ matrix. Specifically, the reliability value was set at different levels (low = 0.3, moderate = 0.5 or high = 0.7) to compute the respective error variances on the diagonal of $\Theta$:
\begin{equation}
\Theta^{*} = \text{Var}(\eta)\Lambda^{T} \times \frac{1}{r - 1}
\label{eq:theta}
\end{equation}
where $\text{Var}(\eta)$ is the factor variance, $\Lambda^{T}$ is the transposed factor loading matrix, and $r$ is the reliability level.

To investigate additional possible and realistic scenarios beyond the ones studied by @rosseel_structural_2022, model 1.3 included a combination of measurement and structural misspecifications as opposed to only measurement misspecifications to introduce an even more severely misspecified model under which SAM methods might perform even better than traditional SEM. Further, model 1.4 included a (not estimated) bidirectional structural relation between factors 3 and 4 as opposed to the unidirectional reversed one. For all models, the population-level values of the structural parameters were set to 0.1.

::: {#fig-study1-models}
```{r}
#| layout: "[[1,1], [1.2,0.9]]"
#| out.width: "90%"
#| out.height: "90%"
knitr::include_graphics(c("figures/model1_1.tex", 
                          "figures/model1_2.tex", 
                          "figures/model1_3.tex", 
                          "figures/model1_4.tex"))
```

\raggedright

\textit{Note.} Error terms are not explicitly shown in the figure. Dashed lines represent relations omitted in the estimation model present in the population model. The figure is adapted from @rosseel_structural_2022.

Population Model Variations of Study 1
:::

##### Study 2.

Data were generated based on a 5-factor population structural model with three indicators for each factor with loadings set to 1, 0.9 and 0.8 for each factor and reliability modulated like in study 1. Regression weights were set to either 0.183 and 0.224 (low) or 0.365 and 0.447 (medium). This should represent varying variance explained ($R^2$) by the endogenous factors set at low ($R^2 = 0.1$) or medium ($R^2 = 0.4$). Note, however, that the computation of this was a simplification and does not accurately result in said $R^2$ values. The aim here was only to generally modulate between lower and higher regression weights. The population models resulted in the following model types with varying misspecification in the estimation model: (1) Structural misspecification with falsely specified paths in the estimation model absent in the population model. (2) correlated residuals and a factor cross-loading in either the exogenous, endogenous part of the model or both with falsely specified paths in the estimation model absent in the population model (see Figure 3). To enable the analysis of the impact of falsely specified paths in the estimation model that are not present in the population model and how well the different methods recover these non-existing relations, both population models included several such misspecifications in addition to the measurement misspecifications evaluated by @dhaene_evaluation_2023.

::: {#fig-study2-models}
```{r, echo=FALSE}
#| layout: "[[1,1]]"
#| out.width: "100%"
#| out.height: "100%"

knitr::include_graphics(c("figures/model2_1.tex", 
                          "figures/model2_2.tex"))
```

\raggedright

\textit{Note.} Error terms are not explicitly shown in the figure. Dotted paths represent relations specified in the estimation model not present in the population model. For the model on the right, orange lines represent misspecifications in the exogenous part of the model, and green lines represent misspecifications in the endogenous part. These types of misspecifications result in different realizations of the model when they are modulated as factors of misspecification (endogenous, exogenous or endo- and exogenous) in study 2 but are subsumed under one model here. The figure adapted from @dhaene_evaluation_2023.

Population Model Variations of Study 2
:::

#### Experimental Design.

##### Study 1.

Study 1 varied three main conditions: (1) sample sizes of small ($N = 100$), moderate ($N = 400$), and large ($N = 6400$); (2) Indicator reliability of low ($= 0.3$), moderate ($0.5$), high ($= 0.7$); (3) Model specifications: correctly specified model and misspecified with not specified cross-loadings in the population model misspecified with not-specified correlated residuals and a reversed structural path between the the third and the fourth latent factor in the population model and a recursive structural relation between factor 3 and 4 in the population specified as only one directional (see Figure 2).

##### Study 2.

Study 2 varied five conditions: (1) sample sizes: small ($N = 100$), medium ($N = 400$), and large ($N = 6400$). (2) Variance explained by endogenous factors: low ($R^2 = 0.1$) and medium ($R^2 = 0.4$). (3) Indicator reliability: low ($0.3$), moderate ($0.5$), and high ($0.7$). (4) Model misspecifications: varying the population model by omitting a residual covariance and a factor cross-loading in different parts of the model. (5) Number of measurement blocks: separate measurement model per latent variable ($b = 5$) and joint measurement model for all exogenous variables ($b = 3$) for the local SAM condition (lSAM-ML).

#### Method Selection.

Both studies compared the performance of four estimation methods: Traditional SEM with maximum likelihood (ML), Global SAM with maximum likelihood (gSAM), Local SAM with maximum likelihood (lSAM-ML), Local SAM with unweighted least squares (lSAM-ULS).

#### Performance Measures.

For both studies, convergence rates were tracked via lavaan's [@rosseel_lavaan_2012] built-in function, which indicates convergence. Further, improper solutions—converged models that showed negative variances (as the only type of improper solution present)—were tracked via lavaan warning messages. Next, for all converged and proper solutions, bias, for tracking systematic deviations, and root mean squared error for tracking overall accurcy were computed for structural parameter estimates were computed using the following equations [@megha_joshi_2024]:
\begin{equation}
\text{Bias} = \bar{T} - \theta
\label{eq:bias}
\end{equation}
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{K} \sum_{k=1}^{K} (T_k - \theta)^2}
\label{eq:rmse}
\end{equation}
where $T_k$ is the estimated parameter, $\bar{T}$ is the mean of the estimated parameters, $\theta$ is the true parameter value, and $K$ is the number of replications computed.
For comparability across varying regression weights for study 2, relative bias and relative RMSE were computed as:
\begin{equation}
\text{Relative Bias} = \frac{\bar{T} - \theta}{\theta}
\label{eq:rel_bias}
\end{equation}
\begin{equation}
\text{Relative RMSE} = \sqrt{\frac{(\bar{T} - \theta)^2 + S_T^2}{\theta^2}}
\label{eq:rel_rmse}
\end{equation}
where $S_T^2$ is the variance of the estimated parameters.
Monte Carlo standard errors (MCSE) were computed for bias, RMSE, relative bias, and relative RMSE using the following equations.
\begin{equation}
\text{MCSE}_{\text{Bias}} = \sqrt{\frac{S_T^2}{K}}
\label{eq:mcse_bias}
\end{equation}
\begin{equation}
\text{MCSE}_{\text{Relative Bias}} = \sqrt{\frac{S_T^2}{K\theta^2}}
\label{eq:mcse_rel_bias}
\end{equation}
\begin{equation}
\text{MCSE}_{\text{RMSE}} = \sqrt{\frac{K-1}{K} \sum_{j=1}^{K} \left( \text{RMSE}_{(j)} - \text{RMSE} \right)^2}
\label{eq:mcse_rmse}
\end{equation}
\begin{equation}
\text{MCSE}_{\text{Relative RMSE}} = \sqrt{\frac{K-1}{K} \sum_{j=1}^{K} \left( \text{rRMSE}_{(j)} - \text{rRMSE} \right)^2}
\label{eq:mcse_rel_rmse}
\end{equation}

#### Software.

The simulations were executed on a high-performance computing cluster available to the Max Planck Institute for Human Development Berlin (MPIB). All analyses were conducted in R (version 4.4) [@r_core_team_r_2023]. Main libraries included `lavaan` [@rosseel_lavaan_2012] for estimation and data generation, `furrr` [@davis_furrr_2022] for parallelization and `tidyverse`[@wickham_welcome_2019], `ggplot2` as well as `kableExtra` [@hao_zhu_2024] for results analysis and display. 
To ensure reproducibility and prevent seed synchronization in parallelized simulation runs, a pre-generated list of seeds was applied across all replications, and the simulations were containerized with Docker [@merkel2014docker]. Further details, full replication instructions, and a complete list of libraries and dependencies are available in [Appendix D](#appendix-d) or on [GitHub](https://github.com/valentinkm/AdversarialSimulation).

#### Analysis and Interpretation.

Similar to the studies by @rosseel_structural_2022 and @dhaene_evaluation_2023, results were interpreted by descriptively comparing the performance measures of the different estimation methods under varying sample sizes, indicator reliability levels, and model misspecifications. Performance metric values were aggregated across all parameters, excluding the misspecified parameters (present in the population but not in the estimation model).