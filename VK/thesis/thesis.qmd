---
Title: "Thesis"
author:
  - name: Valentin Kriegmair
    affiliations: "Humboldt-Universität zu Berlin"
fig-cap-location: top
format:
    pdf:
        fig-numbering: false
        fontsize: 11pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes: |
          \input{preamble.tex}
fontsize: 11pt
engine: knitr
bibliography: ../bibliography.bib
bibliographystyle: apa
appendix: true
---

```{r, include=FALSE}
library(knitr)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r child = 'plots.qmd'}
```

```{=tex}
% \includepdf[pages=-]{cover.pdf}
```

\begin{center} \section*{Abstract} \end{center}
\noindent
{{< include abstract.qmd >}}

\vspace{1cm}

```{r, results = 'asis'}
latest_sha <- Sys.getenv("LATEST_SHA")
cat("#### Document Version: \n")
cat(paste0("Generated using [AdversarialSimulation](https://github.com/valentinkm/AdversarialSimulation) in state of Git commit SHA ", latest_sha))
```

\newpage

```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
}
```
```{=tex}
\newpage
\tableofcontents
\newpage
```
```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
}
```
# Introduction

Karl Popper once described science "as the art of systematic over-simplification — the art of discerning what we may with advantage omit" [@popper1988open]. This pointedly encapsulates the fundamental cycle of empirical research. Researchers formulate general claims about the world, translate them into specific measurable constructs, select appropriate methods to collect data from specific populations, and finally update their beliefs about these general claims based on the gathered data. A core challenge in every research endeavor is this mapping from the general to the specific when designing and conducting a study and, conversely, from the specific & empirical back to the general when interpreting the results. This traversing between layers of abstraction is the crux of most research and it is where many if not all of a researcher's degrees of freedom lie. If there is ambiguity or lack of transparency in these mappings, irreproducible and ungeneralizable results [@earp_replication_2015; @camerer2018evaluating; @nosek2022replicability] as well as persistent and seemingly unresolvable disagreements on the general and verbal level can emerge.

This challenge of generalizability manifests uniquely in Monte Carlo simulation studies. They are widely utilized to assess the reliability and validity of statistical methods by testing their performance against controlled, simulated data, where the true parameter values are known. This approach allows researchers to investigate biases, variances, and other properties of estimators or models under various conditions, often informing best practices and model improvements [@thomopoulos2012essentials, @banack_monte_2021, @morris_using_2019]. These simulations, constrained by their specific settings, heavily depend on inductive reasoning to draw general conclusions, as it is impossible to simulate and test every conceivable combination of population and analysis model [@feinberg_conducting_2016]. Consequently, researchers face numerous degrees of freedom in deciding which prototypical models and settings to examine when assessing a method's general applicability and performance [@bollmann_what_2015, @kulinskaya_exploring_2020]. Hence, it becomes clear that simulation studies follow the same general research cycle as empirical studies, with similar challenges of generalizability ambiguity (see Figure \ref{fig:simulation-cycle}).

::: {fig-simulation-cycle}
```{r, echo=FALSE, fig.cap="The research cycle of simulation studies\\label{fig:simulation-cycle}"}
knitr::include_graphics("figures/research_cycle.pdf")
```

\raggedright \textit{Note.} Arrows represent the progressing from each stage of the research cycle to the next involving various decision making elements when moving from the general to the specific and back to the general. 
:::



\noindent To address these challenges, just like for empirical research, open science practices such as preregistration, transparency, and reproducibility have been proposed for simulation studies [@pawel_pitfalls_2023; @okelly_proposed_2017; @luijken_replicability_2023]. These practices are pivotal for increasing rigor and transparency in simulations and beyond. However, in the basic cyclical process of research as outlined above they mainly address the issue of generalizability at the point of moving from operationalization to data simulation, e.g. by enforcing transparency and reproducibility of the simulation, or at the point of verbally interpreting gathered data through adherence to preregistion protocols. Crucially however, these practices do not address the transition from the verbal claim to its operationalization. Decisions about how to operationalize, for example, which models and settings to choose and how to design the study remain subject to researchers' degrees of freedom and their (often implicit) biases [@buchka_optimistic_2021]. Furthermore, even rigorous and transparent studies may produce conflicting verbal claims stemming from ambiguities in operationalizations, and specific simulation setups. Such disagreements may not be readily resolvable, or only inefficiently so, by independently conducting and publishing simulation studies. Researchers' biases toward specific methods they have developed themselves may further amplify these divergences, affecting not only the interpretation of results but also choices in design of simulations studies [@buchka_optimistic_2021].

For emprical research, Adversarial Collaboration (AC) has been proposed as an approach to address ambiguity and improve generalizability throughout the research cycle, particularly in situations of entrenched disagreements between researchers and theories. Pioneered by Ralph Hertwig and Daniel Kahneman and in their attempt to resolve a debate on frequency representation involving Barbara Mellers as a neutral arbiter [@mellers_frequency_2001] it has since been recognized for its potential wihtin the open science movement [@clark_keep_2022; @rakow_adversarial_2022; @clark_adversarial_2021]. Unlike standard open science practices, which may not account for researchers' degrees of freedom in hypothesis generation and operationalization, AC allows for the detection and reduction of biased methodological decisions. In AC, opposing researchers, first identify general verbal theoretical disputes, agree on a shared research question that could settle the debate, collaboratively design studies they agree to have the potential to change their minds and jointly publish the results regardless of the outcome [@melloni_making_2021]. This process aims to unveil and concretize even subtle discrepancies in methodological assumptionns and decisions as well as framing of conclusions [@clark_keep_2022], thus tracing back general and verbal disagreements of conflicting theories to their specific and empirical roots reducing ambiguity and increasing generalizability by generating a shared language of assumptions and operationlizations. Hence, it promises to not only enhance rigor and transparency but importantly also reduce ambiguity and bias at the stage of operationalization and design of studies [@clark_adversarial_2021].
In this project, we aimed to adapt this concept of AC from empirical research to Monte Carlo simulation studies, examining the feasibility and viability of such an *Adversarial Simulation* (AS).
As a substantive test case, we focused on recent findings regarding the performance of the Structural After Measurement (SAM) — a method for Structural Equation Model (SEM) estimation recently reintroduced by @rosseel_structural_2022.

Structural equation modeling (SEM) encompasses a diverse range of statistical techniques frequently applied in the social and behavioral sciences [@bollen2014structural; @hoyle2012handbook]. SEM is most commonly employed to study models that incorporate both measurement and structural components. The measurement model describes the relationships between latent variables and their observed indicators, while the structural model specifies the relationships among the latent variables themselves often reflecting substantive theoretical constructs of interest.

Traditional SEM estimation methods, like maximum likelihood estimation, optimize all parameters of a model simultaneously (under the assumption of multivariate normality) by minimizing a discrepancy function $ F(\theta) $, where $ \theta $ represents all parameters of both the measurement and structural models.

While powerful this system wide estimation suffers from several shortcomings such as non-convergence, improper solutions (with solutions including parameters out of their definitional range such as negative variances [@van1978various]), as well as poor model fit and estimation biases arising from local measurement misspecifications that can affect the entire model.

They also typically require large sample sizes for adequate performance, especially in complex models with many parameters.

To address these issues, SAM separates the estimation process into two distinct stages.  First, the measurement model parameters are estimated independently to capture the relationships between latent variables ($\eta$) and their observed indicators ($y$), represented by:
$$
y = \mu + \Lambda \eta + \epsilon,
$$

where $\mu$ is a vector of intercepts, $\Lambda$ is the factor loading matrix, and $\epsilon$ denotes measurement errors.

In the second stage, structural model parameters are estimated using the latent variable estimates from the first stage, modeled as:

$$
\eta = \alpha + B \eta + \zeta,
$$

with $\alpha$ as a vector of intercepts, $B$ as the matrix of structural coefficients, and $\zeta$ as structural disturbances.
@rosseel_structural_2022 proposes two distinct approaches to SAM estimation:
(1) Local SAM constructs estimators for latent variable means and covariances from first-stage estimates and applies them directly in second-stage analyses (e.g., linear regression). Expected values $E(\eta)$ and covariance $\text{Var}(\eta)$ are derived as:

$$
E(\eta) = M(y - \mu),
$$
$$
\text{Var}(\eta) = M(S - \Theta)M^T,
$$

where $M$ is derived from factor loadings and measurement error covariances, $S$ is the sample covariance matrix of $y$, and $\Theta$ is the covariance matrix of $\epsilon$.
(2) Global SAM keeps first-stage measurement model parameters fixed while estimating structural model parameters using standard SEM techniques.

Recent studies by @rosseel_structural_2022 and @dhaene_evaluation_2023 have shown that SAM can outperform traditional SEM estimation in the presence of model misspecifications, especially in small to moderate sample sizes. However, these findings have been challenged by @robitzsch_comparing_2022, who argues that SAM may systematically underestimate parameters in the presence of negative misspecifications. This disagreement highlights the need for a systematic evaluation of SAM's performance in the presence of model misspecifications in small to moderate sample sizes.

To create suitable testing grounds for *Adversarial Simulation*, these diverging claims served as the starting point for an adversarial collaboration between a fellow student researcher (Collaborator B, Kosanke) and I (Collaborator A, Kriegmair), each representing one of the above sides of the differing findings. We developed a basic framework to tailor the concept of AC to simulation studies and facilitate a structured and systematic conduct and evaluation of the AC process. The framework, outlined in detail in the following section, consisted of two round: In the first round, each collaborator would independently conduct a separate simulation study. In the second round, we planned to collaboratively design and conduct a study. This two-stage approach waas designed to highlight differences between the individual approaches in a systematic way and to establish an virtual foundation for collaboration before engaging in a joint effort in our case study. As the first step of this process we jointly formulated two *specific* research questions based on above mentioned conflicting claims:


1. How do SAM and traditional SEM methods (including ML and ULS) compare in terms of bias, Mean Squared Error (MSE), and convergence rates in small to moderate samples? 
2. What is the impact of model misspecifications, such as residual correlations and cross-
loadings, on the performance of SAM compared to traditional SEM methods?

Finally by conducting this case study we thus aimed to answer our *general* meta-research question:
Is adversarial collaboration a viable and practical applicable tool to resolve disagreeing research claims and enhance generalizability and rigor in the context of simulation studies?

# Methods

## A Framework for Adversarial Collaboration

We developed a specific adversarial simulation framework and structured the collaboration into two rounds. In the first round, each collaborator independently conducts a separate simulation study. In the second round, they come together to work on a joint study, building on the findings from the first round. This two-step approach is designed to highlight differences in a systematic way and to establish a virtual foundation for collaboration before engaging in a joint effort in our case study.

## Individual Simulation Studies

### Studies by Collaborator A (Kriegmair)

The methodological setup of my individual simulation studies follows the structure we established for our *adversarial simulation* framework to facilitate stepwise collaboration. It is based on a preregistered protocol but includes some deviations from the preregistration ([See Appendix A](#appendix-a) for the full protocol and all deviations from the preregistration). In the initial phase of our case study, I independently conducted two separate simulation studies without my collaborator's involvement with the goal to conceptually replicate the findings regarding SAM compared to standard SEM estimation of @rosseel_structural_2022 and @dhaene_evaluation_2023. However, there are several differences in the design and setup of the studies compared to the original studies as outlined below.

{{< include methods.qmd >}}

### Studies by Collaborator B (Kosanke)

{{< include methods_kosanke.qmd >}}

## Joint Simulation Study

{{< include methods_joint.qmd >}}

# Results

## Individual Simulation Studies

### Results of Collaborator A (Kriegmair)

{{< include results.qmd >}}

### Results of Collaborator B (Kosanke)

{{< include results_kosanke.qmd >}}

## Joint Simulation Study

{{< include results_joint.qmd >}}

# Discussion

The goal of this study was two-fold: First, to test the viability and practical applicability of adversarial collaboration (AC) as a tool to resolve disagreeing research claims and enhance generalizability and rigor in the context of simulation studies. Second, serving as a case study for this, to evaluate the performance of traditional Structural Equation Modeling (SEM) compared to Structural After Measurement (SAM) and resolve the conflicting claims of previous studies whether SAM consitently outperforms traditional SEM in the presence of model misspecifications in small to moderate sample sizes [@robitzsch_comparing_2022, @rosseel_structural_2022, @dhaene_evaluation_2023].

We successfully agreed on a joint starting point and translated conflicting verbal claims from prior studies into shared research questions. Based on these research questions, we independently conducted simulation studies largely based on the simulations by Rosseel (2022) and Dhaene and Rosseel (2023), successfully translating the verbal dispute back into the empirical domain. It is important to note that this constituted only an emulated process of adversarial collaboration (AC), including an additional layer of abstraction through replication of previously published research findings. In a practical application of AC to simulation studies as proposed here, this intermediary step could be bypassed. Instead, collaborators could design two original studies or choose to work directly together on a unified research study, contingent upon their identification of a specific verbal disagreement. Third, after assessing our individual studies and their results, we did not jointly conclude that conducting a collaborative unified simulation study as planned was warranted. Kosanke argued that while in most cases the Structural After Measurement (SAM) approach showed less bias and root mean square error (RMSE) in some settings—especially in cases of negative unmodeled residuals and cross-loadings—the advantages of traditional Structural Equation Modeling (SEM) countered those of SAM, indicating that neither method consistently outperformed the other in broader applications. However, I identified several reasons for conducting another simulation based on this first round of replicated studies and, based on this, set up a *joint* study. Kosanke's conclusion about SAM's inconsistent outperformance of SEM only in the presence of negative misspecifications was applied to a very specific type of confirmatory factor analysis (CFA) model and was not tested in a more complex model with directed structural paths of interest. These represent scenarios for which @rosseel_structural_2022 proposed SAM to be advantageous. In addition, to thoroughly investigate this assumed systematic underestimation of SAM, a parameter-wise analysis of bias was warranted. Aggregation of bias values across model parameters could lead to canceling out negative and positive values or not showing them at all when using absolute values. Furthermore, a joint study allowed for unifying simulation choices, such as extending the sample range to very small sizes (*N* = 50) to examine more extreme settings. Finally, collaborator-specific choices of tracking convergence rates and computing modulated indicator reliability levels could be identified as another potential source of diverging results, which was resolved in the joint study.

Idea: living simulations..

# References

::: {#refs}
:::

# Appendix {.appendix}

## Appendix A: Simulation Protocol {#appendix-a}

Here the full simulation protocol of my simulation studies conducted individually prior to collaboration as well as the follow up study I conducted in light of the collaboration with Kosanke after the first round of conducting and evaluating our individual studies is presented. It is based on the preregistration of my individual studies [@kriegmair_preregistration_2024] and outlines all deviations from it.

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

{{< include appendix_a.qmd toc=false >}}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

## Appendix B: Supplementary Figures

```{=tex}
\renewcommand{\thefigure}{B\arabic{figure}}
\setcounter{figure}{0}
```
{{< include appendix_b.qmd toc=false >}}

## Appendix C: Detailed Error and Warning Messages

```{=tex}
\renewcommand{\thetable}{C\arabic{table}}
\setcounter{table}{0}
```
In the following, all different warning and error messages raised during the studies are listed (see Table C1) and shown how often they occurred under various fitting conditions (see Table C2).

{{< include appendix_c.qmd toc=false >}}