---
Title: "Thesis"
author:
  - name: Valentin Kriegmair
    affiliations: "Humboldt-Universit√§t zu Berlin"
fig-cap-location: top
format:
    pdf:
        fig-numbering: false
        fontsize: 11pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes: |
          \input{preamble.tex}
fontsize: 11pt
engine: knitr
bibliography: ../bibliography.bib
csl: ../apa.csl
appendix: true
---
```{r, include=FALSE}
library(knitr)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```
```{r child = 'plots.qmd'}
```

```{=tex}
% \includepdf[pages=-]{cover.pdf}
```

\begin{center} \section*{Abstract} \end{center}
\noindent
{{< include abstract.qmd >}}

\vspace{1cm}

```{r, results = 'asis'}
latest_sha <- Sys.getenv("LATEST_SHA")
cat("#### Document Version: \n")
cat(paste0("Generated using [AdversarialSimulation](https://github.com/valentinkm/AdversarialSimulation) in state of Git commit SHA ", latest_sha))
```

\newpage

```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
}
```

```{=tex}
\newpage
\tableofcontents
\newpage
```

```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
}
```

# Introduction

## The Generalizability Challange

Karl Popper (1957) described science as "the art of systematic over-simplification." This phrase ironically yet accurately encapsulates the fundamental cycle of empirical research. Researchers formulate general hypotheses about the world, translate them into measurable constructs, select appropriate methods to collect data from specific populations, and finally update their beliefs about these general claims based on the gathered data. A core challenge in every research endeavor is mapping the general to the specific when designing and conducting a study and, conversely, mapping the specific empirical findings back to the general domain when interpreting results. This mapping process is central to research and is where many if not all of a researcher's degrees of freedom lie. When there is ambiguity in these mappings or they lack transparency, irrepleicable and ungeneralizable results as well as persistent and seemingly unresolvable disagreements on the general and verbal level can emerge.

This challenge of generalizability manifests uniquely in Monte Carlo simulations. These simulations are commonly used tools to test statistical methods by evaluating them against known ground truths in simulated data. Simulation studies occupy a unique position because they are not directly tied to empirical data but instead operate within a hypothetical world of data generation. [this sentence needs to be worked out]

One perspective views simulations as theoretical studies, akin to mathematical proofs (@beisbart_why_2012), and, one might conclude, thus not subject to the same generalizability and replicability issues as empirical studies. However, as simulations are limited to very specific settings they heavily rely on inductive reasoning to draw general conclusions. Due to this impossibiltiy to simulate and test every possible combination of data and statistical method, researchers face numerous degrees of freedom in deciding which prototypical models and settings to test to draw conclusions about a method's general applicability and performance. From this it becomes clear that simulation studies follow the same general research cycle as empirical studies, with similar challenges of generalizability and replicability.

This generalizability challenge has been recognized in simulation studies. and open science practices such as preregistration, transparency, and reproducibility have been proposed as remedy. These practices are pivotal for increasing rigor and transparency in simulations and beyond. However, in the basic cyclical process of research as outlined above they mainly address the issue of generalizability at the point of moving from operationalization to data simulation, e.g. by enforcing transparancy and reproducibility of the simulation, or at the point of verbally interpreting gathered data through preregistered analysis plans.
Crucially howecer, these practices do not address the transition from the verbal claim to its operationalization. Decisions about how to operationalize, for example, which models and settings to choose and how to design the study remain subject to researchers' degrees of freedom and their (often implicit) biases. Furthermore, even rigorous and transparent studies can result in divergent claims due to ambiguities in the mapping between verbal claims, operationalizations, and specific simulation setups. These divergences may not be readily resolvable, or only inefficiently so, by independently conducting and publishing simulation studies.

::: {fig-simulation-cycle}
```{r fig-simulation-cycle, echo=FALSE, fig.cap="The research cycle of simulation studies."}
knitr::include_graphics("figures/research_cycle.pdf")
```

:::{fig-note}
:::
:::


Comparisons of different methods across various research settings are especially prone to conflicting claims due to divergent simulation decisions. Researchers' biases toward specific methods they have developed may further amplify these divergences, affecting not only the interpretation of results but also the design of simulations.



## Adversarial Collaboration

To address these challenges of entrenched disagreements, the practice of AC has been proposed to unveil discrepancies in underlying methodological decisions and assumptions. It was famously pioneered by Ralph Hertwig and Daniel Kahneman, who tried to settle a persistent scientific disagreement about frequency representation and consulted Barbara Mellers as a neutral arbiter. Today, it is recognized as a potent tool in the social empirical research community. The basic idea is for two researchers in disagreement to first identify a general verbal dispute and agree on a research question to settle the debate. Based on this, they collaboratively work on operationalizing, testing, and interpreting this verbal claim. This process aims to unveil and concretize underlying disagreements and thus reduce ambiguity and increase generalizability. In this project, we aimed to transfer the concept of AC from the empirical domain to Monte Carlo simulation studies and assess its feasibility and viability in a case study in this context. To conduct such an exemplary AC, we first need a framework that structures the collaborative process tailored to the outline of simulation studies. 

## SAM vs. SEM - a Case Study

Traditional SEM methods, like maximum likelihood estimation, optimize all parameters of a model simultaneously under the assumption of multivariate normality. While powerful and although robust estimation techniques relax the normality assumption, all system wide estimators suffer from several shortcomings, they often face issues such as non-convergence, improper solutions (with parameters out of definitional range), and biases from local measurement misspecifications that affect the entire model. They also typically require large sample sizes for adequate performance, especially in complex models.
 
To conduct an AC in the context of simulation studies...

# Methods

## A Framework for Adversarial Collaboration

We developed a specific adversarial simulation framework and structured the collaboration into two rounds. In the first round, each collaborator independently conducts a separate simulation study. In the second round, they come together to work on a joint study, building on the findings from the first round. This two-step approach is designed to highlight differences in a systematic way and to establish a virtual foundation for collaboration before engaging in a joint effort in our case study.

## Individual Simulation Studies

### Studies by Collaborator A (Kriegmair)

The methodological setup of my individual simulation studies follows the structure we established for our *adversarial simulation* framework to facilitate stepwise collaboration. It is based on a preregistered protocol but includes some deviations from the preregistration ([See Appendix A](#appendix-a) for the full protocol and all deviations from the preregistration). In the initial phase of our case study, I independently conducted two separate simulation studies without my collaborator's involvement with the goal to conceptually replicate the findings regarding SAM compared to standard SEM estimation of @rosseel_structural_2022 and @dhaene_evaluation_2023. However, there are several differences in the design and setup of the studies compared to the original studies as outlined below.

{{< include methods.qmd >}}

### Studies by Collaborator B (Kosanke)

{{< include methods_kosanke.qmd >}}

## Joint Simulation Study

{{< include methods_joint.qmd >}}

# Results

## Individual Simulation Studies

### Results of Collaborator A (Kriegmair)

{{< include results.qmd >}}

### Results of Collaborator B (Kosanke)

{{< include results_kosanke.qmd >}}

## Joint Simulation Study

{{< include results_joint.qmd >}}

# Discussion

The goal of this study was two-fold: First, to test the viability and practical applicability of adversarial collaboration (AC) as a tool to resolve disagreeing research claims and enhance generalizability and rigor in the context of simulation studies. Second, serving as a case study for this, to evaluate the performance of traditional Structural Equation Modeling (SEM) compared to Structural After Measurement (SAM) and resolve the conflicting claims of previous studies whether SAM consitently outperforms traditional SEM in the presence of model misspecifications in small to moderate sample sizes (@robitzsch_comparing_2022, @rosseel_structural_2022, @dhaene_evaluation_2023).

We successfully agreed on a joint starting point and translated conflicting verbal claims from prior studies into shared research questions. Based on these research questions, we independently conducted simulation studies largely based on the simulations by Rosseel (2022) and Dhaene and Rosseel (2023), successfully translating the verbal dispute back into the empirical domain.
It is important to note that this constituted only an emulated process of adversarial collaboration (AC), including an additional layer of abstraction through replication of previously published research findings. In a practical application of AC to simulation studies as proposed here, this intermediary step could be bypassed. Instead, collaborators could design two original studies or choose to work directly together on a unified research study, contingent upon their identification of a specific verbal disagreement.
Third, after assessing our individual studies and their results, we did not jointly conclude that conducting a collaborative unified simulation study as planned was warranted. Kosanke argued that while in most cases the Structural After Measurement (SAM) approach showed less bias and root mean square error (RMSE) in some settings‚Äîespecially in cases of negative unmodeled residuals and cross-loadings‚Äîthe advantages of traditional Structural Equation Modeling (SEM) countered those of SAM, indicating that neither method consistently outperformed the other in broader applications.
However, I identified several reasons for conducting another simulation based on this first round of replicated studies and, based on this, set up a *joint* study. Kosanke's conclusion about SAM's inconsistent outperformance of SEM only in the presence of negative misspecifications was applied to a very specific type of confirmatory factor analysis (CFA) model and was not tested in a more complex model with directed structural paths of interest. These represent scenarios for which @rosseel_structural_2022 proposed SAM to be advantageous.
In addition, to thoroughly investigate this assumed systematic underestimation of SAM, a parameter-wise analysis of bias was warranted. Aggregation of bias values across model parameters could lead to canceling out negative and positive values or not showing them at all when using absolute values. Furthermore, a joint study allowed for unifying simulation choices, such as extending the sample range to very small sizes (*N* = 50) to examine more extreme settings. Finally, collaborator-specific choices of tracking convergence rates and computing modulated indicator reliability levels could be identified as another potential source of diverging results, which was resolved in the joint study.

Idea: living simulations..

# References

::: {#refs}
:::

# Appendix {.appendix}

## Appendix A: Simulation Protocol {#appendix-a}

Here the full simulation protocol of my simulation studies conducted individually prior to collaboration as well as the follow up study I conducted in light of the collaboration with Kosanke after the first round of conducting and evaluating our individual studies is presented. It is based on the preregistration of my individual studies (@kriegmair_preregistration_2024) and outlines all deviations from it.

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

{{< include appendix_a.qmd toc=false >}}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

## Appendix B: Supplementary Figures

```{=tex}
\renewcommand{\thefigure}{B\arabic{figure}}
\setcounter{figure}{0}
```
{{< include appendix_b.qmd toc=false >}}

## Appendix C: Detailed Error and Warning Messages

```{=tex}
\renewcommand{\thetable}{C\arabic{table}}
\setcounter{table}{0}
```
In the following, all different warning and error messages raised during the studies are listed (see Table C1) and shown how often they occurred under various fitting conditions (see Table C2).

{{< include appendix_c.qmd toc=false >}}