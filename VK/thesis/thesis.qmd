---
Title: "Thesis"
author:
  - name: Valentin Kriegmair
    affiliations: "Humboldt-Universität zu Berlin"
fig-cap-location: top
format:
    pdf:
        fig-numbering: false
        fontsize: 11pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes: |
          \input{preamble.tex}
fontsize: 11pt
engine: knitr
bibliography: ../bibliography.bib
bibliographystyle: apa.csl
appendix: true
---
\setlength{\parindent}{1.27cm}
```{r, include=FALSE}
library(knitr)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r child = 'plots.qmd'}
```

```{=tex}
%\includepdf[pages=-]{cover.pdf}
```

\begin{center} \section*{Abstract} \end{center}
\noindent
{{< include abstract.qmd >}}

\vspace{1cm}

```{r, results = 'asis'}
latest_sha <- Sys.getenv("LATEST_SHA")
cat("#### Document Version: \n")
cat(paste0("Generated using [AdversarialSimulation](https://github.com/valentinkm/AdversarialSimulation) in state of Git commit SHA ", latest_sha))
```

\newpage

```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
}
```
```{=tex}
\newpage
\tableofcontents
\newpage
```
```{=tex}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
}
```
# Introduction

## The Challenge of Generalizability in Simulation Studies

Karl Popper once described science "as the art of systematic over-simplification — the art of discerning what we may with advantage omit" [@popper1988open]. 
This pointedly encapsulates the fundamental cycle of empirical research. 
Researchers formulate general claims about the world, translate them into specific, measurable constructs, select appropriate methods to collect data from specific populations, and finally update their beliefs about these general claims based on the gathered data [@supino_overview_2012]. 
A core challenge in every research endeavour is this mapping from the general to the specific when designing and conducting a study and, conversely, from the specific and empirical back to the general when interpreting the results. 
This traversing between the abstract and concrete is the crux of most research, and it is where many, if not all, researchers degrees of freedom lie [@dellsen2021disagreement; @carrillo_scientific_2023; @martinez2011epistemic].
If these mappings are ambiguous or lack transparency due to explicit or implicit questionable research practices, irreproducible and ungeneralizable results [@earp_replication_2015; @camerer2018evaluating; @nosek2022replicability; @mccarley2023open] as well as persistent and seemingly unresolvable disagreements on the general and verbal level can emerge [@dellsen2021disagreement; @cleeremans2022theory].
This fundamental challenge of generalizability - i.e. the extent to which research findings apply beyond the specific context of a study [@yarkoni2022generalizability] - manifests uniquely in Monte Carlo simulation studies. These are widely utilized tools to assess the reliability and validity of statistical methods by testing their performance against controlled, simulated data, where the true population parameter values (of the data generating mechanism) are known. 
This approach allows researchers to investigate biases, variances, and other properties of estimators or models under various conditions, often informing best practices and model improvements [@thomopoulos2012essentials; @banack_monte_2021; @morris_using_2019]. 
These simulations, constrained by their specific settings, heavily depend on inductive reasoning to draw general conclusions, as it is impossible to simulate and test every conceivable combination of population and analysis model [@feinberg_conducting_2016; @gilbert2024multilevel]. 
Consequently, researchers face numerous degrees of freedom in deciding which prototypical models and settings to examine when assessing a method's general applicability and performance [@bollmann_what_2015; @kulinskaya_exploring_2020]. 
Hence, it becomes clear that simulation studies follow the same general research cycle as empirical studies, facing similar challenges of generalizability and ambiguity, where valid and reproducible mappings between each stage are pivotal for ensuring generalizability (see Figure \ref{fig:simulation-cycle}).

::: {fig-simulation-cycle}
```{r, echo=FALSE, fig.cap="The Basic Cycle of Research (in simulation studies) \\label{fig:simulation-cycle}"}
knitr::include_graphics("figures/research_cycle.pdf")
```

:::{fig-note}
\raggedright \textit{Note.} The dashed red arrow from *Verbal Claim* to *Operationalization* indicates translational validity as the mapping from the general to the specific not directly addressed by most open science practices.
:::
:::
\vspace{0.5cm}
\setlength{\parindent}{1.27cm}

To address these challenges, just like for empirical research, open science practices such as preregistration, 
open data, and reproducibility have been proposed for simulation studies [@pawel_pitfalls_2023; 
@okelly_proposed_2017]. These practices are pivotal for increasing rigor and transparency in simulations and beyond. 
However, in the basic cyclical process of research, as outlined above, they address the issue of generalizability only indirectly by promoting scientific rigor at the point of moving from operationalization to data simulation, e.g. by enforcing transparency and 
reproducibility of the simulation [@luijken_replicability_2023], or at the point of verbally interpreting gathered 
data through adherence to preregistration protocols. Crucially, however, these practices do not address the transition 
from the verbal claim to its operationalization (See \ref{fig:simulation-cycle}), also referred to as *translational validity* [@slife2016using; @clark_keep_2022]. Decisions about operationalization, for example, which models and simulation settings to choose remain subject to researchers degrees of freedom and their (often implicit) biases [@buchka_optimistic_2021; @flake2020measurement]. This can produce conflicting verbal claims and disagreements that may not be readily 
resolvable, or only inefficiently so, by independently conducting and publishing simulation studies that may require extensive review articles to inform a broader audience of researchers [@hamaker2023within].
Researchers biases toward specific methods they have developed themselves may further amplify these divergences, affecting not 
only the interpretation of results but crucially also choices in the design of simulation studies [@buchka_optimistic_2021].

## Adversarial Collaboration

For empirical research, Adversarial Collaboration (AC) has been proposed to address ambiguity and improve 
generalizability throughout the research cycle in case of entrenched disagreements between researchers and theories. 
Pioneered by @latham1988resolving and @mellers_frequency_2001 it has since been recognized for its potential within the open science movement [@clark_keep_2022; @rakow_adversarial_2022; @clark_adversarial_2021]. Unlike other open science practices, which may not account for researchers degrees of 
freedom in hypothesis generation and operationalization, AC allows for detecting and reducing biased methodological 
decisions. In AC, opposing researchers first identify general verbal theoretical disputes, agree on a shared research 
question that could settle the debate, collaboratively design studies they agree to have the potential to change their minds and jointly publish the results regardless of the outcome [@melloni_making_2021; @latham1988resolving; @kahneman2009conditions]. 
This process aims to unveil and concretize even subtle discrepancies in methodological assumptions, decisions and in the framing of conclusions [@clark_keep_2022]. 
By tracing general and verbal disagreements back to their specific empirical origins, it fosters a shared language of assumptions and operationalizations, reducing ambiguity and enhancing generalizability.
Hence, it promises to enhance rigor and transparency and, importantly, reduce ambiguity and bias at the operationalization and design stage of studies [@clark_adversarial_2021].

## SAM vs. SEM - A Case Study of *Adversarial Simulation*

In this project, we aimed to adapt this concept of AC from empirical research to Monte Carlo simulation studies, 
examining the feasibility and viability of such an *Adversarial Simulation*.
As a substantive test case, we focused on recent conflicting findings regarding the performance of Structural After 
Measurement (SAM) — a method for Structural Equation Model (SEM) estimation recently reintroduced by @rosseel_structural_2022.

Structural equation modelling (SEM) encompasses various statistical techniques frequently applied in the social and 
behavioural sciences [@bollen2014structural; @hoyle2012handbook]. SEM is most commonly employed to study models 
incorporating measurement and structural components. The measurement model describes the relationships between latent variables and their observed indicators, while the structural model specifies the relationships among the latent variables themselves, often reflecting substantive theoretical constructs of interest [@hair2021introduction]. Traditional SEM estimation methods, like maximum likelihood estimation, optimize all parameters of a model simultaneously (under the assumption of multivariate normality) by minimizing a discrepancy function $F(\theta)$, where $\theta$ represents all parameters of both the measurement and structural models [@kline2023principles]. While powerful, this system-wide estimation suffers from several shortcomings, such as non-convergence, improper solutions (with solutions including parameters out of their definitional range, such as negative variances [@van1978various]), poor model fit, and estimation biases arising from local measurement misspecifications that can affect the entire model. They also typically require large sample sizes for adequate performance, especially in complex models with many parameters [@rosseel2020small]. SAM - as proposed by @rosseel_structural_2022 - addresses these issues and separates the estimation process into two 
distinct stages. First, the measurement model parameters are estimated independently to capture the relationships 
between latent variables ($\eta$) and their observed indicators ($y$), represented by:
\begin{equation}
y = \mu + \Lambda \eta + \epsilon,
\label{eq:measurement-model}
\end{equation}
where $\mu$ is a vector of intercepts, $\Lambda$ is the factor loading matrix, and $\epsilon$ denotes measurement errors.
In the second stage, structural model parameters are estimated using the latent variable estimates from the first stage, modelled as:
\begin{equation}
\eta = \alpha + B \eta + \zeta,
\label{eq:structural-model}
\end{equation}
with $\alpha$ as a vector of intercepts, $B$ as the matrix of structural coefficients, and $\zeta$ as structural disturbances.
@rosseel_structural_2022 proposes two distinct approaches to SAM estimation: (1) Local SAM constructs estimators for latent variable means and covariances from first-stage estimates and applies them directly in second-stage analyses (e.g., linear regression). Expected values $E(\eta)$ and covariance $\text{Var}(\eta)$ are derived as:
\begin{equation}
E(\eta) = M(y - \mu),
\label{eq:expected-value}
\end{equation}
\begin{equation}
\text{Var}(\eta) = M(S - \Theta)M^T,
\label{eq:covariance}
\end{equation}
where $M$ is derived from factor loadings and measurement error covariances, $S$ is the sample covariance matrix of $y$, and $\Theta$ is the covariance matrix of $\epsilon$.
(2) Global SAM keeps first-stage measurement model parameters fixed while estimating structural model parameters using standard SEM techniques.

Recent studies by @rosseel_structural_2022 and @dhaene_evaluation_2023 have shown that SAM can outperform traditional SEM estimation in the presence of model misspecifications, especially in small to moderate sample sizes. 
However, @robitzsch_comparing_2022 has challenged these findings, arguing that SAM may systematically underestimate parameters in the presence of omitted negative cross-loadings or residual correlations.
This disagreement highlights the need for a systematic evaluation of SAM's performance in the presence of model misspecifications in small to moderate sample sizes. These diverging claims served as the starting point for an AC between a fellow student researcher (Collaborator B, Kosanke) and me (Collaborator A, Kriegmair), each representing one of the above sides of the differing findings. 
We developed a basic framework to tailor the concept of AC to simulation studies and facilitate a structured and systematic conduct and evaluation of the AC process in which a conceptual replication of respective findings by each collaborator marks the starting point representing a suitable testing ground for *Adversarial Simulation*. 
The framework, outlined in detail in the following section, consisted of two rounds: In the first round, each collaborator would independently conduct a separate simulation study. The AC would take place in the second round, and we planned to collaboratively design and conduct a study based on the first round. This two-stage approach was designed to systematically highlight differences between the individual approaches and establish a virtual foundation for collaboration before engaging in a joint effort in our case study. As the first step of this process, we jointly formulated two *substantive* research questions based on the above-mentioned conflicting claims:

1. How do SAM and traditional system-wide SEM estimation compare in terms of bias, mean squared error (MSE), and convergence rates in small to moderate samples?
2. What is the impact of model misspecifications, such as residual correlations and cross-loadings, on the performance of SAM compared to traditional SEM methods?

Finally, this case study explored the overarching research question: Is AC a viable and practical approach for resolving conflicting research claims and enhancing generalizability and rigor in simulation studies?

# Methods

## A Framework for Adversarial Collaboration

We developed a specific adversarial simulation framework and structured the collaboration into two rounds. First, as outlined above, the collaborators identify their disagreement and jointly formulate research questions that could resolve the diverging claims. Based on these research questions, in the first round, each collaborator independently conducts a separate simulation study. 
In the second round, the collaborators come together to work on a combined study, building on the findings from the first round. This two-step approach is designed to highlight differences in a systematic way and to establish a virtual foundation for collaboration before engaging in a joint effort in our case study. 
Further, we aimed to adhere the individual studies to a protocol of core steps of a simulation study adapted from @paxton_monte_2001 where critical and distinctive decisions by the researchers occur (see Figure 2). Thus, this was aimed at facilitating a systematic and stepwise comparision of the individual studies and integration to a joint study. 

::: {#fig-framework}
```{r}
library(knitr)
table_data <- data.frame(
  Step = c(
    "1. Aims & Research Questions", "2. Population Model Specification", 
    "3. Data Generating Mechanism", "4. Experimental Design", 
    "5. Method Selection", "6. Defining Estimands", 
    "7. Performance Measures", "8. Software Selection", 
    "9. Analysis"
  ),
  Description = c(
    "Agreed upon before any Adversarial Collaboration (e.g., examine model fit under misspecification)",
    "Optional: Define structure (e.g., CFA, SEM), size (number of latent variables and indicators) and complexity (e.g., cross-loadings) of population models.",
    "Choose between resampling and parametric draw and set the random data generation method.",
    "Define varying factors (e.g., sample size and distribution).",
    "Select estimation methods based on the research question.",
    "Reflect applied values, e.g., R², statistical significance, power considerations.",
    "Choose performance metrics (e.g., bias, sensitivity, accuracy); set simulation number for adequate Monte Carlo standard error.",
    "Select software, libraries, and functions for simulation.",
    "Decide on descriptive vs. inferential analysis and performance criteria"
  )
)

kable(table_data, caption = "A Framework of Simulation Studies (adapted from Paxton etl al., 2001)")
```

:::
\vspace{0.5cm}
## Individual Simulation Studies

### Studies by Collaborator A (Kriegmair)

The design and implementation of my (Collaborator A) individual simulation studies follow the structure established for the adversarial simulation framework to facilitate stepwise collaboration. They are based on a preregistered protocol [@kriegmair_preregistration_2024] but include some deviations from it ([See Appendix A](#appendix-a) for the full protocol and all deviations from the preregistration).
As outlined above, in the initial phase of the current case study, I conducted two simulation studies without my collaborator's involvement aimed at conceptually replicating the findings regarding SAM compared to standard SEM estimation of @rosseel_structural_2022 and @dhaene_evaluation_2023 as a basis for further adversarial collaboration with my collaborator. However, there are several differences in the design and setup of the studies compared to the original studies, as outlined below.

{{< include methods.qmd >}}

### Studies by Collaborator B (Kosanke)

{{< include methods_kosanke.qmd >}}

## Combined Simulation Study

The combined simulation study was conducted based on the individual studies of Collaborator A and Collaborator B to reconcile remaining conflicting claims by unifying divergent study designs and settings. As outlined in the next section reporting the results of the AC process this unified study was conducted without the direct involvement of Collaborator B (Kosanke) as he chose to terminate the case study on AC after the initial stage of individual (replication) studies. 

{{< include methods_joint.qmd >}}

# Results

## Individual Simulation Studies

### Results of Collaborator A (Kriegmair)

{{< include results.qmd >}}

### Results of Collaborator B (Kosanke)

{{< include results_kosanke.qmd >}}

## Combined Simulation Study

{{< include results_joint.qmd >}}

## Adversarial Collaboration Process   

Collaborator B (Kosanke) chose to terminate the AC after the initial stage of individual studies. He argued that while the Structural After Measurement (SAM) approach generally showed less bias and root mean square error (RMSE), there were certain conditions — particularly those involving negative unmodeled residuals and cross-loadings — where the advantages of traditional Structural Equation Modeling (SEM) offset those of SAM. This finding aligns with @robitzsch_comparing_2022, who observed a general negative bias in SAM. Further, he reasoned that Collaborator A's findings did not directly contradict this result, and the integration of the individual studies supported a general conclusion: SAM does not consistently outperform SEM in small to moderate samples. However, imporantly, Collaborator B acknowledged that his findings were limited to simple two to three-factor models without directed strucutral paths and that further more targeted investigation in more complex models was warranted and conceded that his prdominant reason for the termination of the AC was due to time constraints.

Thus, following this, Collaborator A (Kriegmair) identified multiple arguments for conducting an additional simulation based on the initial round of replicated studies and proceeded to establish an integrated study.
Collaborator B's conclusion regarding SAM's inconsistent outperformance of SEM in the presence of negative misspecifications was drawn only from a specific type of model with no directed relationships between factors and had not been tested in a more complex model with directed structural paths of interest. Importantly, only the latter represents the type of model for which @rosseel_structural_2022 suggested SAM to be advantageous. Moreover, to thoroughly investigate the hypothesized systematic underestimation by SAM, a parameter-wise analysis of bias was warranted. Aggregating bias values across model parameters could mask effects by averaging out negative and positive values or obscuring them entirely when using absolute values. Further, the claim that constrained SEM estimation methods could mitigate convergence issues and improper solutions was not thoroughly tested in the individual studies.
Although the collaborators did not jointly conclude to conduct a collaborative, unified simulation study as initially planned, the individual studies provided a solid foundation for a joint study. In this context, a unified study was conducted to formally complete the AC process, involving a stepwise integration of settings from the individual studies to address conflicting verbal claims as presented in the following.

#### Population Model and Data Generating Mechanism

In the combined study for parametric data generation, a five-factor population structural model with three indicators per factor, based on Collaborator A's approach, was selected to provide a complex model suitable for testing the advantages of SAM. CFA models from Collaborator B's studies were excluded, as SAM is intended for models with directed structural paths. Indicator reliability was manipulated by adjusting measurement error variances, following Collaborator A's method, to achieve a valid representation of item reliability.

#### Experimental Design

To evaluate the robustness of SAM, especially in response to negative misspecifications identified by Collaborator B, both positive and negative omitted cross-loadings and correlated residuals were included in the model misspecifications.
The experimental design extended sample size variation to include very small samples (N = 50), providing a more comprehensive range than in Collaborator A's studies but very large (population level) samples (N = 10). To limit the scope and narrow down on conditions relevant to expected differences between SAM and standard SEM, varying number of measurement blocks for SAM and strength of structural relations was not carried over from Collaborator A's studies as the results did not suggest a substantial impact on SAM compared to SEM.

####  Method Selection

In selecting estimation methods, bound SEM-ML, unbound SEM-ML, gSAM, and lSAM-ML were compared to address convergence issues with standard SEM, as proposed by Collaborator B. SAM-ULS and SEM-ULS were excluded to focus on the most relevant methods and manage computational demands.

#### Estimands and Performance Measures

As metrics bias and RMSE of the estimated structural parameters were calculated, and parameter-wise bias analysis was conducted to identify any potential negative bias in SAM, eliminating the effects of averaging.

#### Analysis

The analysis was broadly consistent with the individual studies, with the addition of parameter-wise bias displays to provide deeper insights into the biases and direct computation of metric differences between SEM and SAM.

# Discussion

The goal of the current study was twofold: First, to test the viability and practical applicability of Adversarial Collaboration (AC) as a tool to resolve disagreeing research claims and enhance generalizability and rigor in the context of simulation studies. Second, serving as a case study for this first aim, to evaluate the performance of traditional Structural Equation Modeling (SEM) compared to Structural After Measurement (SAM) and resolve conflicting claims of previous studies regarding whether SAM consistently outperforms traditional SEM in the presence of model misspecifications in small to moderate sample sizes [@robitzsch_comparing_2022; @rosseel_structural_2022; @dhaene_evaluation_2023].

## SAM vs. SEM

Overall, this case study systematically assessed how different implementations of SAM and standard SEM performed under varying sample sizes, indicator reliabilities, and degrees of model misspecification.
In Collaborator A's (Kriegmair) individual studies, SAM methods—including global SAM (gSAM) and local SAM (lSAM) with Maximum Likelihood (ML) and Unweighted Least Squares (ULS) estimators—consistently outperformed traditional SEM, particularly under challenging conditions that replicated previous findings [@rosseel_structural_2022; @dhaene_evaluation_2023]. Under these conditions, SAM methods achieved higher convergence rates and demonstrated lower average relative biases and RMSE values compared to standard, unconstrained SEM. Standard SEM exhibited significant convergence issues and a higher incidence of improper solutions, especially when models were misspecified by omitting cross-loadings or correlated residuals.
Collaborator B's (Kosanke) individual studies replicated findings from @robitzsch_comparing_2022, indicating that SAM did not generally outperform SEM in small to moderate samples. According to this, SAM exhibited a negative small-sample bias, which made it appear superior only under conditions with unmodeled positive cross-loadings and residual correlations. Previous research has shown that omitting existing cross-loadings or correlated residuals in SEM measurement models, effectively constraining them to zero, can result in absorption of the unmodelled variance by the latent factor relations in the structural model of the same direction as the unmodelled parameters [@hsu2014forced; @steenkamp2023unrestricted; @lance2010practice; @ferrando2022detecting].
Thus, a general underestimation of structural parameters by SAM was argued to offset the positive bias introduced by the unmodeled positive cross-loadings and residual correlations in Collaborator A's studies, creating an appearance of greater accuracy compared to SEM. In conditions without misspecification or with unmodeled negative cross-loadings and residual correlations, SEM tended to outperform SAM. However, these conclusions mainly rely on simple two to three-factor models without directed latent relations of interest.
The combined study sought to reconcile these differing findings in more complex models representative of use cases of SAM. First, by applying bound maximum likelihood estimation for SEM, as proposed by Kosanke, the convergence issues and improper solutions previously observed in SEM were effectively eliminated across all conditions, even in complex five-factor models with an extensive structural model. Second, parameter-wise analysis revealed that omitting positive cross-loadings led to an overall positive bias, while omitting negative cross-loadings led to a negative bias, as was previously shown [@hsu2014forced; @steenkamp2023unrestricted]. Notably, SAM methods exhibited less pronounced biases in both cases compared to SEM, especially at lower sample sizes and reliability levels. Contrary to Collaborator B's findings, the negative bias was also less pronounced for SAM methods in the combined study. When directly comparing SEM and SAM in terms of bias and RMSE, SAM methods were generally less biased and more accurate, particularly in conditions involving omitted cross-loadings and in scenarios with smaller sample sizes and lower indicator reliability. Moreover, in contrast to previous findings in almost all conditions, all methods showed systematic under- rather than over-estimation of parameters in the presence of positive (as well as negative) misspecifications [@ferrando2022detecting]. Finally, differences between gSAM and lSAM were minimal, with lSAM showing a slight advantage in RMSE. Additionally, the current results support constrained standard SEM as a promising alternative for complex models with relatively low sample sizes, given its improved convergence and stability compared to unconstrained standard SEM, with the benefits in convergence possibly outweighing the potential risk of obscuring model misfit [@savalei2008constrained].

Collectively, these findings suggest that SAM methods may offer advantages over traditional SEM in handling model misspecifications, particularly under challenging conditions. While the individual studies, replicating @robitzsch_comparing_2022, highlighted potential biases associated with SAM in small samples, the combined study indicates that these biases are not sustained in complex models. This supports the notion that SAM could be a favourable choice for SEM estimation in practice, particularly when addressing small sample sizes, low reliability, and potential model misspecifications, especially in more complex models.

However, the simulations conducted in this study have several limitations. They focused on a limited subset of models and conditions, constraining the generalizability of the findings to specific scenarios where the analyzed methods exhibit differential performance. The measurement models for most simulated data were restricted to three indicators per factor, without variation across factors. Apart from certain models where factor loadings were adjusted to influence reliability, these loadings were generally kept constant and unrealistically high (between 0.7 and 0.9), following the studies by @rosseel_structural_2022 and @dhaene_evaluation_2023. Additional modulation of factor loadings, alongside reliability modulation via measurement error variances, could have provided more realistic and extensive simulated data.
Future research could explore the performance of Structural After Measurement (SAM) in longitudinal data contexts, particularly with causal models, as SAM is aimed at applications where structural parameters are central, and measurement models may be (potentially) misspecified [@rosseel_structural_2022]. Additionally, it would be valuable to assess the differential impact of SAM on various structural parameters, such as mediating versus direct effects within mediation models. As suggested by @rosseel_structural_2022, SAM is particularly advantageous for scenarios involving small sample sizes relative to model complexity. Future studies could include conditions that systematically vary model complexity alongside sample size, potentially leading to the development of a metric to balance model complexity and sample size.

## Adversarial Collaboration in Simulation Studies

Shifting perspective, in our collaborative effort, we successfully established a joint starting point by translating conflicting verbal claims from prior studies into shared research questions. Based on these questions, we independently conducted simulation studies, largely drawing on the methodologies of @rosseel_structural_2022, @dhaene_evaluation_2023, and @robitzsch_comparing_2022. This approach effectively translated the verbal dispute into an empirical investigation. Notably, this process was only an emulation of AC, incorporating an additional layer by replicating previously published research findings in the first round of our framework. In a practical application of AC to simulation studies, as proposed here, this intermediate step could be omitted. Instead, collaborators might design two original studies or opt to work directly together on a unified research study, depending on the identification of a specific verbal disagreement.
Furthermore, although the unified simulation study was conducted by one collaborator, the individual studies highlighted both areas of agreement and divergence. While Collaborator B concluded that neither SAM nor SEM consistently outperformed the other across broader applications, The need for further exploration of SAM's performance in more complex models with directed structural paths und negative misspecifications was identified and acknowledged by both researchers. Additionally, even if conducted only by one collaborator, the unified study serves as a proof of concept regarding the techincal fesability for applying AC to simulation studies demonstrating how an actual collaboration process could unfold via a joint study.

In conclusion, the current study demonstrates that AC is a practically applicable and viable approach to further scientific rigor and potentially increase generalizability in the context of Monte Carlo simulation studies. By successfully translating a general conflict in findings about the performance of SAM and standard SEM into a joint research question and, based on this, directly juxtaposing our different simulation setups, we were able to trace back general diverging conclusions to specific methodological operationalizations and technical decisions. This effectively enhanced transparency and reduced ambiguity of the conflicting claims, allowing for more precise identification of the sources of disagreement, akin to previous implementations of AC in empirical research [@mellers_frequency_2001; @melloni_adversarial_2023]. In particular, this approach enforced direct engagement with each adversary's specific arguments for their conclusions, transparently linking each claim to its operationalized source and integrating it into a combined study aimed at falsifying the conflicting claims. This demonstrates how the practice of AC can aid in moving simulation studies closer to a Popperian ideal of falsificationism [@popper1963science; @lakatos1978methodology] by systematically testing and potentially refuting conflicting claims in a structured and transparent way instead of designing explicitly or implicitly confirmationist studies tailored to corroborating specific claims about a method's performance. Hence, adversarial simulation appears as a promising tool to enhance generalizability and rigor beyond other open science practices [@pawel_pitfalls_2023; @okelly_proposed_2017] at the point of simulation design without overly constraining researchers degrees of freedom in hypothesis generation and operationalization [@buchka_optimistic_2021; @clark_keep_2022]. Notably, this falsificationistic nature of AC should not be misunderstood as a strict adherence to a binary logic of true or false claims; it has also been suggested to align with a Bayesian framework of belief updating and evidence accumulation, serving as a systematic and transparent method to reduce ambiguity and increase generalizability by testing conflicting claims in a structured way [@corcoran2023accelerating].
In the current study AC led to more targeted, less ambiguous, and arguably more generalizable results and conclusions by adjusting multiple methodological aspects, such as model type, misspecifications, reliability computation, sample size, and analysis. However, it is important to note that due to the specific circumstances of this case study, which included a prior replication of previous results, these results impacted the collaboration and the combined study. If this step were omitted in a practical application of AC, the combined study would potentially need to be more comprehensive, covering a broader range of settings, if prior results would not be as readily available to inform it. Nevertheless, this initial independent replication phase has the advantage of isolating initial discrepancies for preexisting disagreements, which can clarify specific origins of divergences. As recently demonstrated in an empirical setting by @melloni_adversarial_2023 involving cross-lab replications in an AC project, such a step can help identify key factors that contribute to preexisting conflicting claims, allowing collaborators to later design a more streamlined and focused joint study. This connects to another possible objection: one could argue that most cases of contrasting simulation studies could be resolved by just extending the simulation to include all relevant settings, and this objection is partly valid, as demonstrated, for example, by the extension of the sample size factor in the combined study. However, as also shown by this study, there are diverging operationalizations that are exclusively and directly tied to specific conclusions, which are unresolvable by extending the simulation settings. Furthermore, individual authors might be explicitly or implicitly tempted toward biased conclusions from overly extensive simulation settings when mapping the results back to the general level, cherry-picking the most favourable results and clouding a clear and transparent interpretation [@clark_keep_2022; @buchka_optimistic_2021]. Finally, the current study also demonstrates how Adversarial Collaboration can be advantagously combined with other open science practices, such as preregistration, open source, open data and reproducability practices. Here the preregistration of the individual studies, the availabitilty of open source code and data on GitHub as well as cross reproducability via containerization 

Albeit promising, *Adversarial Simulation* also presented several challenges and limitations.
First, as already emphasized by @clark_road_2022, AC demands increased resources in terms of time and coordination. The process requires careful planning, open communication, and a willingness to reconcile differing viewpoints, and it may only justify its high cost if it advances science by rigorously clarifying core issues in disputes and not as a default. In our case, the termination of the collaboration by one party underscores the potential difficulties in sustaining such efforts. Even though simulation studies are less resource-intensive than empirical data acquisition, here, too, the additional relative time and effort required may pose practical constraints, especially in academic environments with tight schedules and resource limitations.
Second, even after collaborating and merging, the individual studies and settings of the combined study remained limited to very specific conditions, and any general conclusions from its results still rely on induction with the premise of having considered a prototypical scenario representing various application settings [@feinberg_conducting_2016; @gilbert2024multilevel; @bollmann_what_2015].
Third, the current project constitutes only an artificial case of AC, where the collaborators had no disagreement based on personal prior research but were assigned to represent conflicting viewpoints without any stakes or personal investment in the outcome. Furthermore, the final stage of collaboratively designing and conducting a study was conducted by one collaborator. Thus, this merely serves as a proof of concept showcasing the potential and technical applicability of AC in simulation studies.
Fourth, the applicability of AC may be limited to specific settings where there are clear, conflicting viewpoints on particular methods or theories. It may not be as effective in areas where disagreements are less defined or more complex. The focus on specific methodological disputes means that broader issues or more subtle disagreements might not be as amenable to this approach [@clark_adversarial_2023]
However, it can align with the recently recognized need for rigorous and unbiased assessment of comparative simulation studies in methodological research [@boulesteix_plea_2013] by providing an approach for researchers to juxtapose different (and possibly their own) statistical methods against each other.
Despite these challenges, overall, this case study shows that AC has the potential to increase generalizability and rigor in simulation studies beyond other open science practices by promoting transparency and reducing biases. By bringing together researchers with opposing views, it encourages a more thorough examination of assumptions and methodological choices, potentially leading to more robust and reliable conclusions.

An alternative approach to address the generalizability challenge, specifically at the point of operationalization and simulation design, is to ground simulations in empirical data. By incorporating actual models and parameters from practice [@bollmann_what_2015], sampling models and parameters from the literature can effectively bridge the gap between prototypical simulations and real-world applications, more directly targeting the generalizability challenge of simulation studies.
Another avenue for future research inspired by this project is the development of collaborative simulation platforms that could facilitate ongoing contributions from multiple researchers. By leveraging open-source tools and platforms such as GitHub, similar to the current project but more elaborate and refined, simulations can be made "living" open-source projects that are continuously updated and refined. Collaborators could utilize open-source platforms like GitHub to add new conditions or settings to existing simulations, allowing for dynamic testing and continuous integration of new results.
For example, a research team interested in assessing the performance of one method for a specific type of model and data relevant to their work could contribute to an existing simulation repository. With minimal coding effort, they could add their conditions, request a rerun of the simulation, and obtain updated results that inform both their specific research and the general understanding of the method's performance. This collaborative approach could effectively enhance the generalizability of simulation studies, making them more accessible and responsive to the needs of the research community and better capturing the complexities across different contexts.
Implementing such collaborative simulation platforms would require computational infrastructure and a system for regulating when simulations are rerun to manage computational costs. Streamlined pipelines involving containerization technologies like Docker or Singularity [@kurtzer2017singularity; @merkel2014docker] could facilitate the dynamic deployment of simulations on high-performance computing resources. Additionally, establishing guidelines and open peer review mechanisms for contributions would ensure the quality and integrity of the simulations.
Finally, the comparison of the individual studies and the process of conducting a combined study also highlighted the need for a more structured and standardized code of conduct for programming the simulation studies that could facilitate collaboration. This could include a common modular file structure and naming conventions, standardized code comments, and documentation.

Scientific progress relies on a delicate balance between general theoretical claims, concrete operationalizations, and rigorous testing where researchers degrees of freedom are essential for designing and conducting innovative and impactful research. However, these same freedoms can introduce ambiguity and bias, posing risks to the reproducibility and generalizability of findings.
In sum this thesis demonstrates how Adversarial Collaboration can effectively map onto this dynamic in the context of Monte Carlo simulation studies, offering a flexible and yet rigorous testing ground where conflicting general claims can be systematically examined and refined through operationalized inquiry, embracing (methodological) research as a communal endeavor.

\newpage

# References

::: {#refs}
:::

\newpage

# Appendix A: Simulation Protocol {#appendix-a}

Here, the full simulation protocol of my simulation studies conducted individually prior to collaboration as well as the follow up study I conducted in light of the collaboration with Kosanke after the first round of conducting and evaluating our individual studies is presented. It is based on the preregistration of my individual studies [@kriegmair_preregistration_2024] and outlines all deviations from it.

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

{{< include appendix_a.qmd toc=false >}}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\newpage

# Appendix B: Supplementary Figures {#appendix-b}

```{=tex}
\renewcommand{\thefigure}{B\arabic{figure}}
\setcounter{figure}{0}
```
In the following, additional figures are presented that were not included in the main text and provide further insights into the simulation results.

{{< include appendix_b.qmd toc=false >}}

\newpage

# Appendix C: Detailed Error and Warning Messages {#appendix-c}

```{=tex}
\renewcommand{\thetable}{C\arabic{table}}
\setcounter{table}{0}
```
Here, all warning and error messages raised during the studies are listed (see Table C1) and shown how often they occurred under various fitting conditions (see Table C2).

{{< include appendix_c.qmd toc=false >}}

\newpage

# Appendix D: Reproducibility Instructions {#appendix-d}

{{< include appendix_d.qmd toc=false >}}

\newpage

```{=tex}
\includepdf[pages=-]{Statement.pdf}
```