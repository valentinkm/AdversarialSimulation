---
title: "Adversarial Simulation"
subtitle: "Structural after Measurement vs. vanilla SEM estimation - a Case Study on Adversarial Collaboration for Simulation Studies"
fig-cap-location: top
author: 
  - name: "Valentin Kriegmair"
  - name: "Leonard Kosanke"
format:
  revealjs:
    center: true
    theme: default
    slide-number: c/t
    font-size: 12pt
    chalkboard: true

engine: knitr
bibliography: ../../bibliography.bib
csl: ../../apa.csl
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
```


::: {.r-fit-text style="font-size: 0.8em;"}
## Outline {auto-animate=true}
1. The Generalizability Challenge & Adversarial Collaboration
2. An Adversarial Simulation Framework
3. Background (SAM vs. SEM)
4. Research Questions
<br>
<br>
<span style="color: grey;">Time for questions</span>
<br>
<br>

5. Results
6. Collaboration
7. Evaluation and Future Directions

<br>
<span style="color: grey;">Discussion</span>

:::{.notes}
Before we dive in, let me give you a quick overview of today's presentation.

We'll start by exploring the challenges in generalizing research findings and how adversarial collaboration can address these issues. Then, I'll introduce the adversarial simulation framework we've developed.

After setting the stage, I'll provide some background on Structural After Measurement as an alternative to traditional Structural Equation Model estiamtion, which is the substantial topic of our case study. I then introduce the research questions we aimed to answer in this project.

In the latter part, I'll present and discuss our results and the collaborative process to conclude with an evaluation and future directions.

Good afternoon everyone, and thank you for being here today. My name is Valentin Kriegmair, a master's student at Humboldt University working in the Formal Methods Group. I'm excited to present my master's thesis to you today. This project was conducted in collaboration with Leonard Kosanke, who also wrote his thesis on this topic, under the supervision of Aaron Peikert and Mathias Ziegler.

We explored how the practice of adversarial collaboration can be applied to simulation studies, using a comparison between Structural After Measurement and traditional Structural Equation Model estimation as a case study.
:::

:::

## The Generalizability Challenge {auto-animate=true}
:::{.columns}
::: {.column width="0%"}
::: {.r-fit-text}
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
:::
:::
::: {.column width="50%"}
::: {.r-fit-text}
```{dot}
digraph circular_process {
    // Use 'neato' layout for positioning
    layout=neato;
    overlap=false;
    splines=true;

    // Define global node attributes
    node [shape=plaintext, fontsize=12];

    // Define nodes with positions to form a circle
    hypothesize [label="Hypothesize", pos="0,1!"];
    operationalize [label="Operationalize", pos="-0.866,-0.5!"];
    gather_data [label="Gather Data", pos="0.866,-0.5!"];

    // Define edges to create a circular process
    hypothesize -> operationalize;
    operationalize -> gather_data;
    gather_data -> hypothesize;
}
```

:::
:::
:::

:::{.notes}
Karl Popper once famously described science as the art of "systematic over-simplification." This term ironically yet accurately describes the very basic cycle of empirical research, where we lay out general claims about the world as hypotheses, translate them into measurable constructs, and choose how to gather data from certain populations, which finally, in turn, updates our beliefs about general verbal claims about the world.
:::

## The Generalizability Challenge {auto-animate=true auto-animate-easing="ease-in-out"}

:::{.columns}
::: {.column width="25%"}
::: {.r-fit-text}

<br>
<span style="color:blue">general & verbal</span>
<br>
<br>
<br>
<br>


::: {.fragment}

<span style="color: red;">researcher's degrees of freedom</span> <br>
:::

::: {.fragment}

<span style="color: red;">→ ambiguity</span> <br> 
<span style="color: red;">→ **persistent disagreement**</span>


:::

<br>
<br>
<br>
<span style="color: green;">specific</span> & <span style="color: purple">  empirical</span>
:::
:::
::: {.column width="50%"}
::: {.r-fit-text}
```{dot}
digraph circular_process {
    // Use 'neato' layout for positioning
    layout=neato;
    overlap=false;
    splines=true;

    // Define global node attributes
    node [shape=plaintext, fontsize=12];

    // Define nodes with positions to form a circle
        hypothesize [label=< <font color="blue">Hypothesize</font> >, pos="0,1!"];
        operationalize [label=< <font color="green">Operationalize</font> >, pos="-0.866,-0.5!"];
        gather_data [label=< <font color="purple">Gather Data</font> >, pos="0.866,-0.5!"];

    // Define edges to create a circular process
    hypothesize -> operationalize [color="red"];
    operationalize -> gather_data [color="red"];
    gather_data -> hypothesize [color="red"];
}
```
:::
:::
:::

:::{.notes}
One core challenge in every research endeavor is mapping the general to the specific when designing and conducting a study, or from the specific and empirical to the verbal and general when interpreting the results. [▶︎]This mapping appears to be the crux of most research and is where most, if not all, of a researcher's degrees of freedom lie. [▶︎]Divergences in these mappings can be the source of ambiguity and verbal disputes. When differences in mappings from the verbal plane are not recognized or lack transparency, persistent and seemingly unresolvable disagreements in the general and verbal domains can occur
:::

## The Generalizability Challenge {auto-animate=true auto-animate-easing="ease-in-out"}

In Simulation Studies

:::{.columns}
::: {.column width="25%"}
::: {.r-fit-text}

<br>
<span style="color:blue">general & verbal</span>
<br>
<br>
<br>
<br>
<span style="color: red;">researchers degrees of freedom</span> <br>
<span style="color: red;">→ ambiguity</span> <br>

::: {data-id="text1" auto-animate-delay="0"}

<span style="color: red;">→ **persistant disagreement**</span>

:::

<br>
<br>
<br>
<br>
<span style="color: green;">specific</span> & <span style="color: purple">  empirical</span>
:::
:::
::: {.column width="50%"}
::: {.r-fit-text}
```{dot}
digraph circular_process {
    // Use 'neato' layout for positioning
    layout=neato;
    overlap=false;
    splines=true;

    // Define global node attributes
    node [shape=plaintext, fontsize=12];

    // Define nodes with positions to form a circle
    hypothesize [label=< <font color="blue">Method A vs. B</font> >, pos="0,1!"];
    operationalize [label=<
        <table border="0" cellborder="0" cellspacing="0">
            <tr><td><font color="green">Model,<br/> Estimands,<br/> Metrics etc.</font></td></tr>
        </table>
    >, pos="-0.866,-0.5!", color="green"];
    gather_data [label=< <font color="purple">Simulate</font> >, pos="0.866,-0.5!", color="green"];

    // Define edges to create a circular process
    hypothesize -> operationalize [color="red"];
    operationalize -> gather_data [color="red"];
    gather_data -> hypothesize [color="red"];
}
```
:::
:::
:::

:::{.notes}
This challenge transfers to Monte Carlo simulation studies. These are commonly used tools to test statistical methods in simulated data to evaluate any method against a known ground truth. As it is impossible to simulate and test every possible data and analysis model combination, researchers are confronted with a multitude of degrees of freedom and decisions about what "prototypical" models to test in which "prototypical" data and settings. Especially the comparison of different methods in their "general" applicability and performance for various research settings is prone to conflicting verbal claims based on diverging simulation decisions. Biases for a specific method developed by one researcher might additionally amplify these divergences, not only at the step of interpreting results but importantly also when designing a simulation.
:::

## Adversarial Collaboration (AC) as a Remedy?{auto-animate=true 
auto-animate-easing="ease-in-out"}

:::{.r-fit-text}

Pioneered by @mellers_frequency_2001:
![](figures/paper1.png){width="70%"}

Recognized in Empirical Research @melloni_adversarial_2023 @clark_adversarial_2021

:::

:::{.notes}
To address these challenges of entrenched disagreements, the practice of adversarial collaboration has been proposed to unveil discrepancies in underlying methodological decisions and assumptions.
It was famously pioneered by Ralph Hertwig and Daniel Kahneman, who tried to settle a persistent scientific disagreement about frequency representation and consulted Barbara Mellers as a neutral arbiter. Today, it is recognized as a potent tool in the social empirical research community.
:::


## Adversarial Collaboration (AC){auto-animate=true auto-animate-easing="ease-in-out"}

:::{.columns}
::: {.column width="25%"}
::: {.r-fit-text}

:::{.fragment fragment-index=1}
<br>
<span style="color:blue">Identify general & verbal Disagreement → joint Research Question</span>
<br>
:::
:::{.fragment fragment-index=2}
        
<span style="color: red;">Agree on:
<br>  - Operationalizations <br> - Test Design <br> - Interpretation 
:::
:::{.fragment fragment-index=3}
</span>
<span style="color: black;">→ Reduce ambiguity<br>→ Increase generalizability  </span>
:::

<br>

:::{.fragment fragment-index=2}
<span style="color: green;">unify</span>
:::
:::
:::
::: {.column width="50%"}
::: {.r-fit-text}
```{dot}
digraph circular_process {
    // Use 'neato' layout for positioning
    layout=neato;
    overlap=false;
    splines=true;

    // Define global node attributes
    node [shape=plaintext, fontsize=12];

    // Define nodes with positions to form a circle
    hypothesize [label=< <font color="blue">Verbal Dispute</font> >, pos="0,1!"];
    operationalize [label=< <font color="green">Operationalizations</font> >, pos="-0.866,-0.5!", color="green"];
    gather_data [label=< <font color="green">Gather Data</font> >, pos="0.866,-0.5!", color="green"];

    // Define edges to create a circular process
    hypothesize -> operationalize [color="red"];
    operationalize -> gather_data [color="red"];
    gather_data -> hypothesize [color="red"];
}
```
:::
:::
:::

:::{.notes}
The basic idea is for two researchers in disagreement to first identify a general verbal dispute [▶︎] and agree on a research question to settle the debate. Based on this, they collaboratively work on operationalizing, testing, and interpreting this verbal claim [▶︎]. This process aims to unveil and concretize underlying disagreements and thus reduce ambiguity and increase generalizability [▶︎]. In this project, we aimed to transfer the concept of adversarial collaboration from the empirical domain to Monte Carlo simulation studies and assess its feasibility and viability in a case study in this context.
:::

## Creating an Adversarial Simulation <span style="color:#4682B4;">Framework</span>{auto-animate=true}

:::{.notes}
To conduct such an exemplary adversarial collaboration, we first need a framework that structures the collaborative process tailored to the outline of simulation studies.
:::

# <span style="color:#4682B4;">Structure</span> of a Simulation Study {auto-animate="true" data-id="simulation-structure-slide"}

::: {.r-fit-text}
| Steps of a Simulation Study | Content |
|-----------------|-----------------|
| <span class="fragment"><b>1. Research Question</b></span>    | <span class="fragment" style="color:grey;">Verbal description of Research Goals</span>    |
| <span class="fragment"><b>2. Population Model</b></span>   | <span class="fragment" style="color:grey;">Type, size, complexity</span> |
| <span class="fragment"><b>3. Data Generation</b></span>    | <span class="fragment" style="color:grey;">E.g. resampling vs. parametric draw</span>    |
| <span class="fragment"><b>4. Experimental Design</b></span>    | <span class="fragment" style="color:grey;">Specifiy conditions (e.g., sample size)</span>    |
| <span class="fragment"><b>5. Method Selection</b></span>   | <span class="fragment" style="color:grey;">Type, implementation, number</span>    |
| <span class="fragment"><b>6. Estimands</b></span>    | <span class="fragment" style="color:grey;">Population level parameter values</span>    |
| <span class="fragment"><b>7. Performance Metrics</b></span>    | <span class="fragment" style="color:grey;">E.g. Bias, Coverage etc.</span>  |
| <span class="fragment"><b>8. Software</b></span>    | <span class="fragment" style="color:grey;">Applies to steps 2-7</span>    |
| <span class="fragment"><b>9. Analysis</b></span>    | <span class="fragment" style="color:grey;">Decision criteria, graphical display etc.</span>  |
:::

<span style="font-size: 20px;"> @siepe_simulation_2023, @paxton_monte_2001, @morris_using_2019</span>

:::{.notes}
As a basis for this, we first identified the core steps of a simulation study where critical and distinctive decision points occur.
:::

## A <span style="color:#4682B4;">Structured</span> Adversarial Simulation <span style="color:#4682B4;">Framework</span> {auto-animate=true}

::: {.r-fit-text}
|                     | Round 1         | Round 2               | Round 1         |
|---------------------|:---------------:|:---------------------:|:---------------:|
| Steps         | Collaborator 1  | Joint Study           | Collaborator 2  |
|1. Research Question|                |  Agreed upon prior to Round 1 |                |
| 2. Population Model |→                |                       |←                |
| 3. Data Generation  |→                |                       |←                |
| 4. Experimental Design|→              |                       |←                |
| 5. Method Selection |→                |                       |←                |
| 6. Estimands        |→                |                       |←                |
| 7. Performance Metrics|→             |                       |←                |
| 8. Software         |→                |                       |←                |
| 9. Analysis         |→                |                       |←                |
:::

:::{.notes}
We developed a specific adversarial simulation framework and structured the collaboration into two rounds. In the first round, each collaborator independently conducts a separate simulation study. In the second round, they come together to work on a joint study, building on the findings from the first round.

This two-step approach is designed to highlight differences in a systematic way and to establish an virtual foundation for collaboration before engaging in a joint effort in our case study.
:::

## Strucutral after Measurement (SAM) vs traditional SEM estimation <span style="font-size: 20px;"> Background</span>

:::{.notes}
Now, to set the stage for our specific case study, I briefly outline the background of Structural After Measurement (SAM) as a potential alternative to traditional Structural Equation Modeling (SEM) estimation.
:::

## Standard SEM Estimation <span style="font-size: 20px;">(e.g.ML, ADF, GLS, ULS)</span>{auto-animate="true"}

::: {.columns}
:::: {.column width="50%"}
![](figures/model0.svg){width="100%"}
::::

:::: {.column width="50%"}
::: {.r-fit-text}
::: {.fragment .fade-in-then-out}
-  e.g. normal theory-based maximum likelihood (ML) discrepancy function
- System-wide parameter ($\vartheta$) optimization
- Assumes multivariate normal distribution
:::
:::{.fragment .fade-in}
Problems:

- non-convergence issues 
- improper solutions
- bias due to local measurement misspecifications propagating to all model parameters
- requiring large sample sizes for optimal statistical properties.
:::
:::
::::
:::

:::{.notes}
Traditional SEM methods, like maximum likelihood estimation, optimize all parameters of a model simultaneously under the assumption of multivariate normality. While powerful and although robust estimation techniques relax the normality assumption, all system wide estimators suffer from several shortcomings, [▶︎] they often face issues such as non-convergence, improper solutions (with parameters out of definitoral range), and biases from local measurement misspecifications that affect the entire model. They also typically require large sample sizes for adequate performance, especially in complex models.
:::

## Structural After Measurement (SAM)
::: {.columns}
:::: {.column width="50%"}
<img src="figures/model0_1.svg" width="80%">
::::

:::: {.column width="50%"}
::: {.r-fit-text}
**Two-phase process:**

1. <span style="color: red;">$\vartheta_1$: Measurement model</span> <br>
    <span class="fragment">**Local SAM (lSAM):**</span> <br>
    <span class="fragment">Separate "measurement blocks"</span><br>
    <span class="fragment">Latent summary statistics and mapping matrix;</span> <br>
    <span class="fragment">**Global SAM (gSAM):** Fixed measurement parameters for the entire measurement model.</span>

:::{.fragment}
<br>
 → 
<br>

:::
2. <span style="color: blue;">$\vartheta_2$: Structural model</span>
:::
::::
:::

<span style="font-size: 20px;">@rosseel_structural_2022</span>

:::{.notes}
SAM addresses some limitations of SEM by separating the estimation into two phases. First, the measurement model parameters are estimated and then fixed to estimate the structural model. This approach aims to reduce the propagation of bias from the measurement to the structural part and decrease convergence issues, especially in smaller samples and complex models. [▶︎] There are two distinctive implementations of SAM [▶︎] local SAM constructs latent variable summary statistics and a mapping matrix to inform the structural model estimation, [▶︎] while global SAM directly estimates structural parameters using fixed measurment parameters of one measurement model.[▶︎]
:::


## SAM vs. SEM: <br> Disagreeing Reports {auto-animate="true"}
::: {.r-fit-text}
| @rosseel_structural_2022, @dhaene_evaluation_2023 | @robitzsch_comparing_2022 |
|------------------------------|---------------------------|
| - SAM outperformed SEM in terms of convergence, bias & RMSE in small samples x low item reliability, especially under misspecifications <br> | - SAM did not generally outperform traditional SEM in challenging conditions. <br> - SAM appears better: general negative small sample bias of SAM cancels out positive bias from positive misspecifications. |
<br>
<br>

:::{.fragment}
<div style="text-align: center;">
**→ Basis for a case study on adversarial collaboration**
</div>

:::

:::{.notes}
Since SAM was recently reintroduced as a potential alternative to traditional SEM estimation, there have been conflicting reports on its performance. Rossel, Loh and Dahene found in two simulation studies that SAM outperformed SEM in terms of convergence, bias, and RMSE, particularly in small samples with low item reliability, especially under misspecifications. In contrast there was a study by Robitzsch that found that SAM did not generally outperform traditional SEM in challenging conditions. He argued that SAM only appears better in specific conditions because a general negative small sample bias of SAM cancels out the positive bias from positive misspecifications.[▶︎] This disagreement formed the basis for our adversarial simulation case study
:::

:::

## SAM vs. SEM: <br> an <span style="color: #4682B4;"> Adversarial Simulation </span> {auto-animate="true"}
::: {.r-fit-text}
| @rosseel_structural_2022, @dhaene_evaluation_2023 | @robitzsch_comparing_2022 |
|------------------------------|---------------------------|
| - SAM outperformed SEM in terms of convergence, bias & RMSE in small samples x low item reliability, especially under misspecifications <br> | - SAM did not generally outperform traditional SEM in challenging conditions. <br> - SAM appears better: general negative small sample bias of SAM cancels out positive bias from positive misspecifications. |
| <span style="color: #4682B4;">**Replication by Kriegmair**</span> | <span style="color: #4682B4;">**Replication by Kosanke**</span>|
<br>
<br>


<div style="text-align: center;">
**→ Basis for a case study on adversarial collaboration**
</div>

:::

:::{.notes}
A conceptual replication of these studies based on a joint research question was conducted by Leonard Kosanke and myself as round 1 of our collaboration.
:::


## Research Questions

::: {.r-fit-text}

**Is Adversarial Collaboration a viable approach to address ambiguity and increase generalizability in simulation studies?**
<br>
<br>
**Substantive Questions wihtin the Case Study** <span style="font-size: 20px;">(agreed upon prior to individual studies)</span>

1. How do SAM and traditional SEM methods (including ML and ULS) compare in terms of bias, Mean Squared Error (MSE), and convergence rates in small to moderate samples?
2. What is the impact of model misspecifications, such as residual correlations and cross-loadings, on the performance of SAM compared to traditional SEM methods?

:::

:::{.notes}
Our primary research question is whether adversarial collaboration is a viable approach to reduce ambiguity and increase generalizability in simulation studies. Specifically, within our case study, we want to compare SAM and traditional SEM methods in terms of bias, MSE, and convergence rates in small to moderate samples, and assess the impact of model misspecifications on their performance.
:::

## Findings of the Case Study

Replication Results
<br>
Adversarial Collaboration

:::{.notes}
Now, let's dive into the results of our case study
:::


## Round 1: Individual Studies 
| Kriegmair | Kosanke |
|-----------------|-----------------|
| based on @rosseel_structural_2022 and @dhaene_evaluation_2023 | based on @robitzsch_comparing_2022  |

:::{.notes}
As just mentioned in the first round of our adversarial collaboration, Leonard Kosanke and I conducted individual simulation studies based on the conflicting reports by Rossel, Loh, and Dahene and Robitzsch.
:::

## Studies by Kriegmair

:::{.notes}
I will first present the results of my individual simulation studies.
:::

## Study 1 <span style="font-size: 20px;">based on @rosseel_structural_2022</span>

::: {.r-fit-text}
::: {layout="[20,60,20]"}

::: {.column width="0%"}
:::

::: {.column width="60%"}

::: {layout="[[1,1], [1,1]]" layout-valign="top"}

::: {.column width="50%" .fragment}
1.1 <span style="font-size: 20px;">no misspecifications</span>
![](figures/model1_1.svg){width="60%"}
:::

::: {.column width="50%" .fragment}
1.2 <span style="font-size: 20px;"> cross loadings</span>
![](figures/model1_2.svg){width="60%"}
:::

::: {.column width="50%" .fragment}
1.3 <span style="font-size: 20px;"> correlated residuals </span>
![](figures/model1_3.svg){width="80%"}
:::

::: {.column width="50%" .fragment}
1.4 <span style="font-size: 20px;"> structural misspecification </span>
![](figures/model1_4.svg){width="60%"}
:::
:::
:::

::: {.column width="35%"}

::: {.fragment}

#### Other Conditions:

- *N*: 100, 400, 6400
- Indicator reliability: 0.3, 0.5, 0.7

:::

::: {.fragment}

#### Methods:

- Vanilla SEM with Maximum Likelihood (ML)
- Global SAM (gSAM)
- Local SAM (lSAM-ML)
- Local SAM with unweighted least squares (lSAM-ULS)

:::

:::{.fragment}

#### Performance Metrics:
- Bias of path estimates $\hat{\beta}_i - \beta_i$
- RMSE of path estimates
  $\sqrt{\sum_{i=1}^{n} (\hat{\beta}_i - \beta_i)^2}$
- Coverage of 95% CI for path estimates
:::

:::
:::
:::

:::{.notes}
Here I give a brief overview of the set up the studies. In a first study I considered four distinct population models that differed in the presence of misspecifications which was also one condition of interest. I also varied the sample size, the indicator reliability. Four different methods, traditional SEM with ML, global SAM, local SAM with ML, and local SAM with unweighted least squares compared in each condition. As performance metrics, I considered the bias of path estimates which is the difference between the predicted and actual values, indicating whether the model systematically over- or under-predicts, and the RMSE of path estimates, measuring the overall error magnitude and general accuracy of estimates. The coverage of 95% confidence intervals for path estimates was calculated as the proportion of times the true path coefficient was within the 95% confidence interval of the estimated path coefficient.
:::

## Study 2 <span style="font-size: 20px;">based on @dhaene_evaluation_2023</span>

::: {.r-fit-text}
::: {layout="[[45,10,10,35]]" layout-align="top"}

::: {.column width="45%"}
::: {layout="[[1,1]]" layout-valign="top"}

::: {.column width="50%" .fragment}
#### 2.1:

- No measurement misspecifications
- Estimated paths absent in population

![](figures/model2_1.svg){width="100%"}
:::

::: {.column width="50%" .fragment}
#### 2.2:

- Estimated paths absent in population
- <span style="color: orange;">In exogenous analysis model</span>
- <span style="color: green;">In endogenous analysis model
![](figures/model2_2.svg){width="100%"}
:::
:::
:::
::: {.column width="7%"}
<!-- Empty column for space -->
:::

::: {.column width="35%"}
::: {.fragment}
#### Other Conditions:

- *N*: 100, 400, 6400
- Indicator reliability: 0.3, 0.5, 0.7
- **$R^2$: 0.1, 0.4**
- **Measurement blocks: 3, 5**
:::


:::{.fragment}
#### Methods:

- Vanilla SEM
- Global SAM
- Local SAM
- Local SAM with unweighted least squares
:::

:::
:::
:::

:::{.notes}
In the second study, I considered also four population models that differed in the presence of misspecifications. Where the first condition was a model with no measurement misspecifications and estimated paths absent in the population but specified in the analysis model. I then considered the same model with measurement misspecifications either in the exogenous or endogenous analysis model or both. Next to the sample size and indicator reliability, I also varied the $R^2$ of the latent variables and the number of measurement blocks used for SAM. The same methods under the same performance metrics as in study  were compared in each condition.
:::

## Studies by Kosanke <span style="font-size: 20px;">based on @robitzsch_comparing_2022</span>

![](figures/kosanke_studies_overview.png)

:::{.notes}
Now, let me give a brief overview of Kosanke's studies. He conducted six simulation studies replicating key aspects relevant to our research questions. He examined various population models to create different misspecification conditions, similar to my own approach. However he also included negative cross-loadings and residual correlations and he put a special focus on simple 2 factor CFA models. He also varied the sample size ranging from as small as 50 to as large as 100,000 and also varied the reliability of the indicators. He used mostly the same methods and performance metrics as I did in my studies. I will now go into more detail about specific discrepancies between our aproaches when presenting the results.
:::

# Results of individual studies <span style="font-size: 20px;"> (examplatory) </span>

## Convergence Rate
::: {.r-fit-text}
| | Kriegmair | Kosanke |
|--------|----------------|-------------|
| Verbal Dispute |low convergence rate and high rate of improper solutions only for SEM in challenging conditinos| no convergence issues|
|Method selection|un-bounded ML SEM |bounded ML SEM |
|Analysis| condition-wise rates | global rate |
|| direct:<br>`lavInspect(fit, "converged")` | indirect:<br>`quietly(safely(simulation_study_))`|
:::

:::{.notes}
One main discrepancy between the findings of our individual studies was the convergence rate of SEM.  I found that SEM had a low convergence rate, especially in small samples and low reliabillity. Kosanke on the other hand did not report any or just neglible convergence issues. We found that this discrepancy was likely due to differences in the method selection and analysis. I used unbounded ML SEM, while Kosanke used bounded ML SEM. Bounded ML SEM constraints variances and loadings to their theoretical range, which can help with convergence issues and avoid improper solutions.

I also captured the convergence rate condition-wise, while Kosanke tracked the overall rate. 
:::

## Convergence Rate <span style="font-size: 20px;"> E.g.: Kriegmair Study 1:</span>

![](tables/convergence_rate_study1.png)

:::{.notes}
Here you can see the convergence rates for the different methods in the first study. As you can see, SEM had convergence rates as low as 50%, in low sample size and reliability with misspecifications. SAM methods, on the other hand had no convergence issues at all.
:::

## Bias
::: {.r-fit-text}
| | Kriegmair | Kosanke |
|------|-----------|---------|
| Verbal Dispute |"SEM performes worse than SAM in low reliability x low sample size x misspecification"| "SAM generally did not outperform traditional SEM in small to moderate samples." <br> <br> "[under] unmodelled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM"|
:::

:::{.notes}
Another key discrepancy we found in our findings was for example the bias of the path estimates. I found and claimed that SEM performed worse than SAM in low reliability, low sample size, and misspecification conditions. Kosanke, on the other hand, found that SAM generally did not outperform traditional SEM in small to moderate samples. He also found that under unmodeled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM.
:::


## Bias <span style="font-size: 20px;"> E.g.: Kriegmair Study 1: relative $\hat{\beta}$ bias values aggregated across parameters</span>
::: {.r-fit-text}
![](tables/aggregated_bias_study1.png)
:::

:::{.notes}
Here for example you can see the aggregated relative bias values for the path estimates in my first study. As you can see here I found that espcially cross loadings led to a higher bias in SEM compared to SAM in small samples and low and moderate reliability.
:::

## Bias <span style="font-size: 20px;"> E.g.: Kosanke Study 1:</span>
::: {.r-fit-text}
```{r eval = TRUE, echo = FALSE}

# Load the relative bias results
bias_ci_s1 <- readRDS("../../LK/SimulationResultsProcessed/sim1_rel_bias_ci.rds")

shorten_names <- function(df) {
  df$method_metric <- sub("_rel_bias", "", df$method_metric)
  return(df)
}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  data <- shorten_names(data)
  data <- data %>%
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  # Format table
  kbl(data,
      col.names = c("Method", "50", "100", "250", "500", 
                    "1000", "2500", "100000"), booktabs = TRUE) %>%
    kable_styling(full_width = T, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    column_spec(1, width = "3.5cm") %>% 
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    gsub("_", " ", .)
}
create_styled_table(bias_ci_s1[["2_-0.12"]], "2_-0.12")
```

<span style="font-size: 22px;">Relative bias of $\hat{\phi}$ in conditions with two negative unmodelled residual correlations in a 2-factor-CFA</span>

:::

:::{.notes}
Kosanke on the other hand backed his verbal claim with for example findings like these where SEM consistently outperformed SAM in terms of bias in small to moderate samples in a 2-factor CFA model with two negative unmodelled residual correlations.

Kosanke replicating Robitiszsches findings, claimed here that SAM appears to perform worse than traditional SEM under unmodeled negative cross-loadings and residual correlations. This happens because of a  cancellation of bias in LSAM, particularly in small samples:

LSAM tends to have a negative small-sample bias, and when you ignore positive residual correlations, that introduces a positive bias. These two biases cancel each other out, making LSAM seem more accurate than it really is.

However, he claims this is a false perception of robustness, as LSAM performs inconsistently across different conditions, and in cases like unmodeled negative correlations, it actually does worse than SEM.
:::

## Bias
::: {.r-fit-text}
| | Kriegmair | Kosanke |
|------|---------|-----------|
| Verbal Dispute |"SEM performs worse than SAM in low reliability x low sample size x misspecification"| "SAM generally did not outperform traditional SEM in small to moderate samples." <br> <br> "[under] unmodeled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM"|
|4. Experimental Design| *N*:100-6400 <br> Reliability via Θ <br> Positive cross loadings & correlated residuals| *N*:50-100000 <br> Reliability via $\lambda$ <br> Positive & negative cross loadings and correlated residuals (only in CFA)|
| &nbsp;                  | &nbsp;       | &nbsp;                       |
| &nbsp;                  | &nbsp;       | &nbsp;                                  |
:::


:::{.notes}
A closer look then enabled us to trace back our verbal claims in direct contrast to differences in certain decisions in our individual studies. [▶︎] Firstly "low" sample size included a size of 50 in Kosanke's study, while I started at 100. Secondly, I manipulated reliability by adjusting the indicator error variance, while Kosanke varied the factor loadings. Thirdly I only included positive cross-loadings and correlated residuals, while Kosanke also included negative cross-loadings and residual correlations. Fourth, I refer only to results from a 5-factor SEM with a strucutral while Kosankes findings are in large based on estimating CFA models.
:::

## Bias
::: {.r-fit-text}
| | Kriegmair | Kosanke |
|------|---------|-----------|
| Verbal Dispute |"SEM performs worse than SAM in <span style="color: green;">low reliability</span> x <span style="color:blue;">low sample size</span> x <span style="color:orange;">misspecification</span>"| "SAM generally did not outperform traditional SEM in <span style="color:blue;">small to moderate samples</span>." <br> <br> "[under] <span style="color:orange;">unmodeled negative cross-loadings and residual correlations</span>, SAM tended to perform worse than traditional SEM"|
|4. Experimental Design| *N*:<span style="color:blue;">100, 400</span>, 6400<br> <span style="color:green;">Reliability via $\Theta$ </span> <br> <span style="color:orange;">Positive cross loadings & correlated residuals</span>| *N*:<span style="color:blue;">50, 100, 250, 500</span>, 1000, 2500, 100000<br><span style="color:green;"> Reliability via  $\Lambda$</span> <br> <span style="color:orange;">Positive & negative cross loadings & correlated residuals (only in CFA)</span>|
|<span class = "fragment"> 5. Population Model</span >|<span class = "fragment"> 5-factor-SEM</span>| <span class="fragment"> 2-factor CFA & 5-factor SEM </span> |
|<span class = "fragment"> 6. Analysis |<span class = "fragment"> Aggregated relative values and parameter-wise </span> |<span class = "fragment"> Aggregated relative bias </span> |
:::

:::{.notes}
A closer look then enabled us to trace back our verbal claims in direct contrast to differences in certain decisions in our individual studies. [▶︎] Firstly "low" sample size included a size of 50 in Kosanke's study, while I started at 100. Secondly, I manipulated reliability by adjusting the indicator error variance, while Kosanke varied the factor loadings. Thirdly I only included positive cross-loadings and correlated residuals, while Kosanke also included negative cross-loadings and residual correlations. Fourth, I refer only to results from a 5-factor SEM with a strucutral while Kosankes findings are in large based on estimating CFA models.
:::

## Collaboration
<div class="scrollable-table">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Round 1</th>
      <th class="pastel-blue">Round 2</th>
      <th>Round 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Steps</strong></td>
      <td>Kriegmair</td>
      <td class="pastel-blue">Joint Study</td>
      <td>Kosanke</td>
    </tr>
    <!-- Step 2 -->
    <tr>
      <td><strong>Population Model</strong></td>
      <td>
        5-factor-SEM
      </td>
      <td class="pastel-blue fragment" data-fragment-index="2">5-factor-SEM</td>
      <td>
        5-factor-SEM<br>
        <span class="strike fragment" data-fragment-index="2">2-factor-CFA</span>
      </td>
    </tr>
    <!-- Step 3 -->
    <tr class="grey-text">
      <td><strong>Data Generation</strong></td>
      <td>
        parametric & normally distributed
      </td>
      <td class="pastel-blue fragment" data-fragment-index="3">parametric & normally distributed</td>
      <td>
        parametric & normally distributed
      </td>
    </tr>
    <!-- Step 4 -->
    <tr>
      <td><strong>Experimental Design</strong></td>
      <td>
        <span class="strike fragment" data-fragment-index="4">Misspecifications (+)</span><br>
        *N*: 100 - <span class="strike fragment" data-fragment-index="4">6400</span><br>
        Reliability via Θ
      </td>
      <td class="pastel-blue fragment" data-fragment-index="4">
        Misspecifications (+/-)<br>
        *N*: 50, 100, 250, 400<br>
        Reliability via Θ
      </td>
      <td>
        Misspecifications (+/-)<br>
        *N*: 50 - <span class="strike fragment" data-fragment-index="4">100,000</span><br>
        <span class="strike fragment" data-fragment-index="4">Reliability via λ</span>
      </td>
    </tr>
    <!-- Step 5 -->
    <tr>
      <td><strong>Method Selection</strong></td>
      <td>
        <span class="strike fragment" data-fragment-index="5">SEM-ML</span>, gSAM, lSAM (ULS & ML)
      </td>
      <td class="pastel-blue fragment" data-fragment-index="5">
        bounded SEM-ML, gSAM, lSAM (ULS & ML)
      </td>
      <td>
        bounded SEM (<span class="strike fragment" data-fragment-index="5">ULS</span> & ML), gSAM, lSAM (ULS & ML)
      </td>
    </tr>
    <!-- Step 6 -->
    <tr>
      <td><strong>Estimands</strong></td>
      <td>
        β: Fixed at 0.1 <span class="strike fragment" data-fragment-index="6"> and varied</span>
      </td>
      <td class="pastel-blue fragment" data-fragment-index="6">
        β: Fixed at 0.1
      </td>
      <td>
        β: Fixed at 0.1<br>
        <span class="strike fragment" data-fragment-index="6">φ: Fixed and varied</span>
      </td>
    </tr>
    <!-- Step 7 -->
    <tr>
      <td><strong>Performance Metrics</strong></td>
      <td>
        Absolute bias in absolute values <br>
        <span class="strike fragment" data-fragment-index="7">Signed Relative bias</span><br>
        95% CI coverage<br>
        Convergence & Improper Solutions
      </td>
      <td class="pastel-blue fragment" data-fragment-index="7">
        Absolute bias in absolute and sigend values<br>
        RMSE<br>
        95% CI coverage<br>
        Convergence & Improper Solutions
      </td>
      <td>
        Absolute bias in absolute values<br>
        <span class="strike fragment" data-fragment-index="7">Signed Relative bias</span><br>
        RMSE<br>
        95% CI coverage</td>
    </tr>
    <!-- Step 8 -->
    <tr class="grey-text">
      <td><strong>Software</strong></td>
      <td>
        lavaan::simulateData()
      </td>
      <td class="pastel-blue fragment" data-fragment-index="8">
        lavaan::simulateData()
      </td>
      <td>
        lavaan::simulateData()
      </td>
    </tr>
    <!-- Step 9 -->
    <tr>
      <td><strong>Analysis</strong></td>
      <td>
        Aggregated across parameters,<br> heat maps
      </td>
      <td class="pastel-blue fragment" data-fragment-index="9">
        Aggregated, <br> **parameter-wise**,<br>
        decision criteria <br> heat maps
      </td>
      <td>
        Aggregated across parameters,<br> decision criteria
      </td>
    </tr>
  </tbody>
</table>
</div>

<style>
  .scrollable-table {
    max-height: 65vh;
    overflow-y: auto;
    font-size: 0.6em;
  }
  .scrollable-table table {
    border-collapse: collapse;
    width: 100%;
  }
  .scrollable-table th, .scrollable-table td {
    border: 1px solid #ddd;
    padding: 4px;
    text-align: left;
  }
  .scrollable-table th {
    background-color: #f2f2f2;
    position: sticky;
    top: 0;
  }
  .strike {
    text-decoration: none;
  }
  .strike.visible {
    text-decoration: line-through;
  }
  .grey-text {
    color: grey;
  }
  .pastel-blue {
    background-color: #b3cde0;
  }
</style>

<script>
  // Auto-scroll logic
  const scrollableTable = document.querySelector('.scrollable-table');
  let scrollSpeed = 1; // Adjust this for faster/slower scrolling
  
  function autoScroll() {
    if (scrollableTable.scrollTop + scrollableTable.clientHeight >= scrollableTable.scrollHeight) {
      scrollableTable.scrollTop = 0;
    } else {
      scrollableTable.scrollTop += scrollSpeed;
    }
  }
  
  // Set timeout for scrolling to start after 30 seconds
  setTimeout(() => {
    // Set interval for scrolling
    let scrollInterval = setInterval(autoScroll, 50); // 50ms interval, adjust for smoothness
  }, 30000); // 30 seconds delay (30,000 milliseconds)
</script>


:::{.notes}
We then continued to stepwise align our decisions in round 2. Due to time limitations of the project we limited the joint study to a minimal scope. 

We agreed on a 5-factor SEM population model as SAM is particularly designed for complex models with a structural part while not tailored to CFA models. We kept the data generation and software the same as in our individual studies. We agreed on bounded ML SEM as the method of choice as this was brought up as one argument of why SAM has no better convergence rate than SEM. We limited the estimands to fixed coefficients as no relevant differences for the SAM vs SEM comparison were found here in my first studies. For performance metrics we agreed to additionally include a parameter-wise analysis of bias to get a better picture of the proclaimed negative bias of SAM depending on the sign of cross-loadings and residual correlations. 
:::

## Results of the Joint Study

## Convergence Rate <span style="font-size: 20px;"> after collaboration (with bounded-ML SEM) </span>

![](tables/convergence_rate_study3.png)

:::{.notes}
So first of all we found that the as proposed by the first study of Kosanke any convergence issues and issues of improper solutions in SEM could be resolved by using bounded ML SEM questioning whether SAM has a convergence rate advantage over traditional SEM.
:::


## Bias - parameter-wise <span style="font-size: 20px;"> after collaboration </span>

![](tables/abs_bias_parameterwise_study3.png)

:::{.notes}
We then looked at parameterwise bias values under positive and negative cross loadings and residual correlations to see if SAM outperforms SEM even in the presence of negative misspecifications and whether the claim that a negative bias of SAM in small samples cancels out the positive bias from positive misspecifications holds true in a more complex model with a structural part.

Here we can see that the sign switch of the cross loadings from positive to negative also leads to more negative biases in SEM mopared to SAM. This contradicts the claim that SAM outperforms SEM in the presence of negative misspecifications and we dont observe a more negative bias of SAM with negative cross loadings. 

For correlated residuals we can see no clear bias direction dependence on the presign of the correlation with mostly negative biases across the board.
:::

## Bias - aggregated <span style="font-size: 20px;"> after collaboration </span>

![](tables/aggregated_bias_study3.png)

:::{.notes}
When we then aggregate the bias in absolute values across all parameters we can then see that SAM still outperforms SEM in terms of bias in the cross loadings conditions independent of their presign in the low to moderate reliability conditions. We can also see that correlated residuals lead to no or a small benefit of SEM over SAM that is however less pronounced.
:::


##

Updating Verbal Claims
<br>
<br>
SAM has no convergence rate advantage over bounded ML SEM
<br>
<br>
SAM outperforms SEM despite of negative bias under positive and negative cross loadings in small samples and small to moderate reliability

:::{.notes}
So in conclusion we can say that SAM has no convergence rate advantage over bounded ML SEM and that SAM outperforms SEM despite of negative bias under positive and negative cross loadings in small samples and small to moderate reliability.
:::


## Evaluating the Adversarial Collaboration {auto-animate=true auto-animate-easing="ease-in-out"}

:::{.columns}
::: {.column width="25%"}
::: {.r-fit-text}
<br>
<br>

:::{.fragment fragment-index=2}
        
<span style="color: red;">Diverging operationalizations:</span>
<br> <span style="color: red;"> - Method </span> <br> <span style="color: red;"> - Design </span> <br> <span style="color: red;"> - Reliability etc. </span>

:::

:::{.fragment fragment-index=3}
</span>
<span style="color: black;">→ Reduced ambiguity

:::

:::{.fragment fragment-index=4}

→ Increased generalizability?  </span>

:::

<br>

:::{.fragment fragment-index=2}
<span style="color: green;">unified</span>
:::
:::
:::
::: {.column width="50%"}
::: {.r-fit-text}
```{dot}
digraph circular_process {
    // Use 'neato' layout for positioning
    layout=neato;
    overlap=false;
    splines=true;

    // Define global node attributes
    node [shape=plaintext, fontsize=12];

    // Define nodes with positions to form a circle
    hypothesize [label=< <font color="blue">Reformulated Verbal Claim</font> >, pos="0,1!"];
    operationalize [label=< <font color="green">Operationalizations</font> >, pos="-0.866,-0.5!", color="green"];
    gather_data [label=< <font color="green">Gather Data</font> >, pos="0.866,-0.5!", color="green"];

    // Define edges to create a circular process
    hypothesize -> operationalize [color="red"];
    operationalize -> gather_data [color="red"];
    gather_data -> hypothesize [color="red"];
}
```
:::
:::
:::

:::{.notes}
Assessing our case study we found that we were able to trace back our disagreeing verbal claims to diverging design choices in our individual studies. Through collaboration we were able to jointly map our verbal claims to a unified operationalization. This enabled us to find a reformulated verbal claim that integrates the differences of our individual studies. So we achieved a redction of ambiguity.
However, the question remains whether this process meaningfully increased generalizability of our verbal claims to the range of applications in which researchers might actually use SAM or SEM. 
:::

## Limitations and Future Directions

:::{.fragment}
- "Toy" case study for AC<br>
  low stakes & no "real" adversaries <br>
  <br>
:::

:::{.fragment}
- Increased Generalizability? <br>
:::
:::{.fragment}
  still limited to specific (somewhat less) arbitrary choices for simulation<br>
:::
:::{.fragment}
  → empirically ground simulations by sampling models (and data) from the literature (Taxonomy.jl)
:::

:::{.fragment}
  → Establish the practice of (at least low-code) adversarial simulation by all co-authors of a study?
:::

:::{.notes}
This brings me to the limitations and future directions. First of all we have to acknowledge that our case study was just a "toy" case study for adversarial collaboration. We were not real adversaries and the stakes were low as we do not actually come from disagreeing research backgrounds and have little to no experience in this field compared to more senior researchers.
Secondly, as just mentioned this process only partially increased generalizability as we still were limited to specific and somewhat arbitrary choices for our individual and joint simulations.

One way to address this would be to empirically ground simulations by sampling models and data from the literature. Our group is currently developing a Julia Package called Taxonomy.jl to facilitate this.

Another way would be to establish the practive of at least low-code adversarial simulation by all co-authors of a study. This would, at a cost of more resources, allow for a more diverse set of designs and thus a more generalizable set of results.

:::

## Discussion

1. Is the settling of verbal disputes through unified operationalizations really increasing generalizability?
2. Are individual studies in prior to adversarial collaboration beneficial?
3. How could an incentive structure be designed to encourage adversarial collaboration?
4. Could adversarial collaboration be implemented in the peer review process?


## References
