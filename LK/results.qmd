---
title: "Adversarial Simulation"
subtitle: "A case study"
author: "Leonard Kosanke"
format:
    pdf:
        fontsize: 11pt
        linestretch: 1.2
        geometry: "left=15mm, right=15mm, top=30mm, bottom=30mm"
        classoption: twoside
        papersize: a4
fontsize: 11pt
bibliography: ../bibliography.bib
engine: knitr
---

The git hash is: `{r} suppressWarnings(system2("git", c("rev-parse", "--short=5", "HEAD"), stdout = TRUE, stderr = TRUE))`

# Deviations from the simulation protocol

## Across studies

For pragmatic purposes of the simulation, we were unable to track and include non-convergence explicitly and exclude cases where it occured.

## Studies 1,2,3

For all these studies, 3 deviations are present.
Firstly, the total number of conditions turned out to be higher, as the calculation in the protocol forgot the addition of N=50 to be added to the factor sample size.
Secondly, It was not detailed what type of LSAM estimation will be implemented. For completeness sake, we included ML- as well as ULS-LSAM estimation.
Lastly, instead of just calculating the relative bias as originally planned, we also calculated the absolute bias to mitigate biases of opposite directions canceling each other out, as rightly pointed out in study 4 of @robitzsch_comparing_2022.

## Study 1b

In Study 1b, contrary to the simulation protocol, we added another sample size with N=50 to get a more nuanced understanding of the small-sample bias of LSAM. 
For the same reasons as in studies 1,2 and 3, we used ML as well as ULS LSAM estimation. Calculation of the relative Bias was not feasible in this study, as for the conditions where phi is 0, dividing by 0 would lead to infinite values.

## Study 4

Again, I forgot the addition of N=50 in the original condition calculation. 
Importantly, the model parameters and resulting population-level covariance matrices for the different DGM's provided by @robitzsch_comparing_2022 in their github repository were not the same as in the first paper by @rosseel_structural_2022: They omitted, for example, the cross-loading of the last factor in DGM2, without an apparent reason. Additionally, they had different values for the residual variances without giving insight into why they chose to do so and how (there was no calculation in the simulation script provided). For these reasons, and because of the fact that the calculation of the values in @rosseel_structural_2022 was replicable, we chose to use the values of @rosseel_structural_2022 by generating them with the same functions they used. In consequence, a fourth cross-loading was present in DGM 2.
The only difference in our data-simulation is that we only used the 3 out of 4 misspecification conditions that were present in @rosseel_structural_2022. 
Importantly, due to the implementation of the functions in the paper by @rosseel_structural_2022, the naming of the DGM's changed. In their paper, what @robitzsch_comparing_2022 called DGM1 is implemented with a value of 0. DGM 2 and 3 are thus also implemented with values 1 and 2. To avoid confusion, I will the terminology of the code I used, from @rosseel_structural_2022. Thus, the naming is now different when interpreting the results later on.

## Study 4a

I decided to include a variation of study 4a after all, as the relation between beta/phi values and N is interesting: 
The original paper showed that results differed between estimators, depending on the size of the factor correlation in Studies 5, 6 and 1b, as well as differential effects due to N in Study 1b.
We deemed it interesting to investigate these results in a model with a different number of factors.
These findings, combined with our goal to investigate realistic data conditions, rather than just the population level, left us to include Study 4a, with the addition of an additional factor sample size.
As DGM1 neither contained misspecification (which is central to our research question), nor did it lead to interesting results in the original study 4, it was omitted.
Similarly, as no substantial conclusions were drawn with regards to RMSE in the original papers study 1 and 2, we omitted its calculation as well.
Lastly, for the same reasons as argued for Study 4, we again created the DGM's based on the functions used in @rosseel_structural_2022.
As in Study 4, from now on we will also adhere to the naming that follows from the code taken from @rosseel_structural_2022, so that DGM's 1-3 are now DGM's 0-2.

## Studies 5,6

As we are not interested in the comparative performance of SAM vs. SEM at the population level for our research question, but in the performance comparison under realistic conditions, especially in small to moderate sample sizes, we decided to not replicate Studies 5 and 6 after all.
We deemed this a fair deviation from our simulation protocol, especially because no substantial performance differences arose in the results of these studies, and the most relevant aspect for us, the differential effect due to the size of factor correlation, is already covered in two different models in studies 1b and 4a.

# Result analysis

## Original paper

### Bias
In the original paper, @robitzsch_comparing_2022 had some inconsistencies when it came to the interpretation of the results of their simulation studies. 
They did not thoroughly define an a prior criterium for when an estimator outperformed another. 
Only in their Studies 5 and 6, they defined an absolute bias equal to or larger than 0.05 as relevant. 
Additionally, when an estimator had a minimum percentage error of 20% higher than another, they defined it as being better than the other, in these studies.
Across the other studies, interpretations sometimes varied. 
In some studies (e.g. Study 1), they seemed to orient on the same value of 0.05, but this time as an absolute difference value, without the addition of a certain percentage cut-off. 
In other studies (e.g. Study 2), they seemed to deviate from this value to define relevant difference in absolute bias.
Importantly, this deviation was present in both directions, and therefore did not appear to be consciously biased towards one direction of possible outcomes.
In consequence, it appears that interpretations aren't coherent across the studies.

Another important aspect to highlight in the original papers results refers to the general presence of a negative small sample bias in LSAM estimation. 
This bias makes LSAM appear to perform superior in conditions with positive values in unmodelled cross-loadings and residual correlation, but worse when the values are negative.
Even though this bias is correctly identified and explained already in Study 1, @robitzsch_comparing_2022 conduct multiple follow-up studies only investigating positive values in unmodelled cross-loadings and residual correlation, namely in study 3. 
This is done without a general note of some sorts, that this bias should apply in these data-generating scenarios the same as in Studies 1 and 2.
In consequence, the interpretations @robitzsch_comparing_2022 make, are somewhat misleading on first sight, if one does not account for this bias.
This does not, however, explain why they did not include more studies or conditions with negatively valenced residual correlations or cross-loadings. 
We hypothesize, that the goal of the paper was to show that even in these favorable constellations, SAM does not perform better than SEM.

To be fair, it should be stated that most of the final conclusions drawn in the paper are correct, anyway. 
With the use of the term "generally", @robitzsch_comparing_2022 are right in that there are many data constellations, were SAM does not outperform traditional SEM.
They also acknowledge, that for non-saturated structural models, SAM seems to be the better choice.

For our studies, in order to have a more coherent interpretation of results, we will define an estimator to outperform another, based on the cut-off of 0.05 for bias.

### SD and RMSE

Even though @robitzsch_comparing_2022 report tables with results for SD and RMSE, they rarely explicitly mentioned them in the results sections or give cut-offs for substantial relevance.

SD is only reported and mentioned in the conditions without misspecification in Study 1, even though also claimed to be calculated in studies 2 and 3. 
In this study, they mentioned a 6.6% reduction of SD for LSAM in comparison to ML. 
This calculation, however, is wrong as the reduction amounts to 8%.
For our interpretation, we will report percentage reductions larger than 5% as well and compare them with the original study.

RMSE (and average RMSE) is reported in Studies 1-4, but only in studies 2 and 4 explicitly interpreted for our estimators of interest.
In study 2, differences of up to 0.03 are present and interpreted as not being substantial differences.
In study 4, differences of 0.01 are present and interpreted as slight efficiency gains.
We infer that this lack of mentioning and consistency can be interpreted as there being no differences of interest. 
Thus, we expect our studies to yield similar results. 
If, however, there are differences large than 0.03, we will report them, without drawing a substantive conclusion.

## Packages
```{r}
library(knitr)
library(kableExtra)
library(tidyverse)
```

# Preliminary information for results

## Recalculation of CI's

We did multiple mistakes with the calculation of the confidence intervals, as we did not include the absolute, but relative mean in the two limits for studies 1b, 4 and 4a. 
Also, the standard error was wrongly calculated as relative.
For Studies 1, 2 and 3, we misscalculated the standard error, as well as the confidence interval by including absolute values in their calculation, even though we were interested in relative values in these studies.
Thus, we have to reextract the results for Bias correctly, which was done in postprocess_results.R.
All we did there, was reapply the corrected extract_results() function and the same report_bias() function from the individual simulations, one after another.
For the results of bias, we load these processed files, instead of the original ones, from the newly created folder: SimulationResultsProcessed.

# Results Simulation 1: 2-factor-CFA with 0,1 or 2 correlated residuals

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s1 <- readRDS("SimulationResults/sim1_results_error.rds")

#errors
errors_s1 <- all_messages_s1$errors
errors_sum_s1 <- unlist(errors_s1)
length(errors_sum_s1)

#warnings
warnings_s1 <- all_messages_s1$warnings
warnings_sum_s1 <- unlist(warnings_s1)
length(warnings_sum_s1)

#messages
messages_s1 <- all_messages_s1$messages
messages_sum_s1 <- unlist(messages_s1)
length(messages_sum_s1)

```

No errors and messages were present. There were, however, 245 warnings, we will investigate in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".
```{r}
#| ouput: true

unique(warnings_s1)

```
 
This warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Unfortunately, due to the implementation of the study, we are unable to check for more details, as whether there is any kind of pattern with regards to the warning occurrence. 
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
Even though in @robitzsch_comparing_2022 the authors claim that constraining estimation to realistic values (e.g. loadings between 0 and 1) should solve this, applying these same constraints still lead to these issues in our study.
The combined study should check this more properly to evaluate these claims thoroughly.
As the 245 are less than 0.5% of the 525000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

### Print all tables

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResultsProcessed/sim1_rel_bias_ci.rds")

# Display the relative bias results for each condition, without formatting
#for (condition in names(bias_ci)) {
#  cat("## Condition:", condition, "\n\n")
#  print(bias_ci[[condition]])
#  cat("\n\n")
#}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  kbl(data, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s1)) {
  print(create_styled_table(bias_ci_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------


### Print all tables

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResultsProcessed/sim1_rel_bias_ci.rds")

# Display the relative bias results for each condition, without formatting
#for (condition in names(bias_ci)) {
#  cat("## Condition:", condition, "\n\n")
#  print(bias_ci[[condition]])
#  cat("\n\n")
#}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  kbl(data, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s1)) {
  print(create_styled_table(bias_ci_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

### Condition without correlated residuals (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["0_0.12"]], "0_0.12"))

```

Both standard SEM estimators are unbiased (i.e. relative Bias < 0.05) across all N conditions.
The 4 SAM estimators are negatively biased for sample sizes up to 500 and have nearly identical estimation results.
Across all estimators, the relative Bias decreases with increasing N. 

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is no missspecification.

### Condition with one negative residual correlation (1_-0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["1_-0.12"]], "1_-0.12"))

```

All estimators are negatively biased (i.e. relative Bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, it increases slightly but unsubstantially up to N=500, but is lowest overall.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is one negative residual.

### Condition with one positive residual correlation (1_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["1_0.12"]], "1_0.12"))

```

Besides SEM-ML in N=50, both SEM estimators are positively biased across all N, with no visible trend for increasing N.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased up to N=250 and positively biased for the other N conditions. 
For N=250 and N=500, their relative bias is unsubstantial.
In relative comparison, the two SEM estimators outperform all SAM estimators for sample sizes up to 100. For all higher N, the SAM estimators outperform the SEM estimators, even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in smaller samples (N=50-100), whereas SAM outperforms SEM in moderate samples (N250-500), and slightly in higher sample sizes.

### Condition with two negative residual correlations (2_-0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["2_-0.12"]], "2_-0.12"))

```

All estimators are negatively biased across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, it slightly, but unsubstantially increases up to N=500, but is lowest overall in these conditions.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there are two unmodelled negative residual correlations.

### Condition with two positive residual correlation (2_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["2_0.12"]], "2_0.12"))

```

Both SEM estimators are positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased up to N=100 and positively biased for the other N conditions. 
In relative comparison, the two SEM estimators outperform all SAM estimators for N=50. For all higher N, the SAM estimators outperform the SEM estimators (substantially, besides ML in N=100), even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in small samples (N=50), whereas SAM outperforms SEM in small to moderate samples (N100-500), and slightly in higher sample sizes.

### Summary

in 0.5% of estimations, a warning referring to potential problems with convergence or improper solutions due to small sample sizes in one or multiple estimators occured. 
This should be investigated more explicitly in the joint study.

In conditions with no, one or two negative residual correlations,  SEM outperforms SAM in small to moderate sample sizes. 
This seems to be independent of the amount of misspecification (1 vs. 2).

Comparing the conditions with one and two positive residual correlations, the results suggest that SAM outperforms SEM in smaller to moderate samples, with increasing amount of misspecifiation (even better for 2 than for 1 residual correlation). In small samples, SEM tends to perform better.

### Comparison with original paper

Comparing these results with the original paper, the results are  similar and can be seen as mostly replicated. The only difference lies in the potential convergence issues in a small number of estimations. 
Besides that, we draw the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appear to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This leads to the comparatively worse performance in the conditions with no, one or two negative residual correlations. Additionally, this negative bias corrects for unmodelled positive residual correlations and makes SAM appear superior in these conditions.


## SD

General interpretation: Substantial if percentage reductions are larger than 5%

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------

### print all tables
```{r}
#| ouput: true

# Load the relative bias results
sd_s1 <- readRDS("SimulationResults/sim1_sd.rds")


# Loop through each condition and print the styled table
for (condition in names(sd_s1)) {
  print(create_styled_table(sd_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}
```

### No residual correlations (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(sd_s1[["0_0.12"]], "0_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

There are only two substantial differences:
Substantial percentage reduction of SD up to 22.4% when comparing SEM-ML and LSAM-ML in N=50: 
0.299*x = 0.232
0.232/0.299 = 0.775
1-(0.232/0.299) = *0.224*

Substantial percentage reduction of SD up to 8.6% when comparing SEM-ML and LSAM-ML in N=100:
1-(0.19/0.208)

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples

### One negative residual correlation (1_-0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["1_-0.12"]], "1_-0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

There are only two substantial differences:
Substantial percentage reduction of SD up to 27.4% when comparing SEM-ML and LSAM-ML in N=50: 
1-(0.233/ 0.321)

Substantial percentage reduction of SD up to 12.2% when comparing SEM-ML and LSAM-ML in N=100:
1-(0.187/0.213)

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### One positive residual correlation (1_0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["1_0.12"]], "1_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 16.9%.
ULS-SEM outperforms all SAM estimators with a percentage reduction of SD up to 8.16% in N=100-250.
No substantial differences between the two SEM and all SAM estimators starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperforms SEM in terms of SD reduction in small samples, whereas SEM outperforms SAM in moderate samples.

### Two negative residual correlations (2_-0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["2_-0.12"]], "2_-0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 29% and in N=100 with up to 12.8%.
For higher N, there are no substantial differences.

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### Two positive residual correlations (2_0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["2_0.12"]], "2_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 15.2%.
Both SEM estimators ourperform the SAM estimators in N=100 with a percentage reduction of SD up to 11.1% and in N=250 with up to 10.3%.
For higher N, there are no substantial differences.

Thus, SAM outperforms SEM in terms of SD reduction in small samples, but SEM outperforms SAM in smaller to moderate samples.

### Summary and comparison with original paper

Overall, SAM outperforms SEM in terms of SD reduction in small to smaller samples, especially for negatively correlated residuals. 
In smaller to moderate samples, SEM outperforms SAM only for positively correlated residuals.
For conditions with no misspecification, which were the only conditions reported in @robitzsch_comparing_2022, the percentage reduction favoring SAM is nearly identical for N=100, and the reduction seems to be even more significantly in favor of SEM in smaller samples.

## RMSE
differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N

-----------------

###Print all tables
```{r}
rmse_s1 <- readRDS("SimulationResults/sim1_rmse.rds")

```

### No residual correlations (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["0_0.12"]], "0_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
In N=100, both SEM estimators were more efficient than all  SAM estimators.
Besides, there are no substantial differences present.

### One negative residual correlation (1_-0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["1_-0.12"]], "1_-0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-250), both SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### One positive residual correlation (1_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["1_0.12"]], "1_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For smaller samples (N=50-100), SEM-ULS was more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### Two negative residual correlations (2_-0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["2_-0.12"]], "2_-0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-500), The two SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### Two positive residual correlations (2_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["2_0.12"]], "2_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For moderate samples (N=250-500), all SAM estimators were more efficient than the two SEM estimators.
For higher and lower N, there are no substantial differences present.

### Summary

Overall, classic SEM estimation was more efficient than SAM estimation in small to moderate samples. 
Only in the case of two positive residual correlation, SAM was more efficient than SEM in moderate samples.

## Summary Study 1

We gained the following insights with regards to the 2-factor-CFA model:
Unlike the original paper by @robitzsch_comparing_2022, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of bias in small to moderate samples, SEM outperforms SAM in conditions with no, one and two negative residual correlations. SAM appears to outperform SEM for positive residual correlations. This, however, is an artifact of its negative small sample bias and not its performance. The results align with the ones obtained by the original paper.

With regards to SD and RMSE, results are incoherent with no clear pattern emerging. 

Thus, it cannot be stated that one estimation method is generally performing better than the other, based on the results of Study 1.

# Results Simulation 1b: 2-factor-CFA with 2 residual correlations and varying lambda and phi

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s1b <- readRDS("SimulationResults/sim1b_results_error.rds")

#errors
errors_s1b <- all_messages_s1b$errors
errors_sum_s1b <- unlist(errors_s1b)
length(errors_sum_s1b)

#warnings
warnings_s1b <- all_messages_s1b$warnings
warnings_sum_s1b <- unlist(warnings_s1b)
length(warnings_sum_s1b)

#messages
messages_s1b <- all_messages_s1b$messages
messages_sum_s1b <- unlist(messages_s1b)
length(messages_sum_s1b)

```

No errors and messages were present. There were, however, 414 warnings, we will investigate in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

```{r}
unique(warnings_s1b)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 414 estimation are around 0.5% of the 75000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within table condition:

- Unbiased judgement overall

- ALL across increasing lambda

- ALL across increasing phi

- All within one lambda 
- All within one phi

Layers between tables:

-Comparison between estimators

-Comparison between N's

-----------------

```{r}
#Load processed results
bias_ci_s1b <- readRDS("SimulationResultsProcessed/sim1b_abs_bias_ci.rds")

```


### Conditions with N=50

```{r}
#| ouput: true

#Normal print
#print(bias_ci_s1b[["N_50"]][["LSAM_ML"]])
#print(bias_ci_s1b[["N_50"]][["LSAM_ULS"]])

#Formatted tables
kbl(bias_ci_s1b[["N_50"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=50" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_50"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ULS for N=50" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)


```

Comparing LSAM-ML and LSAM-ULS, the values are nearly identical. 
Therefore, the interpretation is the same for both estimators.

Across all conditions, both estimators were biased.

For higher values of lambda, the estimators had less absolute Bias. This difference increased for higher phi values.

For lower values of lambda, the absolute bias increased with increasing phi. This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.

### Conditions with N=100

```{r}
#| ouput: true

kbl(bias_ci_s1b[["N_100"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=100" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_100"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ULS for N=100" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```
Again, the values and therefore the interpretation of LSAM-ML and LSAM-ULS are identical.

Across all conditions, both estimators were biased.
For higher values of lambda, the estimators had less absolute Bias. This difference increased for higher phi values.
For lower values of lambda, the absolute bias increased with increasing phi. 
This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.

Additionally, there seems to be no differential effect due to N, as the interpretations between the different N-conditions are identical.
Naturally, the bias values are larger for N=50, still.

### Comparison to original paper

In contrast to the original paper by @robitzsch_comparing_2022, we computed absolute bias to avoid infinite values in some conditions.
Additionally, computing absolute instead of relative Bias solves the problem of positive and negative biases canceling each other out.
Results partly align in terms of absolute bias:
For higher values of lambda, LSAM has less absolute Bias. 
This difference increases for higher phi values.
Only for low values of lambda, the bias increases for higher values of phi. 
Unlike in this study, the original paper did not find a reversal effect for higher lambda values.
Another difference is that we, unlike @robitzsch_comparing_2022, found estimates to be biased for low phi-values.
This might be due to the difference of using absolute bias, thereby avoiding cancellation of positive and negative values.
Still, our general conclusion corresponds to the original paper:
LSAM has a general negative small sample bias. 
The bias is larger in magnitude for lower loadings and higher true factor correlations.
Importantly, the bias seems not to disappear for low values of phi and lambda, unlike in the original paper.

# Results Simulation 2: 2-factor-CFA with 1 or 2 cross-loadings

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s2 <- readRDS("SimulationResults/sim2_results_error.rds")

#errors
errors_s2 <- all_messages_s2$errors
errors_sum_s2 <- unlist(errors_s2)
length(errors_sum_s2)

#warnings
warnings_s2 <- all_messages_s2$warnings
warnings_sum_s2 <- unlist(warnings_s2)
length(warnings_sum_s2)

#messages
messages_s2 <- all_messages_s2$messages
messages_sum_s2 <- unlist(messages_s2)
length(messages_sum_s2)


```

No errors and messages were present. There were, however, 612 warnings we will investigate in detail.


### Warnings investigation
Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

Sometimes 4-6 warnings per repetition, more than before (up to 2 per repetition)

```{r}
unique(warnings_s2)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 612 estimation are around 1.5% of the 42000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------


### Print all tables
```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s2 <- readRDS("SimulationResultsProcessed/sim2_rel_bias_ci.rds")

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s2)) {
  print(create_styled_table(bias_ci_s2[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

### One negative cross-loading (1_-0.3)
```{r}

print(create_styled_table(bias_ci_s2[["1_-0.3"]], "1_-0.3"))

```


All estimators are negatively biased (i.e. relative Bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, the bias stays constant across N, and is lowest by a substantial marging compared to all SAM estimators up until N=500.
For higher N, SEM-ML is the best, but the differences between all estimators are negligible
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is one negative cross-loading.

### One positive cross-loadings (1_0.3)
```{r}

print(create_styled_table(bias_ci_s2[["1_0.3"]], "1_0.3"))

```

All estimators are biased across all N conditions.
For the two SEM-estimators, the relative Bias is always positive and shows no trend across N.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
Their bias turn from being negative for N=50-100 to positive from N=250 on.
In relative comparison, the two SEM estimators outperform all SAM estimators for N=50. For N=100-2500, the SAM estimators outperform the SEM estimators, even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in small samples (N=50), whereas SAM outperforms SEM in smaller to moderate samples (N100-500), and slightly in higher sample sizes.

### Two negative cross-loadings (2_-0.3)
```{r}

print(create_styled_table(bias_ci_s2[["2_-0.3"]], "2_-0.3"))

```
All estimators are negatively biased across all N conditions.
Across all estimators, the Bias decreases substantially with increasing N.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators substantially outperform all SAM estimators in every condition of N.

Thus, SEM estimation outperforms SAM, when there are two negative residuals.

### Two positive cross-loadings (2_0.3)
```{r}

print(create_styled_table(bias_ci_s2[["2_0.3"]], "2_0.3"))

```

All estimators are biased across N conditions.
Both SEM estimators are positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased in N=50 and positively biased for the other N conditions. 

In relative comparison, the four SAM estimators outperform the two SEM estimators for N=50-500, even though unsubstantial for N=500. For all higher N, no estimator substantially differs from another, but the two SEM slightly outperform.

Thus, SAM outperforms SEM in small to moderate samples (N50-250).

### Summary

Again, in 1.5% of estimations, a warning referring to potential problems with convergence or improper solutions due to small sample sizes in one or multiple estimators occured. 
This should be investigated more explicitly in the joint study.

For small to moderate samples, SEM outperforms SAM in conditions with negative cross-loadings. SAM outperforms SEM in conditions with positive cross-loadings.

### Comparison with original paper

Comparing these results with the original paper, the results are  similar and can be seen as replicated. The only difference lies in the potential convergence issues in a small number of estimations. 

Besides that, we draw the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appear to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This leads to the comparatively worse performance in the conditions with no, one or two negative residual correlations. Additionally, this negative bias corrects for unmodelled positive cross-loadings and makes SAM appear superior in these conditions.

## SD

General interpretation: Substantial if percentage reductions are larger than 5%

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------

### print all tables
```{r}
#| ouput: true

# Load the relative bias results
sd_s2 <- readRDS("SimulationResults/sim2_sd.rds")


# Loop through each condition and print the styled table
for (condition in names(sd_s2)) {
  print(create_styled_table(sd_s2[[condition]], condition))
  cat("\n\n")  # Add space between tables
}
```

### One negative cross-loading (1_-0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["1_-0.3"]], "1_-0.3"))

```


Across all estimators, the standard deviation decreases with increasing N. 
There are only two substantial differences:
Substantial percentage reduction of SD up to 30.8% when comparing SEM-ML and LSAM-ML in N=50: 
Substantial percentage reduction of SD up to 16% when comparing SEM-ML and LSAM-ML in N=100:

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### One positive cross-loading (1_0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["1_0.3"]], "1_0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 6.4%.
ULS-SEM outperforms all SAM estimators with a percentage reduction of SD up to 14.1% in N=100, and of 12.3% in N=250 and of 8.6% in N=500.
Above that point, overall SD is very small and negligible in all estimators
No substantial differences between the two SEM and all SAM estimators starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperforms SEM in terms of SD reduction in small samples, whereas SEM outperforms SAM in smaller to moderate samples.

### Two negative cross-loadings (2_-0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["2_-0.3"]], "2_-0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators substantially, but decreasingly outperform both SEM estimators in samples N=50-500 with a SD percentage reduction of 10.1% remaining in N=500.
For higher N, there are no substantial differences and overall SD is very small across estimators.

Thus, SAM outperforms SEM in terms of SD reduction in small to moderate samples.

### Two positive cross-loadings (2_0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["2_0.3"]], "2_0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

Both SEM estimators outperform the SAM estimators in N=50-500 with a SD percentage reduction up to 11.7% remaining in N=500.
For higher N, there are no substantial differences.

Thus, SEM outperforms SAM in small to moderate samples.


### Summary

For negative cross-loadings, SAM tends to outperform SEM in terms of SD reduction small to moderate samples.
For positive cross-loadings, SEM tends to outperform SAM in terms of SD reduction smaller to moderate samples.
There seems to be an effect due to the amount of misspecification, such that the conditions with 2 cross-loadings had larger differences in performance (in both directions).

## RMSE
differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N

-----------------

###Print all tables
```{r}
rmse_s2 <- readRDS("SimulationResults/sim2_rmse.rds")

```

### One negative cross-loading (1_-0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["1_-0.3"]], "1_-0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-500), both SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### One positive cross-loading (1_0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["1_0.3"]], "1_0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For smaller and large samples (N=50-100 and N=1000-100000), none of the estimators differed from another.
For N=250, all 4 SAM estimators substantially outperformed the two SEM estimators.
For N=500, all 4 SAM estimators substantially outperformed SEM-ML estimation.

### Two negative cross-loadings (2_-0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["2_-0.3"]], "2_-0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For small samples with N=50, none of the estimators differed in terms of efficiency.
For all other conditions, The two SEM estimators were more efficient than all SAM estimators.

### Two positive cross-loadings (2_0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["2_0.3"]], "2_0.3"))

```

Only for the two SEM estimators, the RMSE decreases with increasing N, but remains at higher values between 0.22-0.25 for all estimators.

For small to moderate samples (N=50-250), all SAM estimators were more efficient than the two SEM estimators.
For higher N, there are no substantial differences present.

### Summary

In moderate samples, SAM generally outperformed SEM in terms of RMSE.
The only exception is the condition with 1 negative cross-loading, where SEM outperformed SAM in small to moderate samples.
In the condition with 2 positive cross-loadings, SAM was more efficient in small samples as well.
Unlike in Study 1, there seems to be no effect due to amount of misspecification.

## Summary Study 2

We gained the following insights with regards to the 2-factor-CFA model:
Unlike the original paper by @robitzsch_comparing_2022, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of bias in small to moderate samples, SEM outperformed SAM in conditions with negative cross-loadings. 
SAM appeared to outperform SEM in conditions with positive cross-loadings. 
This, however, is again an artifact of its negative small sample bias and not its performance. 
The results align with the ones obtained by the original paper.

With regards to SD reduction, findings were different. 
In conditions with negative cross-loadings, SAM tended to outperform SEM, wheras the opposite was true for conditions with positive cross-loadings.
Here, an effect due to the amount of misspecification seemed to be present, like for RMSE in Study 1.

In terms of RMSE, results differed again. 
Generally, SAM was more efficient than SEM in moderate samples. 
For small samples, the results were not as clear.
Unlike in Study 1, there seems to be no effect due to amount of misspecification.

Overall, we can infer that findings are somewhat incoherent and it can not be stated that one estimation method is generally performing better than the other, based on the results of Study 2.

# Results simulation 3:2-factor-CFA with one cross-loading and one residual correlation

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s3 <- readRDS("SimulationResults/sim3_results_error.rds")

#errors
errors_s3 <- all_messages_s3$errors
errors_sum_s3 <- unlist(errors_s3)
length(errors_sum_s3)

#warnings
warnings_s3 <- all_messages_s3$warnings
warnings_sum_s3 <- unlist(warnings_s3)
length(warnings_sum_s3)

#messages
messages_s3 <- all_messages_s3$messages
messages_sum_s3 <- unlist(messages_s3)
length(messages_sum_s3)


```

No errors and messages were present. There were, however, 193 warnings we will investigate in detail.

### Warnings investigation
Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

```{r}
unique(warnings_s3)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 193 estimation are around 1.8% of the 10500 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s3 <- readRDS("SimulationResultsProcessed/sim3_rel_bias_ci.rds")

#Format table for results
kbl(bias_ci_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

All estimators were biased for every condition of N.
The SAM estimators had nearly identical values.
Besides SEM-ULS, all estimators' Bias increased substantially, even though the increase was stronger for the SAM estimators.
Only the SAM estimators had negative Bias for N=50-100.
For N=50, none of the estimators outperformed another.
For N=100-500, all SAM estimators outperformed the two SEM estimators.
For higher N, none of the estimators substantially differed from another.

Thus, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

### Comparison with original paper

Comparing these results with the original paper, our results were somewhat different.
Even though the results were similar for most conditions (@robitzsch_comparing_2022 had the same results in conditions N=100-250 and N=1000-100000), the results were different for N=500.
This, combined with the findings from the additional condition of N=50 revealed that SAM estimation outperformed SEM estimation in this data generating scenario in small to moderate samples.
Another difference lies, again, in the potential convergence issues in a small number of estimations. 

## SD

```{r}
#| ouput: true

metrics_s3 <- readRDS("SimulationResults/sim3_metrics_list.rds")

# Load the SD results
sd_s3 <- metrics_s3[["sd"]]

kbl(sd_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

Across all estimators, the standard deviation decreases with increasing N. 
There are thre substantial differences:
Substantial percentage reduction of SD up to 5.5% in favor of SEM-ULS when comparing with GSAM-ULS in N=50. 
Substantial percentage reduction of SD up to 21.7% when comparing SEM-ULS when comparing with GSAM-ULS in N=100.
Substantial percentage reduction of SD up to 15.6% when comparing SEM-ULS when comparing with GSAM-ULS in N=250.
For higher N, the SD still partly differs but becomes negligibly small.

Thus, SEM outperforms SAM in terms of SD reduction in small to moderate samples.

## RMSE

```{r}
#| ouput: true

# Load the rmse results
rmse_s3 <-  metrics_s3[["rmse"]]

#Format table for results
kbl(rmse_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

Across all estimators, the RMSE decreased with increasing N.
For N=50 and N=1000-100000, none of the estimators differed from another.
For N=100-500, the two SEM-estimators outperform all SAM-estimators in terms of efficiency.

Thus, SEM outperforms SAM in smaller to moderate samples.

## Summary Study 3
As before, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of Bias, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

These results differed from the ones obtained with regards to SD and RMSE, where SEM tended to outperform SAM in smaller to moderate samples.

Importantly, one has to bear in mind that same as before, SAM only appears to be more robust against unmodelled positive residual correlations and cross-loadings due to its general negative bias in small samples. 
As in this study, same as in @robitzsch_comparing_2022, the misspecification values were positive, we again cannot conclude that SAM performs better than SEM.

# Summary 2-factor-CFA models (Studies 1-3)

SAM does not generally outperform SEM in small to moderate samples. 
SAM exhibits a negative small sample bias that makes SAM appear superior in conditions with unmodelled positive cross-loadings and residual correlations.
This bias is especially strong for lower lambda and higher phi values.
If there is no misspecification or unmodelled negative cross-loadings and residual correlations, SAM tends to perform even worse than traditional SEM.

# Results Simulation 4: 5-factor-model under 3 different data-generating mechanisms

Importantly, for this study, we do not investigate CFA's anymore, but models with regressions included.
Also, keep in mind that the naming of the DGM changed to correspond with the code implementation. 
What @robitzsch_comparing_2022 called DGM 1-3, we now will call DGM 0-2.

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s4 <- readRDS("SimulationResults/sim4_results_error.rds")

#errors
errors_s4 <- all_messages_s4$errors
errors_sum_s4 <- unlist(errors_s4)
length(errors_sum_s4)

#warnings
warnings_s4 <- all_messages_s4$warnings
warnings_sum_s4 <- unlist(warnings_s4)
length(warnings_sum_s4)

#messages
messages_s4 <- all_messages_s4$messages
messages_sum_s4 <- unlist(messages_s4)
length(messages_sum_s4)

```

No errors and messages were present. There were, however, 9007 warnings, we will investigate in detail.

### Warnings investigation

```{r}
#| ouput: true

# Flatten the list of warnings into a single character vector
all_warnings <- unlist(warnings_s4)

# Count the occurrences of each unique warning
warning_counts <- table(all_warnings)

# Convert to data frame for better formatting
warning_counts_df <- as.data.frame(warning_counts)
colnames(warning_counts_df) <- c("Warning Message", "Count")

# Display the data frame
kable(warning_counts_df, format = "html") %>%
  kable_styling(full_width = F, position = "left")
```

The three warnings that amount to 7 overall, are referring to problems with regards to positive definite matrices or model identification.
This suggests potential issues with multicollinearity or redundancy among latent variables, but only in a negligible number of estimations.
The 9000 warnings with regards to Gamma computation in smaller samples is negligible as well, as we are not interested in the computation of robust fit indices.


## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------

### Print all tables

```{r}
#| ouput: true

# Load the absolute bias results
bias_ci_s4 <- readRDS("SimulationResultsProcessed/sim4_abs_bias_ci.rds")


# Loop through each condition and print the styled table from Study 1 function
for (condition in names(bias_ci_s4)) {
  print(create_styled_table(bias_ci_s4[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

### Conditions without misspecification (DGM 0)

```{r}

 print(create_styled_table(bias_ci_s4[["DGM_0"]], "DGM_0"))
```


All estimators are biased for small to moderate samples (N=50-250). 
For higher N, no estimator is biased.
In N=50, LSAM outperforms SEM-ULS. 
For all other conditions, there is no substantial difference between any of the estimators biases.

Thus, for the most part, no estimator generally outperforms another in the correctly specified model conditions.

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r}

 print(create_styled_table(bias_ci_s4[["DGM_1"]], "DGM_1"))

```

All estimators were biased across all conditions of N.
Still, their bias decreases for higher N.
LSAM-ML substantially outperforms the two SEM-estimators in smaller samples (N=50-100).
It outperforms SEM-ULS in every other condition of N, as well.
With regards to SEM-ML, LSAM-ML only slightly outperforms, for higher N.

Thus, SAM tends to outperforms SEM in presence of unmodelled cross-loadings.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r}

 print(create_styled_table(bias_ci_s4[["DGM_2"]], "DGM_2"))

```

All estimators are biased for small to moderate samples (N=50-500).
Only SEM-ULS is unbiased for N=500.
For higher N (N=1000-100000), no estimator is biased.
In N=50, LSAM outperforms SEM-ULS estimation. 
For all other conditions, there is no substantial difference between any of the estimators bias.

### Summary

LSAM-estimation appears to generally outperform SEM-ULS estimation in smaller samples across all DGM's.
Moreover, LSAM- appeared to outperform both SEM-estimators in smaller samples, when unmodelled cross-loadings are present.
Importantly, as we found LSAM to have a negative small sample bias in conditions with low values of phi as well in our simulation study 1b, we have to take this into account:
Even though LSAM appears to outperform SEM-estimation in DGM 1, this should be attributed to its general negative small sample bias.
Thus, it cannot simply be stated, that SAM estimation should be preferred in these scenarios.

In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there is no general advantage of one estimation method over another, for smaller to moderate samples.

### Comparison with original paper

Interestingly, unlike @robitzsch_comparing_2022, we have found some bias in DGM 0 even though the model was correctly specified in that scenario.

For DMG 1, results are very similar, with LSAM-ML seemingly outperforming SEM-ULS estimation.
A new insight is that LSAM-ML seems to outperform SEM-ML in smaller samples as well.

In DGM 2, we did not find the disadvantage of SEM-ML in comparison to the other estimators that @robitzsch_comparing_2022 found.

An additional finding is, that LSAM-ML tends to generally perform better than SEM-ULS in smaller samples.

Importantly, our conclusion with regards to LSAM's performance is different, due to the different findings in our Study 1b, that found LSAM to be biased even for low values of phi and lambda.

## RMSE

differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N

-----------------

###Print all tables
```{r}
rmse_s4 <- readRDS("SimulationResults/sim4_rmse.rds")

```

### Conditions without misspecification (DGM 0)

```{r}
#| ouput: true

print(create_styled_table(rmse_s4[["DGM_0"]], "DGM_0"))

```

Across increasing N, the average RMSE decreases substantially for all estimators.
For N=50, SEM-ULS performs substantially worse than the other two estimators.
For all other conditions, there is no difference between any of the estimators.

Thus, no estimator generally outperforms another in conditions with a correctly specified model.

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r}
#| ouput: true

print(create_styled_table(rmse_s4[["DGM_1"]], "DGM_1"))

```

Across increasing N, the average RMSE decreases substantially for all estimators.
Across all N, SEM-ULS performs substantially worse than the other two estimators.
For N=50-500, SEM-ML performs substantially worse than LSAM-ML.
For higher N, SEM-ML performs worse than LSAM-ML as well, but only slightly.

Thus, LSAM-ML appears to outperform the two SEM-estimators for small to moderate samples sizes, in the conditions with four unmodelled cross-loadings.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r}

 print(create_styled_table(rmse_s4[["DGM_2"]], "DGM_2"))

```

Across increasing N, the average RMSE decreases substantially for all estimators.
For N=50, SEM-ULS performs substantially worse than the other two estimators.
For all other conditions, there is no difference between any of the estimators.

Thus, no estimator generally outperforms another in conditions with 20 unmodelled residual correlation.

### Summary

LSAM-ML appears to outperfoms the two SEM-estimators in conditions with unmodelled cross-loadings. 
For conditions without misspecifications and unmodelled residual correlations, no difference arises besides a worse performance of SEM-ULS.

### Comparison with original paper

The results align closely to the findings in @robitzsch_comparing_2022.
A new insight is the comparatively worse performance of SEM-ULS in small samples (N=50).

## Summary Study 4

Overall, the main performance difference is that LSAM appears to be outperforming the two SEM-estimators in smaller to moderate samples sizes, in conditions with unmodelled cross-loadings (DGM1).
Even though LSAM appears to outperform SEM-estimation in DGM 1, this should be attributed to its general negative small sample bias.
Thus, it cannot simply be stated, that SAM estimation should be preferred in these scenarios.

In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there is no general advantage of one estimation method over another.
A new insight is the comparatively worse performance of SEM-ULS in small samples (N=50).
Due to our different findings with regards to the small sample bias of LSAM in Study 1b, we arrive at different conclusions than @robitzsch_comparing_2022.

# Results Simulation 4a: 5-factor-model under 2 different data-generating mechanisms and varying beta size

Again, we investigated a five factor model with regressions included, and this time under vary size of the regression coefficient beta.
Unlike the original paper, were the investigation was only conducted on the population level, we also varied the sample size as a factor in our simulation.
Also, keep in mind that the naming of the DGM changed to correspond with the code implementation. 
What @robitzsch_comparing_2022 called DGM 1-3, we now will call DGM 0-2.

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s4a <- readRDS("SimulationResults/sim4_results_error.rds")

#errors
errors_s4a <- all_messages_s4a$errors
errors_sum_s4a <- unlist(errors_s4a)
length(errors_sum_s4a)

#warnings
warnings_s4a <- all_messages_s4a$warnings
warnings_sum_s4a <- unlist(warnings_s4a)
length(warnings_sum_s4a)

#messages
messages_s4a <- all_messages_s4a$messages
messages_sum_s4a <- unlist(messages_s4a)
length(messages_sum_s4a)

```

No errors and messages were present. There were, however, 9007 warnings, we will investigate in detail.

### Warnings investigation

```{r}
#| ouput: true

# Flatten the list of warnings into a single character vector
all_warnings <- unlist(warnings_s4a)

# Count the occurrences of each unique warning
warning_counts <- table(all_warnings)

# Convert to data frame for better formatting
warning_counts_df <- as.data.frame(warning_counts)
colnames(warning_counts_df) <- c("Warning Message", "Count")

# Display the data frame
kable(warning_counts_df, format = "html") %>%
  kable_styling(full_width = F, position = "left")
```

Identically as in Study 4, the three warnings that amount to 7 overall, are referring to problems with regards to positive definite matrices or model identification.
This suggests potential issues with multicollinearity or redundancy among latent variables, but only in a negligible number of estimations.
The 9000 warnings with regards to Gamma computation in smaller samples is negligible as well, as we are not interested in the computation of robust fit indices.

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within table condition:

- Unbiased judgement overall

- ALL across increasing N

- ALL across increasing beta

- All within one N 
- All within one beta

Layers between tables:

-Comparison between estimators

-Comparison between DGMs

-----------------

```{r}
#| ouput: true

# Load the absolute bias results
bias_ci_s4a <- readRDS("SimulationResultsProcessed/sim4a_abs_bias_ci.rds")

```

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r}

#Normal print
#print(bias_ci_s4a[["DGM_1"]][["SEM_ML_metrics"]])
#print(bias_ci_s4a[["DGM_1"]][["SEM_ULS_metrics"]])
#print(bias_ci_s4a[["DGM_1"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(bias_ci_s4a[["DGM_1"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of SEM-ML for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_1"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of SEM-ULS for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_1"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of LSAM-ML for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

All three estimators were biased across all conditions.
The only exception is LSAM for very large samples (N=2500-100.000) and beta=0.3.
For all three estimators, across increasing N, the bias decreased substantially, in all conditions of beta.
For higher beta, the bias increased substantially, in all conditions of N for SEM-ML and SEM-ULS.
This was not the case for LSAM-ML.

Across all conditions, LSAM-ML substantially outperformed the two SEM-estimators.

The bias difference between LSAM-ML and the two other estimators is higher for lower N.

The only exception was for beta =0.1 in N=250 and all higher N. 
Here, there was only a slightly better performance of LSAM- over SEM-ML estimation, though not substantial.

This effect was stronger for higher values of beta.
Additionally, SEM-ML outperformed SEM-ULS in all conditions as well.

Thus, LSAM-ML generally appeared to outperform SEM-estimation and SEM-ML outperformed SEM-ULS estimation, in the presence of four unmodelled cross-loadings.
This was especially evident for higher beta values and lower N.


### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r}

#Normal print
#print(bias_ci_s4a[["DGM_2"]][["SEM_ML_metrics"]])
#print(bias_ci_s4a[["DGM_2"]][["SEM_ULS_metrics"]])
#print(bias_ci_s4a[["DGM_2"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(bias_ci_s4a[["DGM_2"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of SEM-ML for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_2"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of SEM-ULS for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_2"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute Bias of LSAM-ML for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

All estimators were generally biased across conditions, besides conditions of very high N (N=1000-100.000) and lower to moderate beta (0.1-0.3).
For all three estimators, across increasing N, the bias decreased substantially, in all conditions of beta.
Only in SEM-ML in N=50 and SEM-ULS in N=100, there was a substantial increase in bias for higher beta values.

For N=50, both SEM- and LSAM-ML substantially outperformed ULS estimation.
In these conditions, LSAM-ML also outperformed SEM-ML. for beta 0.2-0.4.
Lastly, LSAM-ML outperformed SEM-ULS in N=100 for beta = 0.4.

For all other conditions, none of the estimators differed from another, in terms of bias.

Thus, LSAM-ML appeared to perform best in small samples and higher beta.
Besides that, none of the estimators generally outperforms another in the presence of 20 unmodelled residual correlations.

## RMSE

General interpretation: substantial difference if RMSE estimates vary by 0.03

Interpretation layers within table condition:

- ALL across increasing N

- ALL across increasing beta

- All within one N 
- All within one beta

Layers between tables:

-Comparison between estimators

-Comparison between DGMs

-----------------

```{r}
#| ouput: true

# Load the rmse results
rmse_s4a <- readRDS("SimulationResults/sim4a_rmse.rds")

```

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r}

#Normal print
#print(rmse_s4a[["DGM_1"]][["SEM_ML_metrics"]])
#print(rmse_s4a[["DGM_1"]][["SEM_ULS_metrics"]])
#print(rmse_s4a[["DGM_1"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(rmse_s4a[["DGM_1"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ML for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_1"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ULS for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_1"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of LSAM-ML for DGM 1" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

Across all conditions of N and beta, LSAM-ML estimation was more efficient than the two SEM-estimators.
This difference is stronger in smaller samples.
Additionally, SEM-ML outperformed SEM-ULS estimation, in terms of efficency, in all conditions as well.
Both SEM estimators have reduced RMSE values for higher N.
All three estimators have higher RMSE values for higher beta.
This effect was especially present in the two SEM estimators.

Thus, LSAM-ML appeared to outperform SEM-estimation and SEM-ML appeared to outperform SEM-ULS estimation, in the presence of four unmodelled cross-loadings.
This was especially true in lower samples and higher beta values.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r}

#Normal print
#print(rmse_s4a[["DGM_2"]][["SEM_ML_metrics"]])
#print(rmse_s4a[["DGM_2"]][["SEM_ULS_metrics"]])
#print(rmse_s4a[["DGM_2"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(rmse_s4a[["DGM_2"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ML for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_2"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ULS for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_2"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of LSAM-ML for DGM 2" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

Across all conditions of N, LSAM-ML generally outperforms the two SEM-estimators for lower to moderate beta values (approx. beta= 0.2-0.4).
This effect is stronger for higher values of beta and lower N.
SEM-ULS had reduced RMSE values for higher N.
All three estimators have higher RMSE values for higher beta.
This effect was especially present in the two SEM estimators.

Thus, LSAM-ML appeared to generally outperform SEM-estimation, in the presence of 20 unmodelled residual correlation.
Even more so, for higher values of beta and lower N.


## Summary Study 4a

In terms of Bias, only in the presence of four unmodelled cross-loadings, did LSAM-estimation appear to generally outperform both SEM-estimators.
This was especially evident for higher beta values.
In the presence of unmodelled residual correlations, LSAM-ML appeared to perform best in small samples and higher beta.

With regards to RMSE, LSAM-ML appeared to outperform SEM-estimation both under unmodelled cross-loadings and residual correlations.
Again, this appeared to be especially present in conditions with small samples and high beta values.

Going all the way back to Studies 1-3, we again have to remember that LSAM has a general negative bias in smaller samples.
This makes it appear to outperform in conditions of lower N.
As replicated in Study 1b, this bias tends to be even stronger for higher parameter values in the structural model, and thus probably for higher beta values as well.

To paint a complete picture of the comparative performance of SAM vs. SEM estimation, the joint study should include more conditions with negatively valenced unmodelled cross-loadings and residual correlations.

## Comparison to original paper

Importantly, @robitzsch_comparing_2022 did not investigate differential effects due to sample size.
Nor did they investigate RMSE.
Thus, results are only partly comparable.

In DMG1, the original paper indicated the same results, in that LSAM performed best.
Interestingly, @robitzsch_comparing_2022 did not find the better performance of SEM-ML over SEM-ULS estimation that we found.
Also no differential effect for higher beta values was found in the original paper.

In DGM2, @robitzsch_comparing_2022 found SEM-ULS to perform slightly better than the two others.
Thus, results are quite similar, as we also did not find substantial differences between the three estimators.

We disagree with the conclusion that @robitzsch_comparing_2022 draw with regards to this study, as they deem SEM-ULS and LSAM do not differ in performance, when in fact LSAM outperformed SEM-ULS in DGM1 in their studies.

Additionally, later on in the paper, they posit that SAM can be expected to be more robust than both SEM-estimators in models with non-saturated structural models.
With this last conclusion, we disagree with regards to the now multiple times mentioned negative small sample bias of LSAM, that appears to be present in non-saturated structural models as well.

# Summary for 5-factor-model under 3 different data-generating mechanisms (Studies 4 and 4a)

For low beta (0.1), LSAM appeared to outperform the two SEM-estimators in smaller to moderate samples sizes, in conditions with unmodelled cross-loadings (DGM1).
In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there is no general advantage of one estimation method over another.

With increasing values of beta (0.2-0.4), the effect with regards to unmodelled cross-loadings became even stronger (DGM 1).
in this context, LSAM appeared to not only outperform SEM-estimation in terms of efficiency in DGM1, but also in conditions with unmodelled residual correlations (DGM2).

Very important to keep in mind here is a finding of the earlier study and the original paper, that LSAM exhibits a negative finite sample bias that skews results in conditions with positive values for unmodelled cross-loadings or residual correlations [@robitzsch_comparing_2022].
This effect was even stronger for higher values of phi, which aligns with the findings in our studies.

Thus, in the joint study, conditions with negative unmodelled cross-loadings or residual correlations should be included for unsaturated structural models.

# Overall summary

With regards to all studies conducted, as in the original paper by @robitzsch_comparing_2022, SAM does not generally outperform SEM in small to moderate samples. 
SAM exhibits a negative small sample bias that makes SAM appear superior in conditions with unmodelled positive cross-loadings and residual correlations.
This bias is especially strong for lower lambda and higher phi or beta values.
Going ahead of what was investigated in @robitzsch_comparing_2022, we found that this bias is also present in models with regressions.
Thus, it cannot be concluded that SAM is more robust in non-saturated structural model, as of yet.
If there is no misspecification or unmodelled negative cross-loadings and residual correlations, SAM tends to perform even worse than traditional SEM, as far as can be concluded from the 2-factor-CFA-models.


# Starting points for joint simulation

With regards to the joint simulation, two main aspects might be interesting to investigate in further data constellations.
Firstly, with regards to the conceptual side of the simulation, we should include the explicit monitoring of non-convergence, as we found potential issues with it in multiple studies.
The implementation of the studies should allow to capture and investigate these issues in more detail.

Secondly, conditions with negative unmodelled cross-loadings or residual correlations should be included for unsaturated structural models.
This would be interesting to be able to differentiate whether the negative finite sample bias of LSAM, that became apparent in the 2-factor-CFA, is also influencing the results in the 5-factor-model with regressions.
Only if this can be ruled out, we one could argue that SAM generally outperforms SEM in these models.


