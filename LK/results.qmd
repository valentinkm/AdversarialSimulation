---
title: "Adversarial Simulation"
subtitle: "A case study"
author: "Leonard Kosanke"
format:
    pdf:
        fontsize: 12pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
fontsize: 12pt
bibliography: ../bibliography.bib
engine: knitr
---

The git hash is: `{r} suppressWarnings(system2("git", c("rev-parse", "--short=5", "HEAD"), stdout = TRUE, stderr = TRUE))`

```{r include = FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
```

# Deviations from the simulation protocol

## Across studies

For pragmatic purposes of the simulation implementation, I was unable to track and include non-convergence explicitly and exclude cases where it occurred.

## Studies 1,2,3

For all these studies, 3 deviations were present.
Firstly, the total number of conditions turned out to be higher, as the calculation in the protocol forgot the addition of N=50 to be added to the factor sample size.
Secondly, It was not detailed what type of LSAM estimation will be implemented. For completeness sake, I included ML- as well as ULS-LSAM estimation.

## Study 1b

In Study 1b, contrary to the simulation protocol, I added another sample size with N=50 to get a more nuanced understanding of the small-sample bias of LSAM. 
For the same reasons as in studies 1,2 and 3, I used ML- as well as ULS-LSAM estimation. Calculation of the relative bias was not feasible in this study, as for the conditions where phi was 0, dividing by 0 would have lead to infinite values.

## Study 4

Again, I forgot the addition of N=50 in the original condition calculation. 
Importantly, the model parameters and resulting population-level covariance matrices for the different DGM's provided by @robitzsch_comparing_2022 in their github repository were not the same as in the first paper by @rosseel_structural_2022: 
They omitted, for example, the cross-loading of the last factor in DGM2, without an apparent reason. 
Additionally, they had different values for the residual variances without giving insight into why they chose to do so and how (there was no calculation in the simulation script provided). 
For these reasons, and because of the fact that the calculation of the values in @rosseel_structural_2022 was replicable, I chose to use the values of @rosseel_structural_2022 by generating them with the same functions they used. 
In consequence, a fourth cross-loading was present in DGM 2.
The only difference in my data-simulation is that I only used the 3 out of 4 misspecification conditions that were present in @rosseel_structural_2022. 
Importantly, due to the implementation of the functions in the paper by @rosseel_structural_2022, the naming of the DGM's changed. 
In their paper, what @robitzsch_comparing_2022 called DGM1 was implemented with a value of 0. 
DGM 2 and 3 were thus also implemented with values 1 and 2. 
To avoid confusion, I used the terminology of the code from @rosseel_structural_2022. 
Thus, the naming was different when interpreting the results.

## Study 4a

I decided to include a variation of study 4a after all, as the relation between beta/phi values and N was interesting: 
The original paper showed that results differed between estimators, depending on the size of the factor correlation in Studies 5, 6 and 1b, as well as differential effects due to N in Studies 1,2 and 3.
I deemed it interesting to investigate these results in a model with a different number of factors.
These findings, combined with our goal to investigate realistic data conditions, rather than just the population level, left me to include Study 4a, with the addition of an additional factor sample size.
As DGM1 neither contained misspecification (which is central to our research question), nor did it lead to interesting results in the original study 4, it was omitted.
Similarly, as no substantial conclusions were drawn with regards to RMSE in the original papers study 1 and 2, I omitted its calculation as well.
Lastly, for the same reasons as argued for Study 4, I again created the DGM's based on the functions used in @rosseel_structural_2022.
As in Study 4, from now on I will also adhere to the naming that follows from the code taken from @rosseel_structural_2022, so that DGM's 1-3 are now DGM's 0-2.

## Studies 5,6

As we were not interested in the comparative performance of SAM vs. SEM at the population level for our research question, but in the performance comparison under realistic conditions, especially in small to moderate sample sizes, I decided to not replicate Studies 5 and 6 after all.
I deemed this a fair deviation from my simulation protocol, especially because no substantial performance differences arose in the results of these studies, and the most relevant aspect for us, the differential effect due to the size of factor correlation, is already covered in two different models in studies 1b and 4a.

# Result analysis

## Original paper

### Bias
In the original paper, @robitzsch_comparing_2022 had some inconsistencies when it came to the interpretation of the results of their simulation studies. 
They did not thoroughly define an a prior criterium for when an estimator outperformed another. 
Only in their Studies 5 and 6, they defined an absolute bias equal to or larger than 0.05 as relevant. 
Additionally, when an estimator had a minimum percentage error of 20% higher than another, they defined it as being better than the other, in these studies.
Across the other studies, interpretations sometimes varied. 
In some studies (e.g. Study 1), they seemed to orient on the same value of 0.05, but this time as an absolute difference value, without the addition of a certain percentage cut-off. 
In other studies (e.g. Study 2), they seemed to deviate from this value to define relevant difference in absolute bias.
Importantly, this deviation was present in both directions, and therefore did not appear to be consciously biased towards one direction of possible outcomes.
In consequence, it appears that interpretations aren't coherent across the studies.

Another important aspect to highlight in the original papers results refers to the general presence of a negative small sample bias in LSAM estimation. 
This bias makes LSAM appear to perform superior in conditions with positive values in unmodelled cross-loadings and residual correlation, but worse when the values are negative.
Even though this bias was correctly identified and explained already in Study 1, @robitzsch_comparing_2022 conduct multiple follow-up studies only investigating positive values in unmodelled cross-loadings and residual correlation, namely in study 3. 
This is done without a general note of some sorts, that this bias should apply in these data-generating scenarios the same as in Studies 1 and 2.
In consequence, the interpretations @robitzsch_comparing_2022 make, are somewhat misleading on first sight, if one does not account for this bias.
This does not, however, explain why they did not include more studies or conditions with negatively valenced residual correlations or cross-loadings. 
I hypothesize, that the goal of the paper was to show that even in these favorable constellations, SAM does not perform better than SEM.

To be fair, it should be stated that most of the final conclusions drawn in the paper are correct, anyway. 
By saying that SAM- should not "generally" be preferred over SEM-estimation, they are right in that there are many data constellations, where SAM does not outperform traditional SEM.
They also acknowledge, that based on their results for non-saturated structural models, SAM seems to be the better choice.
Unfortunately, they use the term "generally" in a second context, when they argue in the discussion that still, SAM should still generally be used.
Here, however, they say it should be used to avoid confounding the definition of measurement models from the estimation of structural models.
This, according to @robitzsch_comparing_2022, often happens in applied research by including cross-loadings and residual correlations at will, without substantive reasons.
In this sense, SAM should generally be used.

For my studies, in order to have a more coherent interpretation of results, I defined an estimator to outperform another, based on the cut-off of 0.05 for bias.
Additionally, I only used the term "generally" to refer to the first meaning, that it outperforms across multiple data generating scenarios or conditions.

### SD and RMSE

Even though @robitzsch_comparing_2022 reported tables with results for SD and RMSE, they rarely explicitly mentioneded them in the results sections or gave cut-offs for substantial relevance.

SD was only reported and mentioned in the conditions without misspecification in Study 1, even though also claimed to be calculated in studies 2 and 3. 
In this study, they mentioned a 6.6% reduction of SD for LSAM in comparison to ML. 
This calculation, however, was wrong as the reduction amounts to 8%.
For my interpretation, I reported percentage reductions larger than 5% as well and compared them with the original study, where possible.

RMSE (and average RMSE) was reported in Studies 1-4, but only in studies 2 and 4 explicitly interpreted for my estimators of interest.
In study 2, differences of up to 0.03 were present and interpreted as not being substantial differences.
In study 4, differences of 0.01 were present and interpreted as slight efficiency gains.
I infered that this lack of mentioning and consistency can be interpreted as there being no differences of interest. 
Thus, I expected my studies to yield similar results. 
If, however, there were differences larger than 0.03, I reported them.

## Recalculation of CI's

I did multiple mistakes with the calculation of the confidence intervals, as I did not include the absolute, but relative mean in the two limits for studies 1b, 4 and 4a. 
Also, the standard error was wrongly calculated as relative.
For Studies 1, 2 and 3, I misscalculated the standard error, as well as the confidence interval by including absolute values in their calculation, even though I was interested in relative values in these studies.
Thus, I had to reextract the results for bias correctly, which was done in postprocess_results.R.
All I did there, was reapply the corrected extract_results() function and the same report_bias() function from the individual simulations, one after another.
For the results of bias, I loaded these processed files, instead of the original ones, from the newly created folder: SimulationResultsProcessed.

# Results Simulation 1: 2-factor-CFA with 0,1 or 2 correlated residuals

## Error, warnings and messages

First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s1 <- readRDS("SimulationResults/sim1_results_error.rds")

#errors
errors_s1 <- all_messages_s1$errors
errors_sum_s1 <- unlist(errors_s1)
length(errors_sum_s1)

#warnings
warnings_s1 <- all_messages_s1$warnings
warnings_sum_s1 <- unlist(warnings_s1)
length(warnings_sum_s1)

#messages
messages_s1 <- all_messages_s1$messages
messages_sum_s1 <- unlist(messages_s1)
length(messages_sum_s1)

```

No errors and messages were present. There were, however, 245 warnings, I will investigate in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings were the same: `no non-missing arguments to min; returning Inf`.
```{r}
#| ouput: true

unique(warnings_s1)

```
 
This warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Unfortunately, due to the implementation of the study, I was unable to check for more details, as whether there is any kind of pattern with regards to the warning occurrence.
As the 245 are less than 0.5% of the 52500 estimations performed, I will continue with the interpretation as planned, as their impact is negligible.

## Bias

```{r include = FALSE}

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResultsProcessed/sim1_rel_bias_ci.rds")

shorten_names <- function(df) {
  df$method_metric <- sub("_rel_bias", "", df$method_metric)
  return(df)
}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  data <- shorten_names(data)
  kbl(data, col.names = c("Method", "50", "100", "250", "500", "1000", "2500", "1e+05")) %>%
    kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

```

### Condition without correlated residuals (0_0.12)

```{r output = TRUE, echo = FALSE}
create_styled_table(bias_ci_s1[["0_0.12"]], "0_0.12")
```
Both standard SEM estimators were unbiased (i.e. relative bias < 0.05) across all conditions of N.
The 4 SAM estimators were negatively biased for sample sizes up to 500 and had nearly identical estimation results.
Across all estimators, the relative bias decreased with increasing N. 

Thus, SEM estimation outperformed SAM in small to moderate sample sizes, when there was no misspecification.

### Condition with one negative residual correlation (1_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s1[["1_-0.12"]], "1_-0.12")

```

All estimators were negatively biased (i.e. relative bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the bias decreased substantially with increasing N.
For SEM-ULS, it increased slightly but unsubstantially up to N=500, but was lowest overall.
The 4 SAM estimators had nearly identical estimation results.
In relative comparison, the two SEM estimators outperformed all SAM estimators in every condition of N, even though this difference was not substantive for N=1000 and higher.

Thus, SEM estimation outperformed SAM in small to moderate sample sizes, when there was one unmodelled negative residual correlation.

### Condition with one positive residual correlation (1_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s1[["1_0.12"]], "1_0.12")

```

Besides SEM-ML in N=50, both SEM estimators were positively biased across all N, with no visible trend for increasing N.
All SAM estimators had nearly identical values and increasing relative bias for increasing N.
They were negatively biased up to N=250 and positively biased for the other N conditions. 
For N=250 and N=500, their relative bias was unsubstantial.
In relative comparison, the two SEM estimators outperformed all SAM estimators for sample sizes up to 100. For all higher N, the SAM estimators outperformed the SEM estimators, even though the difference was unsubstantial starting from N=1000.

Thus, SEM estimation outperformed SAM in smaller samples (N=50-100), whereas SAM outperformed SEM in moderate samples (N250-500), and slightly in higher sample sizes, when there was one unmodelled positive residual correlation.

### Condition with two negative residual correlations (2_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s1[["2_-0.12"]], "2_-0.12")

```

All estimators were negatively biased across all N conditions.
Across all estimators except SEM-ULS, the bias decreased substantially with increasing N.
For SEM-ULS, it slightly, but unsubstantially increasd up to N=500, but was lowest overall in these conditions.
The 4 SAM estimators had nearly identical estimation results.
In relative comparison, the two SEM estimators outperformed all SAM estimators in every condition of N, even though this difference was not substantive for N=1000 and higher.

Thus, SEM estimation outperformed SAM in small to moderate sample sizes, when there were two unmodelled negative residual correlations.

### Condition with two positive residual correlation (2_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s1[["2_0.12"]], "2_0.12")

```

Both SEM estimators were positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators had nearly identical values and increasing relative bias for increasing N.
They were negatively biased up to N=100 and positively biased for the other N conditions. 
In relative comparison, the two SEM estimators outperformed all SAM estimators for N=50. For all higher N, the SAM estimators outperformed the SEM estimators (substantially, besides ML in N=100), even though the difference was unsubstantial starting from N=1000.

Thus, SEM estimation outperformed SAM in small samples (N=50), whereas SAM outperformed SEM in small to moderate samples (N100-500), and slightly in higher sample sizes when two unmodelled positive residual correlations were present.

### Summary

in 0.5% of estimations, a warning referring to potential problems with convergence or improper solutions due to small sample sizes in one or multiple estimators occurred.
In conditions with no, one or two negative residual correlations, SEM outperformed SAM in small to moderate sample sizes. 
This seemed to be independent of the amount of misspecification (1 vs. 2).
Comparing the conditions with one and two positive residual correlations, the results suggested that SAM outperformed SEM in smaller to moderate samples, with increasing amount of misspecifiation (even better for 2 than for 1 unmodelled residual correlation).
In small samples, SEM tended to perform better.

### Comparison with original paper

Comparing these results with the original paper, the results were similar and can be seen as mostly replicated. The only difference lied in the potential convergence issues in a small number of estimations. 
Besides that, I drew the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appeared to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This leads to the comparatively worse performance in the conditions with no, one or two negative residual correlations. 
Additionally, this negative bias corrects for unmodelled positive residual correlations and makes SAM appear superior in these conditions.


## SD

```{r include = FALSE}

# Load the relative bias results
sd_s1 <- readRDS("SimulationResults/sim1_sd.rds")

```

### No residual correlations (0_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s1[["0_0.12"]], "0_0.12")

```

Across all estimators, the standard deviation decreased with increasing N. 
There were only two substantial differences:
There was a substantial percentage reduction of SD up to 22.4% when comparing SEM-ML and LSAM-ML in N=50, as well as a substantial percentage reduction of SD up to 8.6% when comparing SEM-ML and LSAM-ML in N=100.

Thus, SAM outperformed SEM in terms of SD reduction in smaller samples, when there was no misspecification.

### One negative residual correlation (1_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s1[["1_-0.12"]], "1_-0.12")

```

Across all estimators, the standard deviation decreased with increasing N.
There were only two substantial differences: A substantial percentage reduction of SD up to 27.4% when comparing SEM-ML and LSAM-ML in N=50, as well as a substantial percentage reduction of SD up to 12.2% when comparing SEM-ML and LSAM-ML in N=100.

Thus, SAM outperformed SEM in terms of SD reduction in smaller samples when one unmodelled negative residual correlation was present.

### One positive residual correlation (1_0.12)
```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s1[["1_0.12"]], "1_0.12")

```

Across all estimators, the standard deviation decreased with increasing N.
The SAM estimators outperformed both SEM estimators in N=50 with a percentage reduction of SD up to 16.9%.
ULS-SEM outperformed all SAM estimators with a percentage reduction of SD up to 8.16% in N=100-250.
No substantial differences between the two SEM and all SAM estimators arose, starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperformed SEM in terms of SD reduction in small samples, whereas SEM outperformed SAM in moderate samples, when one positive unmodelled residual correlation was present.

### Two negative residual correlations (2_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s1[["2_-0.12"]], "2_-0.12")

```

Across all estimators, the standard deviation decreased with increasing N. 
The SAM estimators outperformed both SEM estimators in N=50 with a percentage reduction of SD up to 29% and in N=100 with up to 12.8%.
For higher N, there were no substantial differences.

Thus, SAM outperformed SEM in terms of SD reduction in smaller samples.

### Two positive residual correlations (2_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s1[["2_0.12"]], "2_0.12")

```

Across all estimators, the standard deviation decreased with increasing N. 
The SAM estimators outperformed both SEM estimators in N=50 with a percentage reduction of SD up to 15.2%.
Both SEM estimators ourperformed the SAM estimators in N=100 with a percentage reduction of SD up to 11.1% and in N=250 with up to 10.3%.
For higher N, there was no substantial differences.

Thus, SAM outperformed SEM in terms of SD reduction in small samples, but SEM outperformed SAM in smaller to moderate samples.

### Summary and comparison with original paper

Overall, SAM outperformed SEM in terms of SD reduction in small to smaller samples, especially for negatively correlated residuals. 
In smaller to moderate samples, SEM outperformed SAM only for positively correlated residuals.
For conditions with no misspecification, which were the only conditions reported in @robitzsch_comparing_2022, the percentage reduction favoring SAM was nearly identical for N=100, and the reduction seems to be even more significantly in favor of SEM in the smaller sample size.

## RMSE

```{r include = FALSE}
rmse_s1 <- readRDS("SimulationResults/sim1_rmse.rds")

```

### No residual correlations (0_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s1[["0_0.12"]], "0_0.12")

```

Across all estimators, the RMSE decreased with increasing N.
In N=100, both SEM estimators were more efficient than all SAM estimators.
Besides that, there were no substantial differences present.

### One negative residual correlation (1_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s1[["1_-0.12"]], "1_-0.12")

```

Across all estimators, the RMSE decreased with increasing N.
For small to moderate samples (N=50-250), both SEM estimators were more efficient than all SAM estimators.
For higher N, there were no substantial differences.

### One positive residual correlation (1_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s1[["1_0.12"]], "1_0.12")

```

Across all estimators, the RMSE decreased with increasing N.
For smaller samples (N=50-100), SEM-ULS was more efficient than all SAM estimators.
For higher N, there were no substantial differences present.

### Two negative residual correlations (2_-0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s1[["2_-0.12"]], "2_-0.12")

```

Across all estimators, the RMSE decreased with increasing N.
For small to moderate samples (N=50-500), The two SEM estimators were more efficient than all SAM estimators.
For higher N, there were no substantial differences present.

### Two positive residual correlations (2_0.12)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s1[["2_0.12"]], "2_0.12")

```

Across all estimators, the RMSE decreased with increasing N.
For moderate samples (N=250-500), all SAM estimators were more efficient than the two SEM estimators.
For higher and lower N, there were no substantial differences present.

### Summary

Overall, classic SEM estimation was more efficient than SAM estimation in small to moderate samples. 
Only in the case of two positive residual correlation, SAM was more efficient than SEM in moderate samples.

## Summary Study 1

I gained the following insights with regards to the 2-factor-CFA model:

In terms of bias in small to moderate samples, SEM outperformed SAM in conditions with no, one and two negative residual correlations. 
SAM appeared to outperform SEM for positive residual correlations. 
This, however, was an artifact of its negative small sample bias and not its performance. 
The results align with the ones obtained by the original paper.

With regards to SD and RMSE, results were incoherent with no clear pattern emerging. 

Thus, it cannot be stated that one estimation method was generally performing better than the other, based on the results of Study 1.

# Results Simulation 1b: 2-factor-CFA with 2 residual correlations and varying lambda and phi

## Error, warnings and messages
First, I checked for any messages, warnings or errors the same way as in Study 1:

```{r output = TRUE, echo = FALSE}

#Save all errors, warnings and messages
all_messages_s1b <- readRDS("SimulationResults/sim1b_results_error.rds")

#errors
errors_s1b <- all_messages_s1b$errors
errors_sum_s1b <- unlist(errors_s1b)
length(errors_sum_s1b)

#warnings
warnings_s1b <- all_messages_s1b$warnings
warnings_sum_s1b <- unlist(warnings_s1b)
length(warnings_sum_s1b)

#messages
messages_s1b <- all_messages_s1b$messages
messages_sum_s1b <- unlist(messages_s1b)
length(messages_sum_s1b)

```

No errors and messages were present. There were, however, 414 warnings, I investigated in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings were the same: `no non-missing arguments to min; returning Inf`.

```{r}
unique(warnings_s1b)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Similarly, I was unable to check for more details, besides the fact that the 414 estimation were around 0.5% of the 75000 estimations performed.
I continued with the interpretation as planned, as their impact was negligible.

## Bias

```{r}
#Load processed results
bias_ci_s1b <- readRDS("SimulationResultsProcessed/sim1b_abs_bias_ci.rds")

```


### Conditions with N=50

```{r output = TRUE, echo = FALSE}

#Normal print
#print(bias_ci_s1b[["N_50"]][["LSAM_ML"]])
#print(bias_ci_s1b[["N_50"]][["LSAM_ULS"]])

#Formatted tables
kbl(bias_ci_s1b[["N_50"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=50" = 6), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_50"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ULS for N=50" = 6), bold = TRUE, align = "l", line = TRUE, italic = TRUE)


```

Comparing LSAM-ML and LSAM-ULS, the values were nearly identical. 
Therefore, I interpretated the estimators together:
Across all conditions, both estimators were biased.
For higher values of lambda, the estimators had less absolute bias. 
This difference increased for higher phi values.
For lower values of lambda, the absolute bias increased with increasing phi. This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.

### Conditions with N=100

```{r output = TRUE, echo = FALSE}

kbl(bias_ci_s1b[["N_100"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute bias of LSAM-ML for N=100" = 6), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_100"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute bias of LSAM-ULS for N=100" = 6), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```
Again, the values and therefore the interpretation of LSAM-ML and LSAM-ULS were identical.
Across all conditions, both estimators were biased.
For higher values of lambda, the estimators had less absolute bias. This difference increased for higher phi values.
For lower values of lambda, the absolute bias increased with increasing phi. 
This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.
Additionally, there seemed to be no differential effect due to N, as the interpretations of the different N-conditions were identical.
Naturally, the bias values were larger for N=50.

### Comparison to original paper

In contrast to the original paper by @robitzsch_comparing_2022, I computed absolute bias to avoid infinite values in some conditions.
Additionally, computing absolute instead of relative bias solved the problem of positive and negative biases canceling each other out.
Results partly aligned in terms of absolute bias:
For higher values of lambda, LSAM had less absolute bias. 
This difference increased for higher phi values.
Only for low values of lambda, the bias increased with higher values of phi. 
Unlike in this study, the original paper did not find a reversal effect for higher lambda values.
Another difference was that we, unlike @robitzsch_comparing_2022, found estimates to be biased for low phi-values as well.
This might be due to the difference of using absolute bias, thereby avoiding cancellation of positive and negative values.
Still, my general conclusion corresponds to the original paper:
LSAM has a general negative small sample bias. 
The bias is larger in magnitude for lower loadings and higher true factor correlations.
Importantly, the bias seems to not disappear for low values of phi and lambda, unlike in the original paper.

# Results Simulation 2: 2-factor-CFA with 1 or 2 cross-loadings

## Error, warnings and messages

```{r output = TRUE, echo = FALSE}

#Save all errors, warnings and messages
all_messages_s2 <- readRDS("SimulationResults/sim2_results_error.rds")

#errors
errors_s2 <- all_messages_s2$errors
errors_sum_s2 <- unlist(errors_s2)
length(errors_sum_s2)

#warnings
warnings_s2 <- all_messages_s2$warnings
warnings_sum_s2 <- unlist(warnings_s2)
length(warnings_sum_s2)

#messages
messages_s2 <- all_messages_s2$messages
messages_sum_s2 <- unlist(messages_s2)
length(messages_sum_s2)


```
No errors and messages were present. There were, however, 612 warnings I investigated in detail.


### Warnings investigation

Again, all the warnings were the same:
`no non-missing arguments to min; returning Inf`.
One difference was that there were sometimes 4-6 warnings per repetition, more than before were I only saw up to 2 per repetition.

```{r}
unique(warnings_s2)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, I was unable to check for more details, but the 612 estimation were only around 1.5% of the 42000 estimations performed. I continued with the interpretation as planned, as their impact was negligible.

## Bias

```{r include = FALSE}

# Load the relative bias results
bias_ci_s2 <- readRDS("SimulationResultsProcessed/sim2_rel_bias_ci.rds")

```

### One negative cross-loading (1_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s2[["1_-0.3"]], "1_-0.3")

```


All estimators were negatively biased (i.e. relative bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the bias decreased substantially with increasing N.
For SEM-ULS, the bias stayed constant across N, and was lowest by a substantial marging compared to all SAM estimators up until N=500.
For higher N, SEM-ML was best, but the differences between all estimators were negligible.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperformed all SAM estimators in every condition of N, even though this difference was not substantive for N=1000 and higher.

Thus, SEM estimation outperformed SAM in small to moderate sample sizes, when there was one negative cross-loading.

### One positive cross-loadings (1_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s2[["1_0.3"]], "1_0.3")

```

All estimators were biased across all N conditions.
For the two SEM-estimators, the relative bias was always positive and showed no trend across N.
All SAM estimators had nearly identical values and increasing relative bias for increasing N.
Their bias turned from being negative for N=50-100 to positive from N=250 on.
In relative comparison, the two SEM estimators outperformed all SAM estimators for N=50. 
For N=100-2500, the SAM estimators outperformed the SEM estimators, even though the difference was unsubstantial starting from N=1000.

Thus, SEM estimation outperformed SAM in small samples (N=50), whereas SAM outperformed SEM in smaller to moderate samples (N100-500), and slightly in higher sample sizes.

### Two negative cross-loadings (2_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s2[["2_-0.3"]], "2_-0.3")

```
All estimators were negatively biased across all N conditions.
Across all estimators, the bias decreased substantially with increasing N.
The 4 SAM estimators had nearly identical estimation results.
In relative comparison, the two SEM estimators substantially outperformed all SAM estimators in every condition of N.

Thus, SEM estimation outperformed SAM, when there were two negative residuals.

### Two positive cross-loadings (2_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s2[["2_0.3"]], "2_0.3")

```

All estimators were biased across N conditions.
Both SEM estimators were positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators had nearly identical values and increasing relative bias for increasing N.
They were negatively biased in N=50 and positively biased for the other N conditions. 
In relative comparison, the four SAM estimators outperformed the two SEM estimators for N=50-500, even though unsubstantial for N=500. For all higher N, no estimator substantially differed from another, but the two SEM estimators slightly outperformed.

Thus, SAM outperformed SEM in small to moderate samples (N50-250).

### Summary

For small to moderate samples, SEM outperformed SAM in conditions with negative cross-loadings. SAM outperformed SEM in conditions with positive cross-loadings.

### Comparison with original paper

Comparing these results with the original paper, the results were similar and can be seen as replicated. 

Consequently, I drew the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appeared to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This lead to the comparatively worse performance in the conditions with no, one or two negative residual correlations. Additionally, this negative bias corrected for unmodelled positive cross-loadings and made SAM appear superior in these conditions.

## SD

```{r include = FALSE}

# Load the relative bias results
sd_s2 <- readRDS("SimulationResults/sim2_sd.rds")

```

### One negative cross-loading (1_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s2[["1_-0.3"]], "1_-0.3")

```


Across all estimators, the standard deviation decreases with increasing N. 
There were only two substantial differences: One substantial percentage reduction of SD up to 30.8% when comparing SEM-ML and LSAM-ML in N=50, and one substantial percentage reduction of SD up to 16% when comparing SEM-ML and LSAM-ML in N=100.

Thus, SAM outperformed SEM in terms of SD reduction in smaller samples.

### One positive cross-loading (1_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s2[["1_0.3"]], "1_0.3")

```

Across all estimators, the standard deviation decreased with increasing N. 
The SAM estimators outperformed both SEM estimators in N=50 with a percentage reduction of SD up to 6.4%.
ULS-SEM outperformed all SAM estimators with a percentage reduction of SD up to 14.1% in N=100, and of 12.3% in N=250 and of 8.6% in N=500.
Above that point, overall SD was very small and negligible in all estimators
No substantial differences between the two SEM and all SAM estimators arose starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperformed SEM in terms of SD reduction in small samples, whereas SEM outperformed SAM in smaller to moderate samples.

### Two negative cross-loadings (2_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s2[["2_-0.3"]], "2_-0.3")

```

Across all estimators, the standard deviation decreased with increasing N. 

The SAM estimators substantially, but decreasingly, outperformed both SEM estimators in samples N=50-500 with a SD percentage reduction of 10.1% remaining in N=500.
For higher N, there were no substantial differences and overall SD was very small across estimators.

Thus, SAM outperformed SEM in terms of SD reduction in small to moderate samples.

### Two positive cross-loadings (2_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(sd_s2[["2_0.3"]], "2_0.3")

```

Across all estimators, the standard deviation decreased with increasing N. 

Both SEM estimators outperform the SAM estimators in N=50-500 with a SD percentage reduction up to 11.7% remaining in N=500.
For higher N, there were no substantial differences.

Thus, SEM outperformed SAM in small to moderate samples.


### Summary

For negative cross-loadings, SAM tended to outperform SEM in terms of SD reduction small to moderate samples.
For positive cross-loadings, SEM tended to outperform SAM in terms of SD reduction smaller to moderate samples.
There seemed to be an effect due to the amount of misspecification, such that the conditions with 2 cross-loadings had larger differences in performance (in both directions).

## RMSE

```{r include = FALSE}
rmse_s2 <- readRDS("SimulationResults/sim2_rmse.rds")

```

### One negative cross-loading (1_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s2[["1_-0.3"]], "1_-0.3")

```

Across all estimators, the RMSE decreased with increasing N.
For small to moderate samples (N=50-500), both SEM estimators were more efficient than all SAM estimators.
For higher N, there were no substantial differences present.

### One positive cross-loading (1_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s2[["1_0.3"]], "1_0.3")

```

Across all estimators, the RMSE decreased with increasing N.
For smaller and large samples (N=50-100 and N=1000-100000), none of the estimators differed from each other
For N=250, all 4 SAM estimators substantially outperformed the two SEM estimators.
For N=500, all 4 SAM estimators substantially outperformed SEM-ML estimation.

### Two negative cross-loadings (2_-0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s2[["2_-0.3"]], "2_-0.3")

```

Across all estimators, the RMSE decreased with increasing N.
For small samples with N=50, none of the estimators differed in terms of efficiency.
For all other conditions, The two SEM estimators were more efficient than all SAM estimators.

### Two positive cross-loadings (2_0.3)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s2[["2_0.3"]], "2_0.3")

```

Only for the two SEM estimators, the RMSE decreased with increasing N, but remained at higher values between 0.22-0.25 for all estimators.

For small to moderate samples (N=50-250), all SAM estimators were more efficient than the two SEM estimators.
For higher N, there were no substantial differences present.

### Summary

In moderate samples, SAM generally outperformed SEM in terms of RMSE.
The only exception is the condition with 1 negative cross-loading, where SEM outperformed SAM in small to moderate samples.
In the condition with 2 positive cross-loadings, SAM was more efficient in small samples as well.
Unlike in Study 1, there seemed to be no effect due to amount of misspecification.

## Summary Study 2

I gained the following insights with regards to the 2-factor-CFA model:

In terms of bias in small to moderate samples, SEM outperformed SAM in conditions with negative cross-loadings. 
SAM appeared to outperform SEM in conditions with positive cross-loadings. 
This, however, was again an artifact of its negative small sample bias and not its performance. 
The results aligned with the ones obtained by the original paper.

With regards to SD reduction, findings were different. 
In conditions with negative cross-loadings, SAM tended to outperform SEM, whereas the opposite was true for conditions with positive cross-loadings.
Here, an effect due to the amount of misspecification seemed to be present, like for RMSE in Study 1.

In terms of RMSE, results differed again. 
Generally, SAM was more efficient than SEM in moderate samples. 
For small samples, the results were not as clear.
Unlike in Study 1, there seems to be no effect due to amount of misspecification.

Overall, findings are somewhat incoherent and it can not be stated that one estimation method generally performed better than the other, based on the results of Study 2.

# Results simulation 3:2-factor-CFA with one cross-loading and one residual correlation

## Error, warnings and messages

```{r output = TRUE, echo = FALSE}

#Save all errors, warnings and messages
all_messages_s3 <- readRDS("SimulationResults/sim3_results_error.rds")

#errors
errors_s3 <- all_messages_s3$errors
errors_sum_s3 <- unlist(errors_s3)
length(errors_sum_s3)

#warnings
warnings_s3 <- all_messages_s3$warnings
warnings_sum_s3 <- unlist(warnings_s3)
length(warnings_sum_s3)

#messages
messages_s3 <- all_messages_s3$messages
messages_sum_s3 <- unlist(messages_s3)
length(messages_sum_s3)


```

No errors and messages were present. There were, however, 193 warnings I investigated in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings were the same again: `no non-missing arguments to min; returning Inf`.

```{r}
unique(warnings_s3)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, I was unable to check for more details, but as the 193 estimation amount to only around 1.8% of the 10500 estimations performed, I continued with the interpretation as planned, as their impact was negligable.

## Bias

```{r output = TRUE, echo = FALSE}

# Load the relative bias results
bias_ci_s3 <- readRDS("SimulationResultsProcessed/sim3_rel_bias_ci.rds")

#Format table for results
kbl(bias_ci_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05")) %>%
    kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

All estimators were biased for every condition of N.
The SAM estimators had nearly identical values.
Besides SEM-ULS, all estimators' bias increased substantially, even though the increase was stronger for the SAM estimators.
Only the SAM estimators had negative bias for N=50-100.
For N=50, none of the estimators outperformed another.
For N=100-500, all SAM estimators outperformed the two SEM estimators.
For higher N, none of the estimators substantially differed from another.

Thus, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

### Comparison with original paper

Comparing these results with the original paper, my results were somewhat different.
Even though the results were similar for most conditions (@robitzsch_comparing_2022 had the same results in conditions N=100-250 and N=1000-100000), the results were different for N=500.
This, combined with the findings from the additional condition of N=50 revealed that SAM estimation outperformed SEM estimation in this data generating scenario in small to moderate samples.

## SD

```{r output = TRUE, echo = FALSE}

metrics_s3 <- readRDS("SimulationResults/sim3_metrics_list.rds")

# Load the SD results
sd_s3 <- metrics_s3[["sd"]]

kbl(sd_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05")) %>%
    kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

Across all estimators, the standard deviation decreased with increasing N. 
There were three substantial differences:
A substantial percentage reduction of SD up to 5.5% in favor of SEM-ULS when comparing with GSAM-ULS in N=50. 
Another substantial percentage reduction of SD up to 21.7% when comparing SEM-ULS when comparing with GSAM-ULS in N=100.
And lastly, a substantial percentage reduction of SD up to 15.6% when comparing SEM-ULS with GSAM-ULS in N=250.
For higher N, the SD still partly differed but became negligibly small.

Thus, SEM outperformed SAM in terms of SD reduction in small to moderate samples.

## RMSE

```{r output = TRUE, echo = FALSE}

# Load the rmse results
rmse_s3 <-  metrics_s3[["rmse"]]

#Format table for results
kbl(rmse_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05")) %>%
    kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

Across all estimators, the RMSE decreased with increasing N.
For N=50 and N=1000-100000, none of the estimators differed from another.
For N=100-500, the two SEM-estimators outperformed all SAM-estimators in terms of efficiency.

Thus, SEM outperformed SAM in smaller to moderate samples.

## Summary Study 3

In terms of bias, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

These results differed from the ones obtained with regards to SD and RMSE, where SEM tended to outperform SAM in smaller to moderate samples.

Importantly, one has to bear in mind that same as before, SAM only appears to be more robust against unmodelled positive residual correlations and cross-loadings due to its general negative bias in small samples. 
As in this study, same as in @robitzsch_comparing_2022, the misspecification values were positive, I again cannot conclude that SAM performed better than SEM.

# Summary 2-factor-CFA models (Studies 1-3)

SAM did not generally outperform SEM in small to moderate samples. 
SAM exhibited a negative small sample bias that made SAM appear superior in conditions with unmodelled positive cross-loadings and residual correlations.
This bias was especially strong for lower lambda and higher phi values.
In the absence of misspecification or unmodelled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM.

# Results Simulation 4: 5-factor-model under 3 different data-generating mechanisms

Importantly, for this study, I did not investigate CFA's anymore, but models with regressions included.
Also, keep in mind that the naming of the DGM changed to correspond with the code implementation. 
What @robitzsch_comparing_2022 called DGM 1-3, I now named DGM 0-2.

## Error, warnings and messages

```{r output = TRUE, echo = FALSE}

#Save all errors, warnings and messages
all_messages_s4 <- readRDS("SimulationResults/sim4_results_error.rds")

#errors
errors_s4 <- all_messages_s4$errors
errors_sum_s4 <- unlist(errors_s4)
length(errors_sum_s4)

#warnings
warnings_s4 <- all_messages_s4$warnings
warnings_sum_s4 <- unlist(warnings_s4)
length(warnings_sum_s4)

#messages
messages_s4 <- all_messages_s4$messages
messages_sum_s4 <- unlist(messages_s4)
length(messages_sum_s4)

```

No errors and messages were present. There were, however, 9007 warnings, I investigated in detail.

### Warnings investigation

```{r output= FALSE, echo = FALSE}

# Flatten the list of warnings into a single character vector
all_warnings_s4 <- unlist(warnings_s4)

# Count the occurrences of each unique warning
warning_counts_s4 <- table(all_warnings_s4)

# Convert to data frame for better formatting
warning_counts_df_s4 <- as.data.frame(warning_counts_s4)
colnames(warning_counts_df_s4) <- c("Warning Message", "Count")

warning_counts_df_s4

```

There are three warnings that amount to 7 overall, that are referring to problems with regards to positive definite matrices or model identification:

`lavaan WARNING: covariance matrix of latent variables is not positive definite; use lavInspect(fit, "cov.lv") to investigate.`

`lavaan WARNING: Could not compute standard errors! The information matrix could not be inverted. This may be a symptom that the model is not identified.`

`lavaan WARNING: The variance-covariance matrix of the estimated parameters (vcov) does not appear to be positive definite! The smallest eigenvalue (= 1.595219e-17) is close to zero. This may be a symptom that the model is not identified.`

This suggests potential issues with multicollinearity or redundancy among latent variables, but only in a negligible number of estimations.
The 9000 warnings with regards to Gamma computation in smaller samples are negligible as well, as I was not interested in the computation of robust fit indices.

## Bias

```{r include = FALSE}

# Load the absolute bias results
bias_ci_s4 <- readRDS("SimulationResultsProcessed/sim4_abs_bias_ci.rds")

#Change create_styled_table function names function for abs_bias
create_styled_table <- function(data, condition) {
  kbl(data, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05")) %>%
    kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

```

### Conditions without misspecification (DGM 0)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s4[["DGM_0"]], "DGM_0")
```

All estimators were biased for small to moderate samples (N=50-250). 
For higher N, no estimator was biased.
In N=50, LSAM outperforms SEM-ULS. 
For all other conditions, there was no substantial difference between any of the estimators biases.

Thus, for the most part, no estimator generally outperformed another in the correctly specified model conditions.

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s4[["DGM_1"]], "DGM_1")

```

All estimators were biased across all conditions of N.
Still, their bias decreased for higher N.
LSAM-ML substantially outperformed the two SEM-estimators in smaller samples (N=50-100).
It outperformed SEM-ULS in every other condition of N, as well.
With regards to SEM-ML, LSAM-ML only slightly outperformed, for higher N.

Thus, SAM tended to outperform SEM in presence of unmodelled cross-loadings.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r output = TRUE, echo = FALSE}

create_styled_table(bias_ci_s4[["DGM_2"]], "DGM_2")

```

All estimators were biased for small to moderate samples (N=50-500).
Only SEM-ULS was unbiased for N=500.
For higher N (N=1000-100000), no estimator was biased.
In N=50, LSAM outperformed SEM-ULS estimation. 
For all other conditions, there were no substantial differences between any of the estimators biases.

### Summary

LSAM-estimation appeared to generally outperform SEM-ULS estimation in smaller samples across all DGM's.
Moreover, LSAM- appeared to outperform both SEM-estimators in smaller samples, when unmodelled cross-loadings are present.
Importantly, as I found LSAM to have a negative small sample bias in conditions with low values of phi as well in my simulation study 1b, we have to take this into account:
Even though LSAM appeared to outperform SEM-estimation in DGM 1, this is attributable to its general negative small sample bias.
Thus, it cannot be stated, that SAM estimation should be preferred in these scenarios.

In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there was no general advantage of one estimation method over another, for smaller to moderate samples.

### Comparison with original paper

Interestingly, unlike @robitzsch_comparing_2022, I have found some bias in DGM 0, even though the model was correctly specified in that scenario.
For DMG 1, results are very similar, with LSAM-ML seemingly outperforming SEM-ULS estimation.
A new insight is that LSAM-ML seemed to outperform SEM-ML in smaller samples as well.
In DGM 2, I did not find the disadvantage of SEM-ML in comparison to the other estimators that @robitzsch_comparing_2022 found.
An additional finding was, that LSAM-ML tended to generally perform better than SEM-ULS in smaller samples.
Importantly, my conclusion with regards to LSAM's performance was different, due to the different findings in my Study 1b, that found LSAM to be biased even for low values of phi and lambda.

## RMSE

```{r include = FALSE}
rmse_s4 <- readRDS("SimulationResults/sim4_rmse.rds")

```

### Conditions without misspecification (DGM 0)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s4[["DGM_0"]], "DGM_0")

```

Across increasing N, the average RMSE decreased substantially for all estimators.
For N=50, SEM-ULS performed substantially worse than the other two estimators.
For all other conditions, there was no difference between any of the estimators.

Thus, no estimator generally outperformed another in conditions for a correctly specified model.

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s4[["DGM_1"]], "DGM_1")

```

Across increasing N, the average RMSE decreased substantially for all estimators.
Across all N, SEM-ULS performed substantially worse than the other two estimators.
For N=50-500, SEM-ML performed substantially worse than LSAM-ML.
For higher N, SEM-ML performed worse than LSAM-ML as well, but only slightly.

Thus, LSAM-ML appeared to outperform the two SEM-estimators for small to moderate samples sizes, in the conditions with four unmodelled cross-loadings.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r output = TRUE, echo = FALSE}

create_styled_table(rmse_s4[["DGM_2"]], "DGM_2")

```

Across increasing N, the average RMSE decreased substantially for all estimators.
For N=50, SEM-ULS performed substantially worse than the other two estimators.
For all other conditions, there was no difference between any of the estimators.

Thus, no estimator generally outperformed another in conditions with 20 unmodelled residual correlation.

### Summary

LSAM-ML appeared to outperform the two SEM-estimators in conditions with unmodelled cross-loadings. 
For conditions without misspecifications and unmodelled residual correlations, no difference arose besides a worse performance of SEM-ULS.

### Comparison with original paper

The results align closely to the findings in @robitzsch_comparing_2022.
A new insight is the comparatively worse performance of SEM-ULS in small samples (N=50).

## Summary Study 4

Overall, the main performance difference is that LSAM appeared to be outperforming the two SEM-estimators in smaller to moderate samples sizes, in conditions with unmodelled cross-loadings (DGM1).
This performance difference is attributed to its general negative small sample bias, that persisted even for low phi values in my Study 1b.

Thus, it cannot simply be stated that SAM estimation should be preferred in these scenarios.
In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there was no general advantage of one estimation method over another.
A new insight was the comparatively worse performance of SEM-ULS in small samples (N=50).
Due to my different findings with regards to the small sample bias of LSAM in Study 1b, I arrived at different conclusions than @robitzsch_comparing_2022.

# Results Simulation 4a: 5-factor-model under 2 different data-generating mechanisms and varying beta size

Again, I investigated a five factor model with regressions included, and this time under varying size of the regression coefficient beta.
Unlike the original paper, were the investigation was only conducted on the population level, I also varied the sample size as a factor in my simulation.
Also, keep in mind that the naming of the DGM changed to correspond with the code implementation. 
What @robitzsch_comparing_2022 called DGM 1-3, I now called DGM 0-2.

## Error, warnings and messages

```{r output = TRUE, echo = FALSE}

#Save all errors, warnings and messages
all_messages_s4a <- readRDS("SimulationResults/sim4a_results_error.rds")

#errors
errors_s4a <- all_messages_s4a$errors
errors_sum_s4a <- unlist(errors_s4a)
length(errors_sum_s4a)

#warnings
warnings_s4a <- all_messages_s4a$warnings
warnings_sum_s4a <- unlist(warnings_s4a)
length(warnings_sum_s4a)

#messages
messages_s4a <- all_messages_s4a$messages
messages_sum_s4a <- unlist(messages_s4a)
length(messages_sum_s4a)

```

No errors and messages were present. There were, however, 24089 warnings, I investigated in detail.

### Warnings investigation

```{r output= FALSE, echo = FALSE}

# Flatten the list of warnings into a single character vector
all_warnings_s4a <- unlist(warnings_s4a)

# Count the occurrences of each unique warning
warning_counts_s4a <- table(all_warnings_s4a)

# Convert to data frame for better formatting
warning_counts_df_s4a <- as.data.frame(warning_counts_s4a)
colnames(warning_counts_df_s4a) <- c("Warning Message", "Count")

warning_counts_df_s4a
```

The 24000 warnings with regards to Gamma computation in smaller samples were negligible as well, as I was not interested in the computation of robust fit indices:

`lavaan WARNING: number of observations (50) too small to compute Gamma`

`lavaan WARNING: number of observations (100) too small to compute Gamma`

All 89 other warnings referred to problems with positive definite matrices or model identification:

`lavaan WARNING: Could not compute standard errors! The information matrix could not be inverted. This may be a symptom that the model is not identified.`

`lavaan WARNING: The variance-covariance matrix of the estimated parameters (vcov) does not appear to be positive definite! The smallest eigenvalue is smaller than zero. This may be a symptom that the model is not identified.`

This suggests potential issues with multicollinearity or redundancy among latent variables, but only in a negligible number of estimations.


## Bias

```{r}

# Load the absolute bias results
bias_ci_s4a <- readRDS("SimulationResultsProcessed/sim4a_abs_bias_ci.rds")

```

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r output = TRUE, echo = FALSE}

#Normal print
#print(bias_ci_s4a[["DGM_1"]][["SEM_ML_metrics"]])
#print(bias_ci_s4a[["DGM_1"]][["SEM_ULS_metrics"]])
#print(bias_ci_s4a[["DGM_1"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(bias_ci_s4a[["DGM_1"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of SEM-ML for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_1"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of SEM-ULS for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_1"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of LSAM-ML for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

All three estimators were biased across all conditions.
The only exception is LSAM for very large samples (N=2500-100.000) and beta=0.3.
For all three estimators, across increasing N, the bias decreased substantially, in all conditions of beta.
For higher beta, the bias increased substantially, in all conditions of N for SEM-ML and SEM-ULS.
This was not the case for LSAM-ML.

Across all conditions, LSAM-ML substantially outperformed the two SEM-estimators.
The bias difference between LSAM-ML and the two other estimators was higher for lower N.
The only exception was for beta =0.1 in N=250 and all higher N. 
Here, there was only a slightly better performance of LSAM- over SEM-ML estimation, though not substantial.
This effect was stronger for higher values of beta.
Additionally, SEM-ML outperformed SEM-ULS in all conditions as well.

Thus, LSAM-ML generally appeared to outperform SEM-estimation and SEM-ML outperformed SEM-ULS estimation, in the presence of four unmodelled cross-loadings.
This was especially evident for higher beta values and lower N.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r output = TRUE, echo = FALSE}

#Normal print
#print(bias_ci_s4a[["DGM_2"]][["SEM_ML_metrics"]])
#print(bias_ci_s4a[["DGM_2"]][["SEM_ULS_metrics"]])
#print(bias_ci_s4a[["DGM_2"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(bias_ci_s4a[["DGM_2"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of SEM-ML for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_2"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of SEM-ULS for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_2"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left", latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of LSAM-ML for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

All estimators were generally biased across conditions, besides conditions of very high N (N=1000-100.000) and lower to moderate beta (0.1-0.3).
For all three estimators, across increasing N, the bias decreased substantially, in all conditions of beta.
Only in SEM-ML in N=50 and SEM-ULS in N=100, there was a substantial increase in bias for higher beta values.

For N=50, both SEM- and LSAM-ML substantially outperformed ULS estimation.
In these conditions, LSAM-ML also outperformed SEM-ML. for beta 0.2-0.4.
Lastly, LSAM-ML outperformed SEM-ULS in N=100 for beta = 0.4.

For all other conditions, none of the estimators differed from another, in terms of bias.

Thus, LSAM-ML appeared to perform best in small samples and higher beta.
Besides that, none of the estimators generally outperformed another in the presence of 20 unmodelled residual correlations.

## RMSE

```{r include = FALSE}

# Load the rmse results
rmse_s4a <- readRDS("SimulationResults/sim4a_rmse.rds")

```

### Conditions with four unmodelled cross-loadings (DGM 1)

```{r output = TRUE, echo = FALSE}

#Normal print
#print(rmse_s4a[["DGM_1"]][["SEM_ML_metrics"]])
#print(rmse_s4a[["DGM_1"]][["SEM_ULS_metrics"]])
#print(rmse_s4a[["DGM_1"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(rmse_s4a[["DGM_1"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ML for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_1"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ULS for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_1"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of LSAM-ML for DGM 1" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

Across all conditions of N and beta, LSAM-ML estimation was more efficient than the two SEM-estimators.
This difference was stronger in smaller samples.
Additionally, SEM-ML outperformed SEM-ULS estimation, in terms of efficiency, in all conditions as well.
Both SEM estimators had reduced RMSE values for higher N.
All three estimators had higher RMSE values for higher beta.
This effect was especially present in the two SEM estimators.

Thus, LSAM-ML appeared to outperform SEM-estimation and SEM-ML appeared to outperform SEM-ULS estimation, in the presence of four unmodelled cross-loadings.
This was especially true in lower samples and higher beta values.

### Conditions with 20 unmodelled residual correlation (DGM 2)

```{r output = TRUE, echo = FALSE}

#Normal print
#print(rmse_s4a[["DGM_2"]][["SEM_ML_metrics"]])
#print(rmse_s4a[["DGM_2"]][["SEM_ULS_metrics"]])
#print(rmse_s4a[["DGM_2"]][["LSAM_ML_metrics"]])

#Formatted tables
kbl(rmse_s4a[["DGM_2"]][["SEM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ML for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_2"]][["SEM_ULS_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of SEM-ULS for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(rmse_s4a[["DGM_2"]][["LSAM_ML_metrics"]], col.names = c("beta", "N=50", "N=100", "N=250", "N=500", "N=1000", "N=2500", "N=100.000")) %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: RMSE of LSAM-ML for DGM 2" = 8), bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

Across all conditions of N, LSAM-ML generally outperformed the two SEM-estimators for lower to moderate beta values (approx. beta= 0.2-0.4).
This effect was stronger for higher values of beta and lower N.
SEM-ULS had reduced RMSE values for higher N.
All three estimators had higher RMSE values for higher beta.
This effect was especially present in the two SEM estimators.

Thus, LSAM-ML appeared to generally outperform SEM-estimation, in the presence of 20 unmodelled residual correlation.
Even more so for higher values of beta and lower N.


## Summary Study 4a

In terms of bias, only in the presence of four unmodelled cross-loadings, did LSAM-estimation appear to generally outperform both SEM-estimators.
This was especially evident for higher beta values and lower N.
In the presence of unmodelled residual correlations, LSAM-ML appeared to perform best in small samples and higher beta as well.

With regards to RMSE, LSAM-ML appeared to outperform SEM-estimation both under unmodelled cross-loadings and residual correlations.
Again, this appeared to be especially present in conditions with small samples and high beta values.

Going all the way back to Studies 1-3, we again have to remember that LSAM exhibited a general negative bias in smaller samples for the 2-factor-CFA.
This made LSAM appear to outperform in conditions of lower N and higher phi or beta.
As replicated in Study 1b, this bias tended to be even stronger for higher parameter values in the structural model, and thus for higher beta values as well.

## Comparison to original paper

Importantly, @robitzsch_comparing_2022 did not investigate differential effects due to sample size.
Nor did they investigate RMSE.
Thus, results are only partly comparable.

In DMG1 (which was called DGM 2 in the original paper), the original paper indicated the same results, in that LSAM performed best.
Interestingly, @robitzsch_comparing_2022 did not find the better performance of SEM-ML over SEM-ULS estimation that I found.
Also, no differential effect for higher beta values was found in the original paper.

In DGM2, @robitzsch_comparing_2022 found SEM-ULS to perform slightly better than the two others.
Thus, results are quite similar, as I also did not find substantial differences between the three estimators.

I disagree with the conclusion that @robitzsch_comparing_2022 drew with regards to this study, as they deemed SEM-ULS and LSAM do not differ in performance, when in fact LSAM outperformed SEM-ULS in DGM1 in their studies.

Additionally, later on in the paper, they posited that SAM can be expected to be more robust than both SEM-estimators in models with non-saturated structural models.
With this last conclusion, I disagree with regards to the now multiple times mentioned potential negative small sample bias of LSAM, that was present for lower phi/ beta values as well.

# Summary for 5-factor-model under 3 different data-generating mechanisms (Studies 4 and 4a)

For low beta (0.1), LSAM appeared to outperform the two SEM-estimators in smaller to moderate samples sizes, in conditions with unmodelled cross-loadings (DGM1).
In conditions without misspecification (DGM 0) and in the presence of unmodelled residual correlations (DGM 2), there was no general advantage of one estimation method over another.

With increasing values of beta (0.2-0.4), the effect with regards to unmodelled cross-loadings became even stronger (DGM 1).
In this context, LSAM appeared to not only outperform SEM-estimation in terms of efficiency in small to moderate samples in DGM1, but also in conditions with unmodelled residual correlations (DGM2).

Very important to keep in mind here is the importang finding of the earlier study and the original paper, that LSAM exhibits a negative finite sample bias that skewed results in conditions with positive values for unmodelled cross-loadings or residual correlations [@robitzsch_comparing_2022].
This effect was even stronger for higher values of phi and low N, which aligns with the findings in my studies for models with regressions.

# Overall summary

With regards to all studies conducted, as in the original paper by @robitzsch_comparing_2022, SAM did not generally outperform SEM in small to moderate samples. 
SAM exhibited a negative small sample bias that made SAM appear superior in conditions with unmodelled positive cross-loadings and residual correlations.
This bias was especially strong for lower lambda and higher phi or beta values.
Going ahead of what was investigated in @robitzsch_comparing_2022, I found that this bias is also present in models with lower phi or beta values.
Thus, it cannot be concluded that SAM is more robust in models with non-saturated structural parameters.
If there was no misspecification or unmodelled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM, as far as can be concluded from the 2-factor-CFA-models.
